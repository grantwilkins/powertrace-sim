{
  "date": "20251007-050043",
  "backend": "vllm",
  "model_id": "meta-llama/Llama-3.1-8B-Instruct",
  "tokenizer_id": "meta-llama/Llama-3.1-8B-Instruct",
  "num_prompts": 9600,
  "tensor_parallel_size": 2,
  "request_rate": 16.0,
  "burstiness": 1.0,
  "max_concurrency": null,
  "duration": 680.135362907,
  "completed": 8078,
  "total_input_tokens": 32284284,
  "total_output_tokens": 119305,
  "request_throughput": 11.87704748283257,
  "request_goodput:": null,
  "output_throughput": 175.41361103482788,
  "total_token_throughput": 47642.852830798605,
  "input_lens": [
    13911,
    3103,
    5864,
    16384,
    14946,
    681,
    5682,
    1691,
    1783,
    3138,
    2341,
    17622,
    15168,
    2284,
    3255,
    2884,
    10337,
    1594,
    2819,
    10438,
    120,
    4100,
    5171,
    883,
    16384,
    403,
    2101,
    1626,
    10786,
    10059,
    2369,
    3028,
    752,
    226,
    46575,
    2373,
    7733,
    7499,
    1304,
    1432,
    630,
    419,
    305,
    16384,
    1140,
    1234,
    503,
    4699,
    338,
    1581,
    746,
    3058,
    1139,
    545,
    20821,
    3200,
    2149,
    2786,
    994,
    1340,
    953,
    1345,
    816,
    299,
    2428,
    1284,
    332,
    3324,
    11222,
    2115,
    4455,
    2302,
    6997,
    513,
    3110,
    940,
    766,
    1057,
    1418,
    11855,
    554,
    5382,
    3335,
    368,
    10270,
    16082,
    23414,
    1639,
    615,
    6373,
    1282,
    7667,
    2512,
    5850,
    2957,
    4346,
    2021,
    14249,
    18247,
    3109,
    15858,
    453,
    493,
    5804,
    549,
    16384,
    1267,
    878,
    16384,
    10183,
    16858,
    5413,
    774,
    28158,
    1488,
    4830,
    5664,
    1684,
    11156,
    11226,
    3023,
    596,
    2774,
    8595,
    930,
    32759,
    1238,
    15278,
    4186,
    3128,
    856,
    3616,
    951,
    18361,
    992,
    4205,
    3767,
    1589,
    3089,
    600,
    387,
    3240,
    2400,
    4017,
    16384,
    5647,
    24658,
    6827,
    469,
    1202,
    1853,
    13156,
    880,
    805,
    1793,
    963,
    6900,
    609,
    565,
    1234,
    1155,
    16384,
    5678,
    2200,
    519,
    5058,
    664,
    365,
    7382,
    2831,
    5502,
    2837,
    5128,
    976,
    640,
    4229,
    825,
    11612,
    1210,
    2036,
    1353,
    440,
    984,
    173,
    3974,
    343,
    592,
    2116,
    885,
    10908,
    481,
    2680,
    10159,
    552,
    3553,
    1654,
    4670,
    4943,
    17704,
    8692,
    1331,
    1535,
    6698,
    4108,
    4040,
    337,
    28870,
    887,
    2718,
    10543,
    5438,
    2832,
    4745,
    1196,
    707,
    1272,
    1961,
    3032,
    16384,
    1907,
    698,
    1365,
    41865,
    3393,
    366,
    2142,
    2373,
    2579,
    1035,
    1538,
    417,
    1161,
    1099,
    3157,
    560,
    4718,
    10341,
    205,
    3193,
    4207,
    991,
    1290,
    13486,
    1440,
    1422,
    316,
    7097,
    6552,
    816,
    398,
    3544,
    1060,
    2335,
    1406,
    4275,
    4290,
    899,
    436,
    350,
    3910,
    18419,
    1144,
    1036,
    1885,
    237,
    2459,
    3555,
    2202,
    1419,
    2224,
    13716,
    94,
    16384,
    3068,
    974,
    1299,
    13929,
    1758,
    10986,
    16384,
    1769,
    6137,
    933,
    10829,
    2737,
    3903,
    632,
    7572,
    4267,
    8366,
    1001,
    1177,
    16384,
    622,
    1720,
    6978,
    2224,
    3794,
    12532,
    3002,
    16115,
    12381,
    1754,
    945,
    4158,
    1203,
    460,
    454,
    4286,
    1676,
    1724,
    6538,
    578,
    894,
    1308,
    2216,
    1907,
    1457,
    1867,
    1775,
    905,
    817,
    2702,
    749,
    13960,
    1417,
    1680,
    16384,
    920,
    5639,
    4545,
    540,
    4677,
    543,
    13093,
    31740,
    289,
    3281,
    941,
    12400,
    6472,
    10059,
    937,
    525,
    1230,
    1467,
    1337,
    2374,
    3775,
    2935,
    862,
    410,
    8964,
    17291,
    975,
    1126,
    263,
    1181,
    1178,
    3953,
    4308,
    2006,
    5569,
    2904,
    1964,
    2385,
    1620,
    1294,
    1488,
    577,
    2720,
    670,
    5043,
    1518,
    2110,
    3439,
    4054,
    355,
    1591,
    5261,
    308,
    3059,
    167,
    15854,
    22851,
    19312,
    675,
    395,
    12245,
    2393,
    3729,
    1564,
    1354,
    337,
    1449,
    864,
    5134,
    7010,
    10029,
    5104,
    1034,
    585,
    4643,
    2956,
    285,
    2954,
    4895,
    11724,
    1630,
    821,
    406,
    4819,
    1422,
    1545,
    13440,
    4242,
    3004,
    2336,
    10635,
    13247,
    5555,
    3791,
    199,
    2289,
    1731,
    2215,
    5638,
    98,
    1068,
    2688,
    11066,
    10247,
    5197,
    11902,
    686,
    2825,
    4933,
    2009,
    12961,
    2177,
    14121,
    558,
    17597,
    2474,
    5236,
    1760,
    3304,
    691,
    844,
    1769,
    626,
    13146,
    3325,
    2716,
    2900,
    16384,
    1193,
    177,
    2487,
    1890,
    1130,
    680,
    1232,
    2439,
    1149,
    16384,
    694,
    835,
    161,
    2634,
    217,
    1103,
    1475,
    915,
    13531,
    24533,
    8527,
    756,
    6915,
    3448,
    4668,
    16533,
    735,
    1252,
    5160,
    107,
    10558,
    3671,
    1900,
    2546,
    20111,
    1359,
    6703,
    8331,
    16384,
    1842,
    10552,
    1134,
    652,
    1834,
    24449,
    1924,
    6674,
    1544,
    1363,
    10739,
    331,
    356,
    546,
    8362,
    5349,
    9067,
    461,
    229,
    966,
    2424,
    3458,
    6328,
    2731,
    13587,
    1564,
    731,
    314,
    751,
    2607,
    751,
    5599,
    9448,
    12614,
    5169,
    170,
    3107,
    7687,
    2145,
    488,
    1049,
    1498,
    1635,
    1598,
    1770,
    2527,
    528,
    1531,
    10615,
    1308,
    1226,
    6542,
    119,
    19960,
    997,
    2393,
    2221,
    5634,
    1488,
    947,
    8330,
    148,
    2043,
    453,
    864,
    11868,
    1902,
    2476,
    281,
    896,
    2480,
    11183,
    3938,
    2017,
    3567,
    3291,
    267,
    2081,
    4650,
    3823,
    1339,
    823,
    583,
    1729,
    6949,
    233,
    966,
    570,
    4738,
    1086,
    1190,
    1573,
    3261,
    1297,
    70,
    3632,
    3238,
    1569,
    606,
    2942,
    3032,
    1191,
    1574,
    718,
    11064,
    363,
    3162,
    707,
    15298,
    425,
    1044,
    1769,
    321,
    2268,
    12270,
    293,
    476,
    3887,
    5351,
    1728,
    3118,
    2556,
    2871,
    8222,
    380,
    4205,
    1312,
    1561,
    1432,
    1322,
    518,
    2444,
    12557,
    1878,
    1995,
    938,
    1755,
    3336,
    1329,
    1212,
    3113,
    727,
    2637,
    4926,
    8919,
    1809,
    8994,
    6234,
    667,
    523,
    1428,
    6197,
    1845,
    1032,
    11020,
    2739,
    155,
    2832,
    3540,
    17251,
    15228,
    1855,
    10731,
    1329,
    706,
    716,
    498,
    3287,
    2225,
    1220,
    978,
    1947,
    6549,
    220,
    3024,
    1096,
    251,
    235,
    732,
    2543,
    3079,
    10663,
    6116,
    9559,
    3089,
    1042,
    6883,
    4586,
    5188,
    970,
    88,
    16384,
    339,
    1921,
    16384,
    2874,
    5676,
    382,
    282,
    1112,
    6633,
    13599,
    833,
    2484,
    6569,
    407,
    527,
    839,
    6661,
    2587,
    16384,
    5597,
    1922,
    8035,
    2521,
    920,
    4221,
    928,
    1451,
    8608,
    1787,
    825,
    1198,
    26668,
    1088,
    1305,
    1139,
    2446,
    1307,
    11549,
    753,
    716,
    7845,
    4885,
    3812,
    1146,
    814,
    1143,
    628,
    16384,
    169,
    3716,
    486,
    1781,
    673,
    547,
    570,
    13773,
    1726,
    860,
    3682,
    2021,
    4411,
    268,
    2790,
    4674,
    321,
    3271,
    12910,
    1965,
    4932,
    4178,
    917,
    2087,
    356,
    1216,
    2676,
    4426,
    2053,
    4411,
    593,
    1786,
    2041,
    11013,
    1578,
    1154,
    2045,
    727,
    2470,
    1337,
    10182,
    1873,
    1409,
    331,
    1855,
    10283,
    3545,
    10042,
    456,
    3376,
    2352,
    3575,
    3180,
    447,
    1909,
    868,
    1891,
    744,
    8465,
    776,
    743,
    12862,
    611,
    1252,
    801,
    9436,
    4742,
    1875,
    30008,
    5625,
    3120,
    3455,
    1941,
    311,
    1765,
    10997,
    4062,
    6081,
    968,
    25343,
    13486,
    958,
    12709,
    782,
    2049,
    1973,
    11866,
    795,
    1042,
    958,
    2863,
    2872,
    16384,
    9028,
    1140,
    2856,
    5983,
    2066,
    1850,
    2114,
    5187,
    785,
    1396,
    3352,
    2814,
    2600,
    12103,
    5824,
    16384,
    3124,
    1615,
    4588,
    1104,
    875,
    2071,
    116,
    25652,
    10317,
    450,
    641,
    1235,
    327,
    1278,
    1108,
    2054,
    7112,
    2415,
    2045,
    2229,
    2566,
    653,
    1761,
    2806,
    442,
    5178,
    6565,
    997,
    1532,
    760,
    4312,
    11848,
    1564,
    776,
    10090,
    277,
    8596,
    15035,
    2134,
    29750,
    864,
    752,
    5597,
    18938,
    12826,
    827,
    980,
    3359,
    5560,
    1647,
    418,
    16384,
    778,
    366,
    16384,
    1281,
    400,
    942,
    2993,
    10546,
    782,
    14838,
    1125,
    542,
    5748,
    8621,
    813,
    427,
    6207,
    210,
    11307,
    5791,
    24251,
    1494,
    2945,
    1689,
    478,
    8132,
    8582,
    2504,
    2099,
    16384,
    1474,
    10943,
    2983,
    18922,
    11522,
    1503,
    2805,
    438,
    12499,
    792,
    660,
    12704,
    835,
    1113,
    2988,
    8329,
    3392,
    16384,
    1840,
    2656,
    2705,
    9687,
    3491,
    1758,
    704,
    2614,
    9334,
    1272,
    3575,
    2619,
    5166,
    824,
    16384,
    489,
    1336,
    5607,
    2769,
    4979,
    1157,
    1840,
    2025,
    11228,
    4270,
    4799,
    969,
    5800,
    14209,
    9210,
    16384,
    1425,
    1278,
    772,
    34677,
    1312,
    2967,
    1704,
    1342,
    6444,
    712,
    3217,
    1278,
    4432,
    9170,
    1431,
    3245,
    2432,
    829,
    22275,
    2746,
    3146,
    1606,
    2216,
    565,
    1347,
    3683,
    16069,
    1255,
    2242,
    2567,
    2493,
    3622,
    25398,
    1892,
    2599,
    664,
    12599,
    2386,
    11156,
    837,
    736,
    2557,
    315,
    2531,
    2223,
    23294,
    10880,
    1262,
    15069,
    13141,
    836,
    632,
    605,
    6829,
    1129,
    872,
    2324,
    1591,
    947,
    4579,
    16386,
    5906,
    4645,
    3111,
    283,
    12533,
    2785,
    3900,
    6812,
    9669,
    3166,
    3226,
    1033,
    2072,
    780,
    905,
    747,
    1683,
    11926,
    16384,
    2461,
    453,
    8034,
    17817,
    966,
    2517,
    510,
    2552,
    1813,
    2226,
    3039,
    2152,
    2034,
    2731,
    3155,
    642,
    17658,
    1867,
    413,
    2200,
    18560,
    3896,
    630,
    775,
    2867,
    1285,
    1410,
    3852,
    14167,
    1285,
    828,
    634,
    778,
    19346,
    2115,
    759,
    1549,
    329,
    891,
    16384,
    1809,
    4468,
    1859,
    2930,
    4144,
    592,
    1931,
    11347,
    832,
    1071,
    1424,
    2686,
    3559,
    8055,
    3461,
    1866,
    7983,
    4335,
    385,
    16384,
    14001,
    1660,
    3028,
    8576,
    1653,
    4462,
    6734,
    654,
    1030,
    5505,
    3317,
    5520,
    1727,
    1454,
    221,
    566,
    2104,
    4949,
    3584,
    1735,
    1481,
    2537,
    2177,
    9367,
    2347,
    391,
    492,
    10621,
    550,
    11233,
    1487,
    1657,
    1724,
    7658,
    1616,
    1926,
    370,
    2508,
    3583,
    2599,
    9299,
    2123,
    2776,
    12250,
    363,
    14146,
    9592,
    5595,
    4214,
    4993,
    12308,
    12025,
    3027,
    2601,
    2379,
    2470,
    559,
    4664,
    1731,
    14825,
    1838,
    3174,
    2620,
    14776,
    5951,
    16245,
    1965,
    1435,
    1352,
    249,
    1643,
    2633,
    6375,
    5744,
    1263,
    1473,
    6879,
    1651,
    1140,
    9244,
    10428,
    2039,
    1039,
    218,
    11251,
    745,
    230,
    11422,
    4075,
    570,
    525,
    5208,
    18527,
    8314,
    3936,
    35351,
    3118,
    2466,
    5263,
    1212,
    2196,
    4569,
    3711,
    536,
    1152,
    2638,
    1275,
    14074,
    1296,
    1671,
    4658,
    2874,
    1703,
    869,
    21936,
    6266,
    3384,
    45106,
    13500,
    406,
    19817,
    5748,
    2561,
    1091,
    15060,
    16384,
    22500,
    3596,
    2834,
    33217,
    3619,
    4472,
    1322,
    1449,
    294,
    846,
    2692,
    6307,
    3862,
    1373,
    497,
    94,
    7093,
    1045,
    46983,
    2309,
    425,
    1359,
    16384,
    3483,
    2966,
    350,
    16384,
    417,
    16384,
    195,
    9375,
    11847,
    806,
    3180,
    3649,
    816,
    10879,
    468,
    3623,
    1819,
    1074,
    5787,
    3494,
    29555,
    533,
    3553,
    1106,
    2228,
    11315,
    3472,
    773,
    2384,
    700,
    11723,
    1077,
    2509,
    2803,
    2380,
    231,
    34251,
    1214,
    10221,
    1717,
    697,
    453,
    1284,
    1193,
    3512,
    11913,
    3877,
    1038,
    1507,
    1362,
    845,
    3974,
    816,
    1125,
    1843,
    479,
    1397,
    913,
    1303,
    1870,
    26139,
    1568,
    8429,
    1942,
    7042,
    2925,
    4682,
    852,
    2242,
    2315,
    1018,
    808,
    387,
    10360,
    685,
    8785,
    1194,
    773,
    3963,
    997,
    3734,
    1385,
    3389,
    688,
    4986,
    3417,
    726,
    16384,
    11770,
    16384,
    11617,
    1623,
    1269,
    1282,
    266,
    929,
    2621,
    10705,
    854,
    5272,
    503,
    1048,
    14986,
    3007,
    3304,
    5759,
    4665,
    2610,
    3069,
    11468,
    1139,
    4685,
    273,
    3141,
    1174,
    2003,
    6273,
    2394,
    5290,
    13175,
    3065,
    7245,
    16359,
    1980,
    1124,
    6293,
    3151,
    1143,
    2368,
    6283,
    1913,
    703,
    2310,
    226,
    4654,
    1257,
    1192,
    5235,
    445,
    13663,
    1178,
    1123,
    6144,
    4357,
    16384,
    1583,
    1750,
    392,
    1386,
    903,
    1219,
    293,
    12415,
    420,
    91,
    540,
    1028,
    564,
    6688,
    1717,
    2054,
    3910,
    2736,
    5863,
    589,
    1094,
    4157,
    122,
    18002,
    3467,
    1178,
    5595,
    4866,
    534,
    3125,
    7494,
    30313,
    681,
    5257,
    4019,
    3629,
    4391,
    74,
    5266,
    14602,
    3230,
    12316,
    36728,
    2898,
    48988,
    2001,
    859,
    661,
    666,
    441,
    18838,
    13868,
    4580,
    1004,
    10130,
    2261,
    971,
    10575,
    4700,
    1921,
    2891,
    5298,
    1481,
    19381,
    1421,
    1936,
    1397,
    1116,
    2418,
    3726,
    2347,
    3458,
    887,
    531,
    3161,
    4258,
    2110,
    8802,
    5423,
    16384,
    1602,
    665,
    885,
    17618,
    3373,
    13355,
    8521,
    1535,
    1523,
    12400,
    1762,
    2027,
    1747,
    2901,
    1044,
    11084,
    3652,
    2227,
    2482,
    6405,
    648,
    779,
    7966,
    391,
    473,
    4913,
    2596,
    2243,
    1806,
    2068,
    1805,
    8874,
    1289,
    1673,
    14384,
    2059,
    16384,
    1780,
    43048,
    323,
    2366,
    349,
    5058,
    526,
    2730,
    1464,
    558,
    14625,
    1138,
    13557,
    1446,
    5480,
    1876,
    5241,
    12275,
    1282,
    5677,
    1669,
    1816,
    1244,
    7074,
    2771,
    2097,
    4053,
    3816,
    2524,
    13192,
    1869,
    2713,
    985,
    2356,
    11459,
    46986,
    574,
    5981,
    1696,
    2220,
    1901,
    2179,
    5092,
    793,
    656,
    2193,
    341,
    441,
    15573,
    4597,
    1976,
    7799,
    636,
    1412,
    3967,
    5322,
    3512,
    122,
    688,
    3377,
    1350,
    16384,
    5537,
    3691,
    584,
    1922,
    2605,
    6908,
    5267,
    6224,
    723,
    9446,
    16525,
    1108,
    3209,
    1694,
    660,
    809,
    38779,
    3587,
    7995,
    1789,
    1286,
    395,
    5456,
    16384,
    276,
    620,
    946,
    1074,
    2565,
    11797,
    6059,
    3570,
    901,
    583,
    831,
    10969,
    1866,
    1222,
    1632,
    4949,
    471,
    18221,
    2373,
    1575,
    3252,
    15155,
    1368,
    1513,
    12258,
    4113,
    1112,
    13900,
    16203,
    8621,
    1894,
    1023,
    3115,
    16384,
    404,
    3042,
    2503,
    7170,
    5943,
    1627,
    313,
    21792,
    19459,
    705,
    7066,
    733,
    9349,
    10052,
    2838,
    2801,
    329,
    283,
    2532,
    3732,
    2188,
    809,
    2040,
    1825,
    697,
    6095,
    297,
    3818,
    3049,
    6067,
    10785,
    2233,
    16384,
    4139,
    2231,
    3615,
    2196,
    16384,
    5895,
    1818,
    14465,
    1300,
    7561,
    425,
    341,
    9942,
    16384,
    3344,
    13949,
    19196,
    980,
    2412,
    2085,
    3980,
    360,
    1143,
    5061,
    17572,
    670,
    16384,
    28010,
    1057,
    310,
    4457,
    4311,
    1438,
    594,
    1944,
    796,
    708,
    1783,
    628,
    2621,
    3900,
    793,
    10444,
    11130,
    710,
    966,
    2525,
    3863,
    1507,
    3317,
    1285,
    20953,
    9594,
    16384,
    12907,
    2335,
    15014,
    2955,
    1182,
    3337,
    1801,
    677,
    743,
    4828,
    262,
    3883,
    332,
    193,
    264,
    16384,
    1610,
    20531,
    9520,
    5583,
    432,
    5197,
    2446,
    1372,
    2052,
    35155,
    754,
    3105,
    1977,
    276,
    826,
    2471,
    8325,
    6011,
    3854,
    815,
    14491,
    2531,
    660,
    1634,
    5355,
    2015,
    5300,
    6728,
    10569,
    777,
    2319,
    2099,
    15448,
    333,
    1722,
    1050,
    2888,
    136,
    6811,
    2028,
    262,
    23108,
    3904,
    12639,
    2005,
    624,
    1084,
    2057,
    2444,
    1190,
    2697,
    4913,
    1470,
    9650,
    9980,
    1246,
    990,
    320,
    1738,
    1339,
    4707,
    379,
    1472,
    5800,
    894,
    863,
    406,
    16384,
    878,
    478,
    825,
    852,
    1485,
    13099,
    1439,
    724,
    404,
    35705,
    2093,
    10764,
    2212,
    1792,
    629,
    1427,
    23571,
    1329,
    695,
    3612,
    1708,
    1602,
    303,
    3439,
    3414,
    793,
    5938,
    445,
    12205,
    1482,
    469,
    2461,
    13001,
    17154,
    1200,
    3267,
    2244,
    2060,
    1251,
    9548,
    3301,
    1117,
    1774,
    884,
    1023,
    987,
    573,
    4698,
    1450,
    3675,
    956,
    10237,
    914,
    4410,
    1520,
    894,
    328,
    2706,
    916,
    1963,
    1163,
    5707,
    3635,
    3267,
    1014,
    3337,
    13178,
    799,
    2415,
    325,
    9306,
    1289,
    4726,
    300,
    14433,
    1349,
    3641,
    2358,
    1508,
    12763,
    326,
    2775,
    5440,
    1933,
    14970,
    1707,
    517,
    2083,
    3512,
    2154,
    1481,
    1165,
    1469,
    7970,
    12330,
    2088,
    1393,
    9910,
    2123,
    10233,
    193,
    3312,
    2719,
    10245,
    328,
    1684,
    2148,
    37650,
    7617,
    1376,
    16384,
    6371,
    5686,
    3696,
    620,
    18531,
    3751,
    4210,
    8416,
    1194,
    775,
    5141,
    820,
    11244,
    7477,
    3300,
    1348,
    2090,
    3838,
    2020,
    16384,
    672,
    663,
    682,
    1044,
    11950,
    999,
    973,
    2178,
    3163,
    510,
    5377,
    14508,
    1589,
    11291,
    2487,
    16384,
    6829,
    357,
    2039,
    6372,
    2066,
    1918,
    8076,
    13437,
    2037,
    2852,
    1384,
    1954,
    4686,
    3216,
    820,
    593,
    838,
    2000,
    1675,
    800,
    1034,
    375,
    3164,
    1912,
    499,
    2062,
    8750,
    885,
    8490,
    1399,
    2483,
    2225,
    12676,
    2378,
    569,
    472,
    370,
    303,
    2102,
    696,
    1828,
    921,
    855,
    1177,
    4332,
    5552,
    3005,
    672,
    4056,
    4263,
    2702,
    1028,
    4357,
    3181,
    64,
    4059,
    243,
    4146,
    1686,
    7427,
    1793,
    18169,
    1699,
    6411,
    2056,
    1762,
    4527,
    2518,
    1985,
    8979,
    24511,
    3923,
    1458,
    10369,
    7342,
    4406,
    524,
    2332,
    881,
    10431,
    2602,
    2230,
    1184,
    8105,
    309,
    18981,
    10259,
    3043,
    753,
    5250,
    2197,
    2624,
    651,
    972,
    2509,
    3796,
    16384,
    2553,
    5846,
    360,
    462,
    1351,
    11061,
    13155,
    1272,
    9139,
    10254,
    2094,
    10113,
    1878,
    3612,
    3400,
    1744,
    3482,
    13307,
    4377,
    2859,
    2292,
    655,
    645,
    2851,
    443,
    860,
    8181,
    16384,
    319,
    11959,
    1583,
    1965,
    1763,
    6559,
    340,
    3300,
    706,
    3743,
    10906,
    1997,
    3015,
    3135,
    829,
    16009,
    13057,
    4324,
    2165,
    10396,
    12817,
    35591,
    9469,
    11989,
    351,
    88,
    606,
    1730,
    9327,
    975,
    3481,
    8379,
    2301,
    14317,
    17850,
    532,
    3158,
    1601,
    2286,
    1896,
    4148,
    842,
    1381,
    16087,
    829,
    1465,
    1044,
    3259,
    6152,
    1155,
    1243,
    1470,
    19988,
    885,
    1321,
    147,
    437,
    1765,
    15334,
    2764,
    596,
    428,
    2421,
    324,
    6454,
    2143,
    341,
    690,
    900,
    853,
    388,
    763,
    941,
    913,
    6910,
    11440,
    15813,
    1382,
    759,
    1436,
    5234,
    2637,
    16384,
    3021,
    731,
    4869,
    6554,
    6658,
    14746,
    1698,
    6939,
    313,
    1153,
    13668,
    716,
    656,
    7907,
    1543,
    773,
    19550,
    2335,
    14634,
    8764,
    3463,
    413,
    1000,
    6483,
    1009,
    13467,
    596,
    3751,
    774,
    1140,
    6690,
    1737,
    4889,
    14509,
    4579,
    752,
    1565,
    3186,
    10447,
    11997,
    11950,
    1436,
    660,
    189,
    14423,
    1595,
    1220,
    1605,
    9526,
    691,
    4219,
    24809,
    1036,
    3728,
    5925,
    12392,
    857,
    552,
    6726,
    1061,
    261,
    9423,
    442,
    4709,
    2444,
    2742,
    17117,
    2987,
    319,
    3815,
    11077,
    12563,
    216,
    676,
    1625,
    3413,
    1777,
    1161,
    3846,
    7190,
    1547,
    4455,
    117,
    712,
    1403,
    1167,
    2881,
    14125,
    3488,
    1007,
    374,
    3687,
    264,
    4098,
    1624,
    548,
    13963,
    1991,
    1919,
    1821,
    3169,
    5524,
    3450,
    6083,
    1903,
    11852,
    3745,
    1098,
    599,
    2505,
    474,
    16591,
    2599,
    1023,
    715,
    1923,
    2164,
    1593,
    1319,
    5115,
    13085,
    1562,
    16384,
    2473,
    18832,
    1078,
    446,
    836,
    1488,
    1157,
    8690,
    1751,
    3319,
    1898,
    1240,
    2083,
    13166,
    858,
    4645,
    646,
    1205,
    2006,
    2861,
    22251,
    1891,
    789,
    4084,
    18759,
    2598,
    1758,
    235,
    3495,
    3796,
    5537,
    14479,
    536,
    3526,
    3134,
    1260,
    3092,
    3462,
    522,
    2619,
    727,
    975,
    283,
    1187,
    1597,
    3658,
    2000,
    366,
    11889,
    1753,
    3419,
    5782,
    9555,
    16384,
    1845,
    2813,
    610,
    13749,
    542,
    13516,
    4981,
    1125,
    4700,
    3130,
    35238,
    100,
    598,
    2034,
    521,
    974,
    401,
    2408,
    2208,
    1176,
    9290,
    10378,
    12550,
    275,
    592,
    3127,
    1000,
    1169,
    5371,
    3505,
    8480,
    13376,
    9070,
    1086,
    10398,
    2244,
    775,
    332,
    1434,
    1507,
    5132,
    1769,
    1241,
    6533,
    1560,
    1060,
    3759,
    1165,
    4124,
    1036,
    1563,
    24765,
    1324,
    1968,
    4900,
    11604,
    3391,
    993,
    5105,
    4173,
    6032,
    898,
    1652,
    4011,
    1022,
    1558,
    16384,
    16384,
    7816,
    5610,
    651,
    3511,
    1346,
    623,
    1141,
    2269,
    1094,
    1090,
    4775,
    2341,
    2583,
    2259,
    14191,
    442,
    2811,
    1143,
    218,
    1297,
    42061,
    4309,
    3546,
    3451,
    961,
    4468,
    2840,
    1273,
    1213,
    5334,
    1186,
    2790,
    6231,
    16322,
    12459,
    18756,
    381,
    936,
    1093,
    12313,
    925,
    3055,
    2232,
    714,
    3041,
    3096,
    502,
    21130,
    201,
    1043,
    5820,
    537,
    2937,
    480,
    1803,
    156,
    790,
    366,
    1284,
    1265,
    952,
    4806,
    753,
    4015,
    11994,
    2328,
    777,
    505,
    915,
    4337,
    2371,
    5599,
    4662,
    2332,
    3363,
    15379,
    9482,
    1432,
    5935,
    3806,
    6974,
    12120,
    684,
    336,
    3751,
    10041,
    851,
    1624,
    639,
    7299,
    10161,
    165,
    3018,
    1825,
    1178,
    1434,
    3607,
    1267,
    597,
    720,
    5309,
    1121,
    435,
    2235,
    3482,
    8620,
    2539,
    967,
    3365,
    4446,
    1914,
    1902,
    2660,
    1850,
    2631,
    649,
    563,
    796,
    4049,
    2656,
    6274,
    1627,
    567,
    7037,
    12500,
    8199,
    3857,
    16384,
    1569,
    14900,
    5326,
    12957,
    1652,
    400,
    363,
    2296,
    4780,
    4031,
    2904,
    5163,
    1044,
    1480,
    4497,
    3217,
    1585,
    8543,
    482,
    1129,
    1463,
    4905,
    2003,
    498,
    3517,
    6609,
    4506,
    12205,
    254,
    742,
    3395,
    1881,
    11970,
    496,
    11128,
    433,
    232,
    2766,
    5048,
    2618,
    1926,
    358,
    6043,
    1903,
    16384,
    5633,
    220,
    4585,
    16957,
    866,
    1516,
    1801,
    3094,
    648,
    563,
    3868,
    1969,
    13190,
    21085,
    1644,
    750,
    725,
    5508,
    598,
    600,
    1388,
    3279,
    755,
    7766,
    9846,
    757,
    1526,
    848,
    12544,
    26401,
    4478,
    18830,
    744,
    8689,
    8748,
    23970,
    3875,
    5221,
    16384,
    3379,
    1870,
    770,
    2795,
    6164,
    2615,
    847,
    2203,
    1733,
    2673,
    321,
    3662,
    3846,
    27986,
    1989,
    274,
    1956,
    6417,
    775,
    224,
    367,
    2822,
    3011,
    5297,
    1880,
    383,
    812,
    4918,
    2332,
    4083,
    3223,
    1594,
    1650,
    15410,
    11500,
    1734,
    3297,
    2553,
    186,
    4085,
    14809,
    846,
    9891,
    1506,
    2759,
    6485,
    905,
    7838,
    525,
    763,
    1040,
    4138,
    1373,
    375,
    1574,
    843,
    4466,
    1369,
    13875,
    1279,
    3220,
    1632,
    2857,
    115,
    2223,
    3150,
    1604,
    4172,
    4502,
    11720,
    2242,
    243,
    161,
    343,
    1916,
    1680,
    2707,
    1004,
    888,
    3679,
    4111,
    1506,
    1915,
    3117,
    20704,
    567,
    905,
    516,
    1144,
    11343,
    3644,
    597,
    3510,
    2196,
    1160,
    425,
    1648,
    4221,
    185,
    1292,
    16384,
    4187,
    2659,
    888,
    947,
    14145,
    11651,
    3334,
    765,
    7291,
    1448,
    12927,
    1071,
    662,
    2421,
    5887,
    6255,
    2380,
    673,
    1146,
    216,
    731,
    1642,
    3065,
    16773,
    1876,
    1292,
    4564,
    743,
    5020,
    16384,
    3361,
    1119,
    1107,
    7561,
    578,
    699,
    21359,
    489,
    633,
    4764,
    6717,
    928,
    2510,
    4605,
    11382,
    698,
    396,
    6072,
    3448,
    3769,
    590,
    2587,
    3991,
    2822,
    880,
    6084,
    372,
    5550,
    6562,
    11266,
    1370,
    665,
    4785,
    925,
    2097,
    1409,
    740,
    2844,
    17668,
    3833,
    689,
    297,
    1891,
    3212,
    3769,
    4915,
    150,
    661,
    2246,
    10624,
    4732,
    16179,
    1121,
    2702,
    595,
    1280,
    889,
    994,
    1296,
    2004,
    2047,
    3632,
    2330,
    1368,
    1124,
    1425,
    1219,
    3429,
    5177,
    11332,
    830,
    3348,
    430,
    3012,
    6574,
    1695,
    6502,
    571,
    751,
    1720,
    6151,
    1262,
    1213,
    671,
    2497,
    7874,
    12431,
    13569,
    1591,
    1075,
    20746,
    1929,
    1650,
    5367,
    1607,
    795,
    5432,
    2183,
    12635,
    587,
    2219,
    26320,
    12817,
    1380,
    10680,
    1876,
    406,
    354,
    7702,
    4158,
    4957,
    1875,
    898,
    13423,
    2320,
    794,
    3705,
    499,
    1385,
    1596,
    934,
    176,
    3270,
    870,
    8589,
    1371,
    1101,
    2209,
    6366,
    1075,
    14715,
    45073,
    5939,
    11957,
    8282,
    10576,
    11669,
    12551,
    1246,
    165,
    466,
    1220,
    1313,
    1689,
    664,
    345,
    1731,
    1622,
    821,
    883,
    710,
    1291,
    779,
    7989,
    2604,
    685,
    1467,
    545,
    6083,
    9160,
    7920,
    567,
    1807,
    1284,
    3707,
    659,
    950,
    1268,
    2365,
    4287,
    14935,
    4147,
    4935,
    1580,
    507,
    550,
    3831,
    1183,
    303,
    3921,
    2304,
    11175,
    13833,
    5015,
    2327,
    491,
    427,
    1425,
    303,
    3120,
    423,
    2395,
    1455,
    4371,
    712,
    2700,
    430,
    32529,
    1760,
    3447,
    467,
    3459,
    2798,
    2981,
    2818,
    1615,
    7827,
    1683,
    886,
    1873,
    702,
    1199,
    1644,
    18198,
    2488,
    16384,
    3744,
    6499,
    1148,
    1047,
    1318,
    5106,
    188,
    641,
    2236,
    1561,
    16384,
    3269,
    964,
    352,
    1373,
    476,
    3338,
    2385,
    2841,
    17276,
    736,
    37642,
    526,
    1828,
    11528,
    3735,
    10721,
    2635,
    527,
    1295,
    2194,
    25199,
    371,
    2865,
    2716,
    1319,
    2007,
    390,
    392,
    2317,
    958,
    1972,
    5031,
    1650,
    90,
    1693,
    1177,
    1543,
    5376,
    351,
    2613,
    11241,
    1003,
    3359,
    5784,
    2518,
    940,
    915,
    4528,
    3832,
    841,
    547,
    488,
    12429,
    1854,
    16384,
    10263,
    3240,
    2798,
    5997,
    690,
    16384,
    1789,
    921,
    2787,
    6644,
    659,
    3727,
    908,
    1144,
    11783,
    4628,
    589,
    2469,
    19097,
    786,
    525,
    532,
    331,
    5354,
    1531,
    2379,
    7302,
    522,
    709,
    2644,
    268,
    1057,
    722,
    2871,
    1250,
    16384,
    5606,
    781,
    1304,
    1363,
    16384,
    379,
    2499,
    201,
    1983,
    16331,
    1274,
    6734,
    1858,
    1255,
    1515,
    1047,
    1003,
    462,
    10482,
    1296,
    10888,
    17651,
    6827,
    1478,
    11684,
    267,
    3088,
    14656,
    768,
    1207,
    571,
    32299,
    2143,
    4339,
    3665,
    817,
    2559,
    1392,
    601,
    1737,
    16384,
    16384,
    2330,
    13531,
    1900,
    1884,
    234,
    694,
    843,
    2249,
    12999,
    1056,
    12711,
    1664,
    437,
    1018,
    1313,
    505,
    1389,
    797,
    7776,
    1528,
    1622,
    3199,
    3687,
    295,
    1320,
    735,
    1703,
    14074,
    2375,
    679,
    1070,
    1042,
    6573,
    2995,
    2998,
    1458,
    1308,
    3703,
    4699,
    2031,
    7015,
    8115,
    11979,
    3208,
    1906,
    3062,
    19075,
    725,
    336,
    6746,
    666,
    939,
    2502,
    939,
    346,
    2074,
    845,
    44395,
    1145,
    937,
    1586,
    6357,
    5424,
    600,
    16384,
    25849,
    562,
    5308,
    1342,
    16384,
    5075,
    1605,
    11303,
    5084,
    517,
    5309,
    1132,
    1823,
    2308,
    759,
    460,
    2994,
    433,
    117,
    796,
    2876,
    1484,
    8053,
    2445,
    860,
    1232,
    411,
    6597,
    437,
    726,
    29833,
    2003,
    382,
    737,
    2674,
    6923,
    2933,
    21365,
    358,
    1013,
    12922,
    6561,
    5315,
    2325,
    7518,
    16384,
    1039,
    863,
    10618,
    2588,
    2260,
    3076,
    972,
    640,
    16019,
    7867,
    411,
    3458,
    1081,
    1354,
    4524,
    5056,
    2913,
    254,
    11249,
    8444,
    2209,
    2020,
    14495,
    5664,
    1934,
    1448,
    1615,
    7295,
    6272,
    338,
    3331,
    5169,
    381,
    1991,
    282,
    2304,
    30370,
    943,
    1020,
    916,
    9864,
    3601,
    1287,
    715,
    11898,
    644,
    5827,
    16384,
    6364,
    2878,
    15115,
    1279,
    13627,
    1060,
    11127,
    575,
    1442,
    303,
    2380,
    497,
    12475,
    3144,
    3608,
    2726,
    600,
    2290,
    15074,
    16907,
    652,
    260,
    911,
    287,
    4981,
    4718,
    762,
    3926,
    1059,
    1900,
    3008,
    1226,
    3165,
    312,
    2273,
    1922,
    210,
    272,
    258,
    3151,
    2290,
    2700,
    464,
    6994,
    5890,
    859,
    7285,
    1516,
    283,
    331,
    4478,
    17539,
    5267,
    1813,
    2682,
    16384,
    27022,
    1241,
    3018,
    3442,
    4617,
    2160,
    1161,
    1990,
    1178,
    12729,
    7822,
    1597,
    3200,
    1665,
    7424,
    6334,
    3734,
    5210,
    4132,
    7269,
    3584,
    2361,
    1058,
    354,
    1937,
    884,
    2134,
    7006,
    2416,
    1265,
    781,
    414,
    8656,
    283,
    714,
    156,
    1411,
    1371,
    1280,
    25253,
    809,
    731,
    14759,
    1383,
    5423,
    795,
    11778,
    10568,
    2576,
    596,
    2198,
    2104,
    2600,
    681,
    374,
    2388,
    1975,
    2002,
    6182,
    394,
    6073,
    882,
    1293,
    805,
    2205,
    245,
    1069,
    1133,
    19302,
    2358,
    4294,
    2011,
    630,
    5420,
    788,
    1120,
    2441,
    5653,
    1601,
    10525,
    1065,
    4155,
    2006,
    10952,
    2529,
    11416,
    6615,
    1826,
    10977,
    7671,
    1387,
    3866,
    11684,
    796,
    14641,
    840,
    1050,
    305,
    12003,
    3095,
    2595,
    5840,
    463,
    1101,
    1806,
    363,
    8455,
    13908,
    1400,
    2591,
    873,
    2099,
    16384,
    3886,
    12520,
    1352,
    874,
    2809,
    456,
    157,
    34251,
    4045,
    5000,
    2061,
    16384,
    1815,
    1086,
    73,
    1897,
    4989,
    2013,
    6881,
    16384,
    1651,
    7741,
    349,
    6213,
    2142,
    1566,
    1669,
    1682,
    1422,
    2486,
    1649,
    560,
    3161,
    1023,
    6328,
    1925,
    2297,
    209,
    501,
    5617,
    891,
    1112,
    1471,
    8956,
    3015,
    8444,
    1488,
    2526,
    531,
    10858,
    2251,
    2097,
    2899,
    9813,
    1578,
    12448,
    783,
    3179,
    38531,
    1165,
    2616,
    5246,
    1716,
    383,
    625,
    3785,
    5327,
    4736,
    7052,
    2162,
    2512,
    14564,
    2832,
    3920,
    1052,
    4057,
    271,
    4565,
    2780,
    16384,
    9455,
    10484,
    4910,
    4069,
    544,
    1382,
    14904,
    402,
    1217,
    419,
    9858,
    1072,
    10926,
    1267,
    1147,
    8154,
    5612,
    100,
    3282,
    3866,
    2516,
    969,
    20860,
    1289,
    2521,
    1599,
    11488,
    2392,
    11363,
    2746,
    1086,
    2892,
    35330,
    15316,
    7542,
    3608,
    658,
    444,
    1599,
    424,
    842,
    1657,
    1173,
    1393,
    463,
    2577,
    5979,
    1093,
    4439,
    16384,
    1806,
    4090,
    2481,
    5805,
    301,
    29251,
    2339,
    5301,
    2306,
    320,
    807,
    833,
    3065,
    864,
    944,
    6589,
    8680,
    1268,
    3188,
    252,
    2485,
    7398,
    3566,
    28391,
    21218,
    980,
    19712,
    362,
    11748,
    3716,
    1785,
    1010,
    1849,
    4805,
    6078,
    739,
    684,
    16384,
    6976,
    4356,
    1715,
    5521,
    492,
    10774,
    1921,
    4769,
    4067,
    39029,
    1647,
    7659,
    6180,
    284,
    310,
    705,
    747,
    583,
    2695,
    1199,
    25463,
    9878,
    1936,
    12731,
    2052,
    1238,
    2145,
    5649,
    15017,
    1588,
    2771,
    2979,
    1332,
    3420,
    2755,
    1042,
    16384,
    3240,
    1388,
    1065,
    6200,
    2235,
    16384,
    1237,
    509,
    1938,
    2422,
    194,
    12497,
    1332,
    1331,
    998,
    455,
    4600,
    1051,
    648,
    866,
    1188,
    2251,
    4167,
    700,
    1179,
    454,
    64,
    13644,
    12859,
    3548,
    1143,
    2221,
    548,
    1746,
    1248,
    782,
    3121,
    14905,
    12781,
    1845,
    5284,
    5161,
    980,
    4057,
    409,
    824,
    2598,
    1266,
    11865,
    4075,
    1312,
    1190,
    2454,
    625,
    3847,
    440,
    3965,
    1667,
    3152,
    655,
    2616,
    16384,
    1208,
    2833,
    10468,
    4618,
    3213,
    655,
    490,
    2119,
    1200,
    10290,
    2505,
    949,
    3440,
    15435,
    658,
    1192,
    1892,
    7005,
    541,
    619,
    184,
    1231,
    4222,
    6449,
    2959,
    1031,
    4346,
    18664,
    241,
    517,
    10541,
    2848,
    440,
    4924,
    6422,
    1238,
    15313,
    64,
    5830,
    11162,
    579,
    2725,
    1377,
    1785,
    1252,
    461,
    15985,
    11016,
    2242,
    658,
    638,
    3145,
    3565,
    5248,
    6728,
    1585,
    1225,
    4543,
    991,
    5213,
    3016,
    7119,
    4177,
    626,
    771,
    4472,
    16281,
    466,
    2049,
    2723,
    373,
    3381,
    1803,
    199,
    11895,
    773,
    487,
    3172,
    3689,
    10553,
    1306,
    1974,
    894,
    472,
    6548,
    1784,
    1635,
    221,
    1642,
    790,
    1649,
    1570,
    11086,
    5612,
    3267,
    6904,
    465,
    978,
    1911,
    1277,
    1831,
    544,
    908,
    328,
    750,
    4288,
    1578,
    10470,
    1037,
    2277,
    526,
    16384,
    1641,
    4977,
    3148,
    1051,
    462,
    2372,
    1083,
    1684,
    4090,
    2177,
    3025,
    2356,
    397,
    9280,
    7284,
    572,
    3483,
    964,
    878,
    26350,
    13662,
    6398,
    3888,
    563,
    16384,
    5224,
    150,
    3100,
    1167,
    3136,
    3104,
    727,
    14890,
    2489,
    5627,
    8835,
    887,
    732,
    6864,
    9247,
    439,
    16384,
    1542,
    4276,
    11067,
    686,
    772,
    201,
    563,
    391,
    2088,
    9012,
    1939,
    2900,
    4702,
    4220,
    1307,
    399,
    1245,
    2141,
    766,
    2975,
    1448,
    3960,
    825,
    724,
    18308,
    2062,
    3671,
    596,
    1319,
    4231,
    11183,
    900,
    826,
    1948,
    373,
    1933,
    1860,
    4434,
    9471,
    3739,
    4517,
    2116,
    1345,
    222,
    753,
    2537,
    12191,
    2610,
    1285,
    649,
    1191,
    4454,
    5292,
    237,
    1695,
    5460,
    1504,
    2252,
    385,
    1178,
    14937,
    839,
    409,
    2355,
    4757,
    7429,
    1130,
    2552,
    3636,
    4081,
    1094,
    13147,
    14049,
    4353,
    1950,
    1047,
    985,
    1048,
    1222,
    13626,
    4303,
    921,
    1015,
    3420,
    1774,
    418,
    713,
    1609,
    1061,
    4562,
    15621,
    689,
    657,
    12662,
    3400,
    317,
    3961,
    5151,
    311,
    934,
    9621,
    6043,
    2053,
    3468,
    16384,
    3474,
    521,
    43079,
    766,
    5962,
    4112,
    5315,
    10883,
    16801,
    1850,
    16384,
    5848,
    2428,
    648,
    2247,
    732,
    397,
    3767,
    2147,
    853,
    3085,
    1147,
    14150,
    1932,
    16113,
    817,
    3923,
    15185,
    2692,
    6975,
    295,
    4349,
    19086,
    4635,
    5189,
    167,
    1226,
    956,
    2362,
    236,
    602,
    6015,
    2427,
    603,
    1512,
    6688,
    10725,
    12997,
    1552,
    2454,
    3477,
    16340,
    3674,
    1047,
    27243,
    777,
    3228,
    19039,
    6535,
    35775,
    171,
    13425,
    1047,
    8581,
    5037,
    13336,
    4788,
    977,
    4014,
    15602,
    313,
    2917,
    4349,
    7426,
    12748,
    6047,
    297,
    1325,
    1708,
    1390,
    1811,
    12638,
    4490,
    671,
    2293,
    2165,
    2340,
    704,
    9312,
    1558,
    410,
    4824,
    1990,
    1796,
    14417,
    3087,
    1884,
    853,
    419,
    2790,
    376,
    6847,
    699,
    15705,
    678,
    446,
    854,
    1462,
    154,
    135,
    909,
    5280,
    477,
    1837,
    9639,
    466,
    1233,
    882,
    5330,
    3328,
    3941,
    16384,
    11997,
    20532,
    1600,
    405,
    303,
    1892,
    478,
    3975,
    13872,
    1052,
    362,
    16384,
    4604,
    915,
    25538,
    8564,
    720,
    16272,
    642,
    1644,
    28267,
    122,
    25199,
    2195,
    296,
    328,
    7043,
    1693,
    11502,
    4024,
    1857,
    2070,
    1085,
    2225,
    1868,
    5020,
    1669,
    4943,
    10699,
    2437,
    471,
    1674,
    245,
    529,
    4462,
    3443,
    1991,
    10380,
    885,
    1136,
    5777,
    3375,
    442,
    2486,
    12405,
    12253,
    289,
    844,
    5831,
    13413,
    16384,
    546,
    2063,
    3697,
    5266,
    4388,
    1201,
    5456,
    871,
    1109,
    3302,
    10514,
    1554,
    13613,
    7560,
    497,
    15093,
    676,
    4044,
    5537,
    4738,
    2564,
    2110,
    324,
    846,
    4434,
    4145,
    637,
    2926,
    2636,
    13711,
    1261,
    1127,
    723,
    665,
    3018,
    920,
    6517,
    1002,
    10801,
    3163,
    2655,
    563,
    2869,
    2100,
    4004,
    10054,
    403,
    2704,
    2330,
    2613,
    6040,
    710,
    145,
    3369,
    3122,
    1182,
    2410,
    631,
    196,
    310,
    2093,
    11419,
    1301,
    4891,
    1039,
    1879,
    4799,
    2313,
    1101,
    771,
    1804,
    7334,
    355,
    5142,
    2729,
    10393,
    1958,
    1596,
    704,
    1103,
    13880,
    204,
    2359,
    6303,
    12048,
    588,
    16384,
    10710,
    9564,
    841,
    1076,
    23266,
    2465,
    18869,
    2387,
    797,
    28881,
    2914,
    567,
    1899,
    2732,
    33473,
    2220,
    710,
    1411,
    1930,
    1725,
    263,
    1387,
    932,
    1500,
    16384,
    799,
    9717,
    7162,
    4238,
    2005,
    4104,
    1218,
    1095,
    877,
    2700,
    1586,
    1535,
    9530,
    920,
    14318,
    1125,
    346,
    416,
    1163,
    3994,
    4047,
    1533,
    3945,
    2173,
    1498,
    5116,
    7381,
    6109,
    1425,
    3731,
    192,
    2476,
    2972,
    2361,
    1564,
    8393,
    1803,
    1713,
    1532,
    6232,
    1448,
    798,
    15728,
    13075,
    1435,
    3933,
    1471,
    1473,
    10066,
    524,
    1770,
    21605,
    11314,
    3248,
    813,
    4537,
    3292,
    9303,
    16384,
    11981,
    1297,
    1677,
    7141,
    1143,
    1712,
    4014,
    6487,
    2553,
    13934,
    2902,
    1417,
    682,
    2051,
    6853,
    576,
    11486,
    5864,
    5583,
    815,
    1515,
    1311,
    1998,
    1866,
    1409,
    17143,
    1583,
    2681,
    324,
    3412,
    12926,
    15127,
    705,
    6304,
    16384,
    1530,
    4416,
    16384,
    6206,
    11001,
    598,
    2589,
    19154,
    27011,
    4762,
    2372,
    45363,
    18333,
    3031,
    546,
    4604,
    3671,
    3607,
    1457,
    12062,
    3745,
    838,
    366,
    859,
    147,
    28176,
    2302,
    529,
    1096,
    11032,
    2248,
    10817,
    1630,
    1645,
    177,
    4456,
    6705,
    1371,
    1420,
    4887,
    888,
    845,
    2832,
    1025,
    3913,
    726,
    10513,
    4113,
    2148,
    764,
    624,
    744,
    413,
    8729,
    21079,
    6487,
    13534,
    8410,
    3276,
    1271,
    4556,
    2520,
    1500,
    1133,
    6549,
    10149,
    1476,
    1235,
    4306,
    993,
    3881,
    13502,
    1261,
    2052,
    16384,
    1325,
    2333,
    709,
    922,
    438,
    16384,
    3747,
    1899,
    624,
    268,
    2606,
    1703,
    3470,
    4279,
    6084,
    1907,
    3960,
    1276,
    783,
    1377,
    4655,
    3893,
    1970,
    2848,
    11710,
    275,
    502,
    1969,
    11851,
    1576,
    12446,
    73,
    153,
    1070,
    16384,
    1998,
    169,
    1344,
    3941,
    5758,
    3352,
    1541,
    4216,
    7108,
    1085,
    1744,
    1720,
    306,
    501,
    155,
    2192,
    1440,
    1376,
    1923,
    3537,
    953,
    7266,
    346,
    1968,
    1793,
    5089,
    1159,
    6474,
    1544,
    1786,
    1695,
    4306,
    5107,
    1027,
    424,
    679,
    2396,
    1961,
    2333,
    899,
    4794,
    1645,
    331,
    2370,
    3648,
    2340,
    2305,
    2565,
    626,
    12454,
    1395,
    10031,
    1086,
    5786,
    2602,
    2231,
    711,
    7528,
    3168,
    1809,
    7541,
    2202,
    16384,
    6114,
    1388,
    8903,
    2136,
    16384,
    2185,
    1831,
    2215,
    3100,
    10984,
    2757,
    1776,
    1161,
    4742,
    1085,
    552,
    708,
    1198,
    1742,
    6364,
    1058,
    7456,
    329,
    16384,
    348,
    2484,
    1572,
    842,
    4205,
    6392,
    1653,
    12251,
    2144,
    620,
    1831,
    1118,
    3315,
    11765,
    1841,
    605,
    6478,
    1351,
    616,
    16384,
    422,
    3491,
    184,
    2805,
    2457,
    1513,
    445,
    777,
    822,
    25425,
    704,
    9687,
    3072,
    8637,
    673,
    16671,
    295,
    539,
    1450,
    1258,
    14116,
    2250,
    1492,
    3312,
    2058,
    12926,
    1002,
    9310,
    8384,
    2651,
    6132,
    12174,
    1229,
    2447,
    2548,
    804,
    3265,
    567,
    2236,
    24530,
    3205,
    23480,
    1126,
    15858,
    1722,
    20875,
    956,
    2100,
    3034,
    1184,
    1518,
    1663,
    2766,
    6929,
    26679,
    4849,
    4841,
    2644,
    6567,
    2228,
    2249,
    1650,
    1484,
    1585,
    258,
    3867,
    1401,
    397,
    5810,
    683,
    5143,
    883,
    5737,
    16136,
    345,
    1509,
    5724,
    2718,
    2724,
    5470,
    839,
    487,
    911,
    8663,
    862,
    369,
    7132,
    1137,
    1219,
    431,
    3581,
    3732,
    1090,
    537,
    11825,
    3732,
    16384,
    3117,
    3901,
    2347,
    2935,
    1546,
    2761,
    827,
    510,
    2258,
    2226,
    21272,
    1089,
    1359,
    16746,
    2379,
    1661,
    346,
    2690,
    3576,
    16384,
    3264,
    5522,
    5580,
    272,
    1742,
    1658,
    12898,
    5126,
    11960,
    722,
    1683,
    10248,
    847,
    194,
    1577,
    593,
    236,
    5388,
    12076,
    3575,
    3157,
    3221,
    1834,
    3275,
    1729,
    874,
    8147,
    4553,
    16384,
    812,
    407,
    8742,
    12838,
    8888,
    16384,
    441,
    20585,
    314,
    1693,
    904,
    236,
    2230,
    8184,
    42852,
    7862,
    4300,
    1861,
    624,
    1192,
    16384,
    1302,
    3739,
    1477,
    596,
    4267,
    12260,
    11025,
    11730,
    2338,
    1992,
    3570,
    1564,
    12911,
    346,
    11204,
    2110,
    343,
    270,
    14028,
    6523,
    3046,
    397,
    1493,
    642,
    1338,
    1574,
    899,
    11825,
    4503,
    2628,
    3653,
    6963,
    6722,
    12450,
    5532,
    3311,
    1269,
    483,
    3824,
    1449,
    16384,
    1147,
    2719,
    2732,
    395,
    861,
    224,
    2702,
    2453,
    4528,
    899,
    622,
    1402,
    3148,
    607,
    3041,
    3426,
    4926,
    1348,
    13601,
    7718,
    925,
    1915,
    16384,
    2791,
    5639,
    11274,
    6048,
    3405,
    1569,
    1534,
    1554,
    4119,
    2052,
    1961,
    2183,
    2702,
    16191,
    721,
    6544,
    1250,
    10017,
    1697,
    450,
    10253,
    1720,
    1036,
    8323,
    807,
    1662,
    1028,
    11421,
    456,
    3155,
    7454,
    9400,
    2427,
    10986,
    701,
    1089,
    1104,
    13719,
    195,
    5672,
    7741,
    1455,
    7974,
    3756,
    682,
    934,
    1143,
    2868,
    5623,
    4184,
    1009,
    11911,
    1017,
    11390,
    1309,
    1855,
    1463,
    534,
    34868,
    4934,
    1656,
    325,
    1588,
    914,
    2116,
    5778,
    924,
    1684,
    2102,
    415,
    2946,
    1445,
    697,
    454,
    1906,
    4927,
    3429,
    546,
    1373,
    3309,
    7421,
    10492,
    2533,
    6527,
    428,
    3496,
    11518,
    1909,
    34732,
    8876,
    3975,
    12620,
    1452,
    1553,
    15544,
    3065,
    509,
    2058,
    16384,
    147,
    8556,
    1141,
    3573,
    4952,
    453,
    359,
    767,
    1794,
    313,
    4112,
    3833,
    2774,
    2558,
    468,
    1411,
    2091,
    4679,
    2307,
    12504,
    3971,
    2121,
    3286,
    11804,
    578,
    1344,
    11930,
    11638,
    697,
    3857,
    1208,
    3913,
    1204,
    498,
    7464,
    14447,
    2558,
    3552,
    13499,
    20693,
    7197,
    4378,
    8535,
    2813,
    2776,
    3903,
    452,
    764,
    8929,
    9534,
    3208,
    701,
    7502,
    2181,
    551,
    3950,
    5333,
    5238,
    1421,
    1128,
    474,
    10631,
    2525,
    11194,
    1502,
    13129,
    16343,
    2391,
    17933,
    1026,
    463,
    983,
    16313,
    1075,
    6572,
    233,
    16384,
    41039,
    5450,
    7813,
    1063,
    12157,
    1482,
    14750,
    368,
    1787,
    4542,
    5553,
    2571,
    3152,
    2809,
    887,
    368,
    1076,
    343,
    4948,
    16384,
    6081,
    1466,
    2035,
    18192,
    1031,
    16149,
    183,
    585,
    1068,
    4023,
    858,
    3945,
    2444,
    15448,
    612,
    216,
    136,
    1696,
    7088,
    12350,
    4122,
    1892,
    2594,
    378,
    16384,
    3397,
    469,
    230,
    3591,
    1771,
    1463,
    9673,
    1069,
    784,
    4433,
    3891,
    483,
    4761,
    16384,
    1381,
    1024,
    897,
    5097,
    5309,
    588,
    6985,
    8128,
    2762,
    180,
    4777,
    334,
    4508,
    6097,
    5733,
    112,
    897,
    4522,
    1825,
    373,
    1233,
    1986,
    303,
    13044,
    4877,
    2355,
    889,
    1416,
    973,
    671,
    2353,
    6725,
    12276,
    1269,
    2432,
    4721,
    1122,
    437,
    5653,
    446,
    3599,
    12640,
    745,
    9190,
    5985,
    18183,
    3242,
    3371,
    677,
    1076,
    328,
    2755,
    9666,
    1310,
    17886,
    1112,
    1189,
    1374,
    399,
    6991,
    1987,
    25148,
    2786,
    823,
    1410,
    348,
    4757,
    2323,
    842,
    380,
    1059,
    591,
    2318,
    627,
    16384,
    5906,
    1325,
    389,
    1767,
    12163,
    6417,
    4771,
    9833,
    1083,
    4847,
    9158,
    1237,
    891,
    8803,
    2344,
    3190,
    1813,
    506,
    6734,
    214,
    5989,
    1469,
    1625,
    1112,
    3567,
    3132,
    11762,
    1631,
    1635,
    11857,
    2240,
    1219,
    7120,
    9022,
    241,
    484,
    651,
    4137,
    749,
    3666,
    9031,
    4665,
    633,
    533,
    106,
    3425,
    502,
    2093,
    2575,
    473,
    5640,
    639,
    1039,
    22819,
    2108,
    961,
    16384,
    984,
    2452,
    1467,
    5299,
    2006,
    400,
    2486,
    3242,
    489,
    634,
    971,
    2651,
    1349,
    167,
    865,
    6965,
    4007,
    4102,
    1878,
    16384,
    1852,
    1549,
    1188,
    1288,
    19482,
    201,
    542,
    9328,
    851,
    578,
    1662,
    877,
    2757,
    535,
    10394,
    472,
    10770,
    2124,
    2620,
    1869,
    1985,
    1850,
    8402,
    11652,
    5767,
    892,
    1992,
    1731,
    33719,
    1048,
    14961,
    1682,
    1952,
    197,
    777,
    6776,
    1510,
    1531,
    3741,
    3621,
    13537,
    4001,
    1231,
    10012,
    17678,
    5755,
    740,
    4174,
    13958,
    1651,
    2023,
    721,
    621,
    2023,
    4754,
    1438,
    6742,
    5423,
    1335,
    4460,
    1733,
    2213,
    6510,
    16384,
    430,
    1389,
    7160,
    1527,
    1246,
    12512,
    1868,
    4750,
    1421,
    359,
    1138,
    4239,
    2470,
    830,
    16384,
    2274,
    668,
    8345,
    5476,
    1079,
    4103,
    4767,
    3322,
    2305,
    285,
    1400,
    1885,
    1051,
    1350,
    4322,
    1154,
    309,
    1003,
    1108,
    1672,
    36726,
    19209,
    39633,
    4710,
    1615,
    3649,
    2709,
    486,
    1907,
    11578,
    339,
    5668,
    2227,
    379,
    3754,
    6025,
    1868,
    12422,
    14833,
    1026,
    14925,
    12160,
    465,
    2289,
    2150,
    8289,
    7229,
    10521,
    10721,
    9089,
    3732,
    264,
    8470,
    591,
    4676,
    1048,
    1593,
    3255,
    1586,
    404,
    2797,
    562,
    3636,
    503,
    2329,
    2982,
    10751,
    237,
    2829,
    499,
    3382,
    1766,
    4656,
    9488,
    1378,
    1969,
    5402,
    1116,
    3113,
    5027,
    611,
    373,
    2424,
    23912,
    381,
    10679,
    1336,
    1654,
    26058,
    2332,
    780,
    1971,
    1545,
    1466,
    26847,
    972,
    10460,
    2631,
    4035,
    1724,
    515,
    2252,
    16384,
    3725,
    321,
    701,
    2214,
    3626,
    929,
    16384,
    3610,
    2881,
    7963,
    2618,
    229,
    1295,
    989,
    1378,
    2265,
    4280,
    853,
    861,
    1834,
    3763,
    3663,
    3867,
    6771,
    1343,
    6850,
    2887,
    2377,
    14495,
    1407,
    860,
    10013,
    1901,
    6740,
    3097,
    290,
    2722,
    3972,
    1844,
    3966,
    7415,
    10924,
    4775,
    789,
    2290,
    911,
    1204,
    12997,
    1884,
    2550,
    16384,
    2623,
    14200,
    10487,
    4650,
    1351,
    2079,
    908,
    4281,
    848,
    33537,
    2546,
    3224,
    3559,
    1186,
    447,
    166,
    747,
    970,
    932,
    15577,
    3316,
    1476,
    1718,
    841,
    12027,
    1474,
    150,
    849,
    463,
    427,
    3324,
    16384,
    528,
    1260,
    842,
    184,
    7037,
    702,
    4631,
    8530,
    1750,
    626,
    96,
    4128,
    16384,
    1216,
    3332,
    1304,
    1025,
    10831,
    10072,
    28002,
    2301,
    2289,
    14676,
    1657,
    13235,
    4792,
    1076,
    15121,
    7993,
    2880,
    16384,
    2888,
    2613,
    40448,
    317,
    6200,
    1293,
    861,
    3715,
    15390,
    4342,
    2604,
    1004,
    2652,
    393,
    3240,
    1135,
    1918,
    7765,
    2067,
    31615,
    5913,
    1900,
    7507,
    879,
    1439,
    678,
    466,
    2391,
    1004,
    14428,
    2321,
    623,
    1827,
    1122,
    15094,
    796,
    448,
    10607,
    2368,
    760,
    1085,
    491,
    786,
    2902,
    5300,
    3092,
    5763,
    13586,
    4061,
    3118,
    1951,
    136,
    1665,
    5511,
    823,
    14756,
    4960,
    10618,
    13902,
    198,
    5727,
    10494,
    7338,
    5862,
    1063,
    676,
    669,
    2407,
    717,
    699,
    1018,
    2861,
    827,
    6332,
    845,
    8993,
    649,
    1601,
    2236,
    692,
    890,
    2621,
    1196,
    1070,
    1263,
    2443,
    793,
    12453,
    251,
    11807,
    1696,
    1350,
    427,
    3951,
    317,
    3548,
    1784,
    1104,
    27729,
    903,
    150,
    3939,
    2325,
    3511,
    533,
    1036,
    3845,
    12170,
    6705,
    3188,
    10782,
    2328,
    13441,
    1180,
    982,
    2027,
    1917,
    1528,
    5257,
    3059,
    1147,
    18376,
    1068,
    3819,
    782,
    1102,
    1357,
    15446,
    2248,
    2457,
    12341,
    2567,
    643,
    28680,
    21105,
    1190,
    10559,
    8277,
    2449,
    2163,
    1637,
    2853,
    3351,
    1510,
    555,
    2650,
    2427,
    951,
    1882,
    813,
    3877,
    2375,
    2448,
    4434,
    16384,
    2086,
    3817,
    6600,
    1803,
    5362,
    7943,
    2025,
    16384,
    292,
    854,
    554,
    7071,
    2714,
    876,
    4810,
    1449,
    16384,
    1147,
    6620,
    660,
    3615,
    1218,
    477,
    209,
    1460,
    5884,
    2086,
    420,
    9430,
    3188,
    4724,
    3267,
    16384,
    2437,
    828,
    6553,
    181,
    2743,
    1317,
    22726,
    2102,
    870,
    2328,
    473,
    15247,
    1551,
    4263,
    1000,
    3240,
    1822,
    16384,
    395,
    1268,
    1141,
    515,
    1734,
    5110,
    897,
    11330,
    35310,
    3658,
    1130,
    10415,
    1222,
    16384,
    710,
    628,
    2500,
    2013,
    938,
    1964,
    13267,
    3010,
    3248,
    607,
    4295,
    12365,
    10828,
    6307,
    1938,
    1344,
    1496,
    3574,
    2443,
    4207,
    268,
    6928,
    1821,
    3678,
    9293,
    9016,
    441,
    6103,
    2673,
    6214,
    352,
    671,
    2650,
    742,
    1305,
    9421,
    17509,
    1622,
    5697,
    10402,
    16164,
    33246,
    511,
    8253,
    777,
    477,
    7465,
    1379,
    5044,
    27560,
    692,
    343,
    10775,
    4285,
    10016,
    1639,
    7848,
    13145,
    1519,
    1486,
    2213,
    1917,
    107,
    702,
    16024,
    2760,
    2650,
    1434,
    213,
    2287,
    2527,
    736,
    2945,
    2649,
    1102,
    1744,
    5017,
    2758,
    717,
    11202,
    44039,
    813,
    906,
    995,
    3805,
    4212,
    3819,
    4374,
    486,
    1078,
    23523,
    5022,
    6107,
    6666,
    1286,
    10444,
    3056,
    10597,
    243,
    4521,
    1377,
    11653,
    3997,
    21803,
    784,
    3548,
    3118,
    12274,
    2628,
    4734,
    29515,
    1211,
    685,
    1889,
    13071,
    299,
    1422,
    1900,
    1145,
    9190,
    17677,
    6085,
    10857,
    1779,
    3555,
    2338,
    811,
    16079,
    196,
    6503,
    17127,
    2966,
    5240,
    1937,
    638,
    8817,
    759,
    16384,
    1354,
    15237,
    2012,
    12745,
    1058,
    1392,
    12330,
    413,
    943,
    753,
    1777,
    2151,
    15274,
    2752,
    5703,
    4806,
    1657,
    16885,
    767,
    8556,
    2439,
    136,
    1363,
    736,
    5925,
    13173,
    948,
    1145,
    289,
    399,
    3588,
    828,
    7223,
    10916,
    2142,
    1381,
    565,
    2338,
    979,
    2014,
    3227,
    752,
    16384,
    1048,
    4751,
    1835,
    2207,
    2420,
    14122,
    19479,
    2051,
    2402,
    1312,
    1927,
    487,
    1452,
    1038,
    2095,
    3018,
    2025,
    13902,
    452,
    7383,
    3431,
    2000,
    249,
    18634,
    1326,
    2577,
    421,
    1051,
    1777,
    9079,
    1251,
    245,
    19356,
    10350,
    1440,
    879,
    2848,
    424,
    2346,
    1685,
    1504,
    1589,
    951,
    1132,
    882,
    2677,
    2157,
    1170,
    3557,
    3362,
    13885,
    5372,
    1958,
    629,
    2645,
    541,
    4126,
    1415,
    10449,
    604,
    1888,
    2506,
    1003,
    9240,
    16384,
    6228,
    820,
    602,
    1419,
    1669,
    1204,
    460,
    3522,
    1165,
    2789,
    16384,
    33192,
    1853,
    2128,
    2007,
    1859,
    2584,
    570,
    1975,
    1126,
    602,
    1365,
    262,
    114,
    13866,
    1173,
    2509,
    4584,
    8913,
    4297,
    4231,
    1973,
    8771,
    800,
    1275,
    462,
    2944,
    1105,
    3080,
    2738,
    16384,
    726,
    2266,
    1717,
    8973,
    1498,
    485,
    323,
    2081,
    8081,
    1706,
    4792,
    1845,
    883,
    7694,
    176,
    1114,
    2404,
    688,
    2809,
    5620,
    1669,
    1403,
    1282,
    962,
    1758,
    1263,
    1584,
    16384,
    813,
    2572,
    2318,
    16384,
    677,
    1912,
    915,
    2636,
    6069,
    878,
    11574,
    5047,
    1991,
    2128,
    836,
    10537,
    1246,
    1466,
    1438,
    5816,
    7204,
    1631,
    2012,
    3488,
    2478,
    4739,
    6695,
    990,
    1558,
    1887,
    1164,
    168,
    830,
    820,
    2603,
    3233,
    2786,
    2709,
    18521,
    399,
    4823,
    393,
    16384,
    2755,
    2267,
    3066,
    1202,
    4421,
    1725,
    794,
    3256,
    16384,
    585,
    1555,
    8646,
    1162,
    8910,
    6589,
    2032,
    1651,
    5621,
    849,
    4614,
    9338,
    7745,
    6010,
    681,
    4212,
    1851,
    624,
    1712,
    412,
    1640,
    1745,
    3611,
    6451,
    427,
    19193,
    10201,
    5026,
    938,
    11632,
    1903,
    1987,
    4241,
    185,
    1608,
    515,
    12849,
    3584,
    2942,
    16384,
    1701,
    5355,
    400,
    2493,
    16384,
    3455,
    873,
    683,
    353,
    3255,
    5152,
    2298,
    818,
    2607,
    938,
    7652,
    15994,
    756,
    1096,
    3057,
    516,
    4161,
    1482,
    2186,
    1732,
    16384,
    1658,
    5063,
    16384,
    3191,
    15250,
    2224,
    4800,
    1045,
    587,
    688,
    1945,
    2736,
    584,
    438,
    253,
    16384,
    19128,
    188,
    1978,
    1677,
    439,
    7152,
    1208,
    2091,
    1461,
    2102,
    6119,
    3890,
    1215,
    11305,
    10874,
    399,
    586,
    1020,
    10695,
    10678,
    1065,
    1924,
    1561,
    1683,
    616,
    835,
    1489,
    13025,
    8250,
    5471,
    2979,
    17453,
    14163,
    6835,
    1331,
    979,
    2452,
    2098,
    932,
    10335,
    972,
    2630,
    531,
    1508,
    6571,
    2093,
    1047,
    234,
    1373,
    1352,
    4476,
    8556,
    522,
    4826,
    22504,
    7074,
    1235,
    3688,
    10978,
    36403,
    2348,
    11329,
    4708,
    16384,
    27737,
    6592,
    4404,
    2112,
    656,
    2555,
    16384,
    7901,
    3419,
    1160,
    424,
    1605,
    735,
    1350,
    6720,
    7690,
    6607,
    3635,
    1115,
    3238,
    299,
    6730,
    596,
    8669,
    995,
    16384,
    1494,
    1759,
    4622,
    456,
    2541,
    661,
    4207,
    3462,
    2557,
    7783,
    29694,
    178,
    872,
    695,
    4145,
    12650,
    985,
    816,
    1372,
    3178,
    5315,
    2082,
    1227,
    2086,
    527,
    1207,
    9618,
    6499,
    16384,
    13209,
    16384,
    1144,
    974,
    2770,
    3430,
    20290,
    1834,
    16384,
    534,
    11458,
    2525,
    1795,
    2113,
    16384,
    12567,
    4054,
    860,
    2645,
    294,
    785,
    4558,
    710,
    1016,
    2557,
    78,
    2524,
    2821,
    3731,
    12719,
    1132,
    1593,
    841,
    1009,
    902,
    905,
    231,
    25127,
    4795,
    1374,
    1162,
    1238,
    2821,
    464,
    5440,
    202,
    6121,
    3194,
    207,
    2197,
    10435,
    537,
    1859,
    376,
    5190,
    4334,
    1967,
    120,
    1872,
    1642,
    953,
    402,
    791,
    1162,
    1779,
    4123,
    26986,
    2497,
    644,
    2489,
    4117,
    4270,
    5737,
    155,
    408,
    1296,
    4732,
    5051,
    1002,
    16384,
    257,
    1397,
    4277,
    1198,
    711,
    7582,
    1730,
    2523,
    7490,
    829,
    2497,
    1265,
    534,
    6470,
    3615,
    2732,
    2188,
    9384,
    16962,
    1455,
    403,
    2608,
    7678,
    783,
    10713,
    16511,
    4721,
    18460,
    3086,
    36901,
    900,
    1927,
    1189,
    48827,
    18106,
    2912,
    1794,
    1534,
    20212,
    178,
    4288,
    2063,
    16384,
    4405,
    2526,
    604,
    752,
    710,
    16384,
    804,
    374,
    5509,
    5220,
    1788,
    10009,
    2703,
    492,
    16384,
    174,
    1371,
    10031,
    16384,
    722,
    1984,
    673,
    10276,
    2720,
    1361,
    3676,
    366,
    501,
    2693,
    8260,
    1523,
    2912,
    18330,
    13924,
    5076,
    8674,
    12609,
    1539,
    1351,
    213,
    1227,
    4182,
    6586,
    2708,
    1759,
    11576,
    3141,
    18626,
    1701,
    2697,
    596,
    96,
    13540,
    677,
    5442,
    4131,
    1733,
    14925,
    25323,
    1356,
    11892,
    603,
    123,
    1349,
    5669,
    15982,
    1481,
    4312,
    849,
    541,
    1081,
    889,
    646,
    1668,
    7912,
    1409,
    22850,
    1105,
    1561,
    3688,
    4706,
    13949,
    1073,
    3491,
    1625,
    662,
    2002,
    175,
    1298,
    1811,
    36745,
    887,
    3381,
    1118,
    778,
    1973,
    1373,
    5509,
    3295,
    6778,
    24552,
    1110,
    1678,
    3030,
    11906,
    141,
    689,
    4343,
    2536,
    260,
    12669,
    39503,
    1106,
    1159,
    3911,
    10012,
    2728,
    19132,
    1204,
    917,
    1672,
    1328,
    7638,
    10185,
    604,
    206,
    732,
    224,
    5348,
    4875,
    46423,
    11304,
    4175,
    8442,
    2430,
    3095,
    11691,
    4519,
    141,
    4522,
    4681,
    14584,
    435,
    2457,
    1619,
    376,
    7853,
    3251,
    4552,
    265,
    598,
    1127,
    1181,
    193,
    836,
    1659,
    1301,
    2747,
    1357,
    453,
    1803,
    3000,
    12366,
    2359,
    16384,
    11988,
    1365,
    28720,
    2317,
    1711,
    1499,
    4908,
    2167,
    12177,
    2355,
    24845,
    338,
    10671,
    144,
    5477,
    36519,
    2445,
    15270,
    3099,
    5330,
    7743,
    1947,
    11747,
    2353,
    3235,
    3047,
    10982,
    898,
    5841,
    486,
    16384,
    2565,
    577,
    1466,
    17668,
    116,
    547,
    5815,
    36692,
    4215,
    4715,
    10576,
    2575,
    701,
    624,
    2020,
    8265,
    5053,
    642,
    437,
    1876,
    3136,
    4425,
    1672,
    19026,
    447,
    12907,
    1786,
    6596,
    1615,
    3513,
    2497,
    4722,
    1037,
    2473,
    1524,
    1444,
    6719,
    3248,
    546,
    11667,
    1321,
    2271,
    572,
    5822,
    2408,
    564,
    1081,
    16384,
    16384,
    2832,
    6287,
    9290,
    47790,
    4201,
    1588,
    2091,
    1337,
    45041,
    2071,
    19440,
    1019,
    7204,
    13791,
    1111,
    9020,
    1025,
    735,
    29362,
    4260,
    4140,
    5312,
    3636,
    898,
    938,
    2891,
    3419,
    12644,
    1771,
    15256,
    465,
    16384,
    9354,
    2862,
    2800,
    1525,
    813,
    1218,
    3124,
    16384,
    5292,
    409,
    666,
    5622,
    2059,
    643,
    695,
    539,
    13781,
    1900,
    33673,
    1371,
    1029,
    380,
    2831,
    8340,
    1369,
    2969,
    7059,
    4966,
    833,
    1402,
    2356,
    12844,
    7853,
    1767,
    250,
    1859,
    785,
    5471,
    758,
    11732,
    2162,
    1801,
    758,
    10565,
    619,
    68,
    300,
    2530,
    1837,
    324,
    258,
    11027,
    9777,
    3237,
    802,
    3262,
    802,
    161,
    22983,
    2597,
    5566,
    4501,
    1938,
    4634,
    800,
    423,
    24606,
    927,
    3895,
    5090,
    783,
    3236,
    3973,
    5225,
    3037,
    692,
    12862,
    629,
    523,
    10078,
    4162,
    2055,
    725,
    1106,
    12131,
    5387,
    3970,
    1987,
    5025,
    970,
    14935,
    3777,
    156,
    1048,
    1374,
    13270,
    1911,
    1996,
    5896,
    7240,
    208,
    3862,
    475,
    1783,
    10408,
    5401,
    16384,
    349,
    3452,
    8040,
    1635,
    990,
    18959,
    1354,
    1300,
    2186,
    1157,
    4040,
    561,
    904,
    1283,
    4522,
    4172,
    9684,
    1426,
    2995,
    8062,
    2147,
    5004,
    585,
    5076,
    31911,
    1199,
    566,
    26342,
    3614,
    2644,
    1806,
    6472,
    6592,
    793,
    1637,
    1297,
    2343,
    1873,
    32697,
    635,
    2768,
    15211,
    240,
    1706,
    541,
    2453,
    5741,
    2833,
    9884,
    3728,
    8438,
    2313,
    690,
    10505,
    12240,
    2509,
    5087,
    904,
    2277,
    871,
    5227,
    958,
    2899,
    540,
    1953,
    2969,
    6572,
    3238,
    4163,
    1616,
    1901,
    1207,
    11601,
    3624,
    3701,
    12206,
    8329,
    1549,
    13139,
    2626,
    886,
    2874,
    1875,
    1932,
    5627,
    1034,
    679,
    2509,
    972,
    2310,
    4764,
    443,
    5607,
    3405,
    2611,
    414,
    617,
    868,
    1184,
    14465,
    8310,
    3579,
    1252,
    1031,
    9087,
    43812,
    6189,
    806,
    2651,
    915,
    1690,
    1007,
    30933,
    1613,
    443,
    555,
    3209,
    2315,
    818,
    1117,
    3324,
    8472,
    18956,
    1600,
    2214,
    657,
    3161,
    3218,
    1603,
    1176,
    2117,
    3825,
    3169,
    9719,
    1587,
    2332,
    5064,
    1679,
    4772,
    3700,
    3963,
    978,
    11688,
    402,
    1677,
    5652,
    563,
    7367,
    723,
    389,
    8246,
    1147,
    718,
    9438,
    475,
    1120,
    2350,
    989,
    1212,
    11623,
    16427,
    8306,
    948,
    1924,
    2093,
    2133,
    2261,
    2414,
    2580,
    6050,
    7172,
    16384,
    22979,
    3779,
    371,
    709,
    10965,
    11431,
    18807,
    1674,
    3817,
    4559,
    607,
    2739,
    790,
    1222,
    908,
    10579,
    1340,
    480,
    2554,
    4326,
    8348,
    1571,
    1350,
    5572,
    549,
    5202,
    2941,
    3598,
    3800,
    16418,
    2502,
    576,
    18496,
    1053,
    7797,
    2747,
    661,
    32225,
    741,
    1134,
    21196,
    209,
    19300,
    1377,
    10448,
    1504,
    1685,
    12412,
    1802,
    14325,
    4702,
    16384,
    3813,
    594,
    4074,
    771,
    1406,
    2325,
    7879,
    2243,
    3860,
    2149,
    3945,
    1784,
    7715,
    412,
    864,
    587,
    13828,
    23504,
    3442,
    976,
    1919,
    1109,
    869,
    2231,
    473,
    603,
    320,
    924,
    1838,
    7157,
    796,
    11387,
    1737,
    3219,
    2407,
    21830,
    12570,
    9513,
    681,
    1259,
    3205,
    620,
    3072,
    1840,
    665,
    2918,
    5831,
    708,
    11647,
    2595,
    1735,
    3076,
    115,
    4688,
    2674,
    1209,
    2895,
    3119,
    622,
    2215,
    346,
    14693,
    10346,
    1428,
    426,
    1336,
    43769,
    2704,
    359,
    1331,
    4542,
    2866,
    2356,
    4181,
    1289,
    828,
    1086,
    4379,
    5322,
    2008,
    675,
    1642,
    5520,
    4385,
    649,
    2579,
    1685,
    1286,
    107,
    662,
    9211,
    1847,
    2328,
    1796,
    3113,
    3987,
    3732,
    7333,
    3309,
    13018,
    10526,
    2039,
    526,
    558,
    668,
    2441,
    500,
    1974,
    7649,
    3952,
    1597,
    1871,
    587,
    3001,
    2610,
    402,
    2502,
    2591,
    1588,
    812,
    1481,
    1096,
    482,
    7827,
    10351,
    1566,
    3497,
    364,
    14512,
    1866,
    728,
    47492,
    3914,
    2291,
    286,
    18891,
    1557,
    1528,
    696,
    1345,
    1641,
    259,
    3398,
    10811,
    1562,
    835,
    1978,
    1763,
    557,
    3691,
    535,
    8133,
    1099,
    24832,
    212,
    795,
    14595,
    4187,
    1266,
    2040,
    3975,
    1465,
    5231,
    1132,
    24587,
    2299,
    1713,
    360,
    3188,
    1674,
    8491,
    860,
    1709,
    567,
    3626,
    1869,
    2023,
    1250,
    767,
    2879,
    1563,
    2451,
    20615,
    3392,
    2183,
    1624,
    472,
    1812,
    10551,
    12401,
    1102,
    2233,
    5967,
    2039,
    21737,
    913,
    6729,
    4559,
    935,
    1014,
    762,
    9152,
    399,
    483,
    572,
    270,
    11583,
    352,
    1637,
    25793,
    764,
    3323,
    3747,
    1059,
    1318,
    1625,
    2480,
    2976,
    279,
    6874,
    16384,
    1502,
    3170,
    2984,
    1155,
    5108,
    4174,
    9929,
    2453,
    16384,
    2160,
    1576,
    2063,
    1811,
    2291,
    18772,
    1552,
    630,
    2407,
    1175,
    1463,
    506,
    10030,
    3270,
    927,
    2397,
    1380,
    1665,
    951,
    966,
    485,
    2546,
    405,
    1925,
    4921,
    849,
    17575,
    1115,
    1456,
    1817,
    2885,
    2780,
    5607,
    1195,
    16384,
    1422,
    12649,
    2574,
    2353,
    16384,
    1149,
    5599,
    3179,
    525,
    17633,
    653,
    4903,
    109,
    19946,
    941,
    438,
    1757,
    1151,
    8395,
    551,
    3197,
    1169,
    710,
    2472,
    1731,
    12089,
    506,
    3298,
    4751,
    6376,
    20532,
    1374,
    860,
    4770,
    1069,
    312,
    14394,
    1131,
    9171,
    1168,
    5705,
    1269,
    7942,
    5879,
    16384,
    3408,
    331,
    225,
    4409,
    2874,
    1304,
    11799,
    287,
    1437,
    1465,
    1163,
    10490,
    3913,
    3271,
    4876,
    1890,
    642,
    3791,
    12051,
    1308,
    450,
    3998,
    31213,
    2019,
    5128,
    14504,
    668,
    16384,
    1664,
    14105,
    1383,
    1554,
    1827,
    298,
    3254,
    920,
    10646,
    3310,
    3895,
    584,
    1258,
    1275,
    5472,
    2042,
    690,
    1991,
    589,
    7475,
    728,
    1397,
    971,
    38041,
    787,
    1954,
    6330,
    9166,
    2741,
    2950,
    3458,
    1529,
    1718,
    25659,
    496,
    3390,
    16384,
    1691,
    900,
    2085,
    1751,
    4929,
    659,
    10525,
    2107,
    2422,
    2512,
    242,
    15998,
    11784,
    3197,
    463,
    1699,
    449,
    1334,
    2467,
    9148,
    619,
    1483,
    16384,
    3868,
    8453,
    5850,
    1753,
    1224,
    597,
    1250,
    1478,
    11305,
    6615,
    75,
    561,
    1923,
    2675,
    1282,
    792,
    6210,
    3114,
    2394,
    1154,
    4962,
    16338,
    2702,
    4871,
    4600,
    906,
    437,
    12266,
    2191,
    754,
    3918,
    1670,
    2682,
    422,
    1225,
    498,
    10442,
    3913,
    1200,
    1498,
    15643,
    4204,
    1302,
    636,
    138,
    3296,
    13419,
    16384,
    4035,
    1413,
    1884,
    1774,
    747,
    1481,
    7272,
    11010,
    381,
    429,
    1888,
    1909,
    1542,
    491,
    2428,
    1605,
    5202,
    3154,
    12492,
    496,
    11443,
    2137,
    1644,
    1049,
    1233,
    2037,
    11959,
    11412,
    1399,
    16384,
    1017,
    5527,
    1562,
    5325,
    29693,
    2615,
    3027,
    16384,
    4006,
    2965,
    3276,
    1427,
    11848,
    3415,
    1430,
    402,
    11536,
    4234,
    1351,
    3077,
    1313,
    305,
    1675,
    16951,
    3935,
    16384,
    2826,
    7997,
    2875,
    478,
    3670,
    16096,
    342,
    2726,
    2458,
    592,
    1942,
    400,
    16384,
    2234,
    12923,
    1894,
    4612,
    4375,
    577,
    3064,
    1572,
    10317,
    1750,
    11229,
    1153,
    734,
    412,
    507,
    3427,
    1206,
    287,
    486,
    2454,
    12517,
    10242,
    473,
    1286,
    935,
    1460,
    3704,
    1564,
    2236,
    446,
    1090,
    5631,
    3160,
    11099,
    1201,
    1356,
    4279,
    454,
    429,
    503,
    7422,
    777,
    2619,
    7356,
    319,
    2847,
    2337,
    5305,
    9162,
    206,
    3165,
    315,
    16384,
    950,
    525,
    300,
    47884,
    486,
    4134,
    1759,
    2781,
    693,
    4216,
    3139,
    4656,
    258,
    1500,
    971,
    4898,
    592,
    1942,
    940,
    13557,
    2212,
    272,
    10003,
    16384,
    3506,
    5506,
    832,
    2371,
    11957,
    12738,
    1786,
    1939,
    1083,
    1382,
    3208,
    1413,
    3588,
    3391,
    4025,
    18356,
    7632,
    1815,
    2313,
    2118,
    11868,
    12567,
    6226,
    16038,
    583,
    5440,
    16384,
    8400,
    1142,
    4040,
    199,
    368,
    191,
    521,
    1888,
    1002,
    5428,
    14213,
    2298,
    14021,
    2543,
    1709,
    3355,
    18041,
    2587,
    2677,
    2285,
    1924,
    24132,
    2600,
    10186,
    2345,
    2431,
    759,
    2189,
    2551,
    1501,
    8323,
    325,
    3915,
    1397,
    2259,
    4549,
    3304,
    2232,
    9722,
    428,
    10629,
    867,
    2278,
    13898,
    960,
    1886,
    12289,
    533,
    1967,
    2415,
    6126,
    2977,
    1569,
    4246,
    6075,
    2162,
    1113,
    1500,
    48784,
    5911,
    2841,
    1913,
    10568,
    254,
    779,
    4506,
    5509,
    751,
    1095,
    3702,
    48144,
    1543,
    9692,
    1094,
    2292,
    1001,
    1515,
    2089,
    3450,
    4408,
    13801,
    2715,
    16384,
    13436,
    1721,
    5623,
    14134,
    16384,
    7277,
    2145,
    2887,
    1475,
    739,
    2167,
    13173,
    1987,
    1207,
    34312,
    358,
    7478,
    449,
    2530,
    3883,
    1175,
    3193,
    2962,
    16384,
    1469,
    335,
    1484,
    6020,
    4321,
    2415,
    21323,
    4707,
    5166,
    661,
    4006,
    862,
    2803,
    14518,
    3084,
    5251,
    1730,
    16384,
    421,
    2882,
    7094,
    1735,
    931,
    2266,
    1938,
    298,
    16384,
    12328,
    287,
    236,
    539,
    1987,
    3179,
    2074,
    10651,
    1645,
    2557,
    4451,
    2269,
    421,
    2830,
    5250,
    560,
    3688,
    10409,
    783,
    1039,
    537,
    4230,
    1930,
    10145,
    5312,
    6015,
    1708,
    2647,
    4636,
    2390,
    1518,
    13272,
    79,
    4133,
    2312,
    3839,
    2261,
    327,
    10802,
    16384,
    609,
    1455,
    160,
    806,
    3125,
    14667,
    1128,
    892,
    3318,
    1249,
    543,
    1771,
    553,
    8657,
    386,
    1537,
    493,
    616,
    4304,
    3077,
    7850,
    1349,
    8017,
    2226,
    1057,
    10474,
    4918,
    3331,
    7531,
    1838,
    2554,
    10904,
    1557,
    4820,
    2654,
    2027,
    697,
    3912,
    1392,
    1112,
    2962,
    12563,
    9560,
    11258,
    899,
    3713,
    1237,
    10011,
    155,
    1578,
    13769,
    1106,
    1315,
    2675,
    1702,
    3068,
    1707,
    1967,
    1101,
    5469,
    4651,
    2234,
    4867,
    1999,
    1557,
    8183,
    1843,
    6480,
    4775,
    2903,
    4011,
    1416,
    2987,
    2849,
    1724,
    2787,
    321,
    2439,
    1412,
    659,
    877,
    17757,
    1680,
    5156,
    668,
    558,
    8513,
    335,
    2435,
    4249,
    2135,
    1028,
    6334,
    166,
    1873,
    1762,
    2195,
    19798,
    502,
    387,
    2304,
    2212,
    1129,
    842,
    662,
    1848,
    368,
    758,
    739,
    4334,
    7472,
    15084,
    8599,
    1652,
    1741,
    372,
    272,
    3094,
    13689,
    13309,
    463,
    2464,
    780,
    2340,
    1388,
    2317,
    1034,
    739,
    5061,
    1034,
    1290,
    1607,
    501,
    1233,
    953,
    12622,
    2101,
    245,
    3310,
    1157,
    3590,
    2706,
    6459,
    1621,
    614,
    5597,
    6763,
    1343,
    1027,
    4464,
    1453,
    5506,
    7428,
    2336,
    5939,
    6169,
    4722,
    377,
    1750,
    32910,
    1180,
    500,
    752,
    5280,
    28964,
    1366,
    1773,
    5013,
    818,
    795,
    8491,
    1338,
    16384,
    11422,
    197,
    120,
    23288,
    2495,
    1428,
    536,
    352,
    5089,
    2865,
    1994,
    1921,
    1166,
    16384,
    1496,
    4956,
    984,
    802,
    1598,
    1504,
    2160,
    5988,
    21608,
    5527,
    10134,
    252,
    844,
    427,
    403,
    562,
    2222,
    2755,
    5299,
    2263,
    1934,
    2713,
    5810,
    413,
    427,
    1613,
    1108,
    3884,
    421,
    685,
    6213,
    9889,
    2583,
    8423,
    4619,
    3832,
    512,
    735,
    863,
    826,
    439,
    363,
    1663,
    4609,
    1456,
    1373,
    1234,
    2337,
    1437,
    198,
    16384,
    2898,
    2979,
    2820,
    3186,
    2443,
    3117,
    16384,
    1316,
    1695,
    797,
    852,
    7604,
    4613,
    2832,
    1662,
    12787,
    1110,
    1865,
    3461,
    10986,
    2567,
    497,
    610,
    2971,
    1505,
    4722,
    1183,
    11332,
    4241,
    3288,
    135,
    4022,
    1041,
    1711,
    583,
    1482,
    2889,
    5402,
    179,
    1300,
    524,
    397,
    1476,
    5232,
    659,
    1198,
    1627,
    200,
    1079,
    453,
    1338,
    7312,
    2489,
    12774,
    612,
    3786,
    2786,
    1038,
    9152,
    2860,
    10403,
    1019,
    2486,
    6661,
    5890,
    1854,
    12862,
    292,
    1037,
    1296,
    4722,
    2309,
    8692,
    7594,
    3217,
    4689,
    7506,
    1162,
    16384,
    3876,
    2567,
    3567,
    1691,
    1503,
    1122,
    2694,
    752,
    745,
    1609,
    2209,
    752,
    255,
    1837,
    846,
    3388,
    2069,
    918,
    1200,
    549,
    3637,
    477,
    3001,
    1819,
    901,
    879,
    2633,
    10258,
    12576,
    908,
    1109,
    2961,
    1535,
    17181,
    2484,
    399,
    1146,
    5174,
    474,
    15348,
    12984,
    10753,
    337,
    1049,
    2924,
    14353,
    770,
    2041,
    1522,
    677,
    507,
    427,
    152,
    838,
    1625,
    1945,
    512,
    3258,
    9809,
    1001,
    2150,
    4249,
    15336,
    11382,
    2216,
    959,
    4157,
    3587,
    10515,
    1116,
    226,
    1802,
    1086,
    2376,
    3006,
    3291,
    6973,
    3146,
    2169,
    10099,
    1886,
    1384,
    1560,
    1130,
    631,
    5903,
    1791,
    13575,
    6936,
    11151,
    622,
    12501,
    392,
    245,
    2953,
    880,
    2307,
    31949,
    1771,
    2962,
    2139,
    5583,
    13662,
    1623,
    3639,
    7435,
    2269,
    1964,
    47940,
    2818,
    10166,
    703,
    1151,
    2963,
    13488,
    4356,
    7576,
    14604,
    1785,
    1282,
    1834,
    1767,
    3687,
    4100,
    2273,
    518,
    2732,
    1383,
    739,
    3040,
    1886,
    8910,
    2602,
    1139,
    9361,
    6068,
    1193,
    1620,
    2566,
    2871,
    8478,
    2951,
    12622,
    1204,
    2222,
    5384,
    1018,
    3702,
    13804,
    4244,
    1626,
    1272,
    407,
    1886,
    16384,
    2435,
    4584,
    7362,
    2731,
    10274,
    3229,
    7982,
    1426,
    4338,
    3390,
    1652,
    625,
    6142,
    5538,
    2460,
    1312,
    391,
    7798,
    352,
    757,
    13301,
    1072,
    1858,
    552,
    963,
    4672,
    2661,
    11399,
    857,
    19742,
    1756,
    731,
    2862,
    1705,
    520,
    5280,
    968,
    1560,
    816,
    580,
    618,
    3321,
    875,
    13883,
    2676,
    10595,
    485,
    2837,
    779,
    31731,
    3932,
    4118,
    2434,
    163,
    428,
    4628,
    1030,
    6393,
    509,
    5058,
    3168,
    3622,
    2337,
    670,
    12527,
    3846,
    749,
    22586,
    6490,
    8999,
    4579,
    446,
    2531,
    4644,
    1064,
    716,
    454,
    623,
    16787,
    2296,
    10118,
    2042,
    1688,
    3630,
    11215,
    6503,
    12704,
    632,
    2601,
    1534,
    2494,
    6666,
    862,
    623,
    12298,
    99,
    2056,
    2067,
    1641,
    3169,
    768,
    17001,
    1155,
    1017,
    743,
    930,
    28939,
    16384,
    8240,
    8982,
    201,
    6374,
    968,
    4323,
    390,
    4324,
    888,
    573,
    4835,
    16384,
    2724,
    3936,
    12824,
    18645,
    1801,
    17408,
    1175,
    1466,
    16384,
    2432,
    14984,
    1184,
    1874,
    3910,
    1687,
    1425,
    12052,
    544,
    262,
    469,
    2351,
    973,
    1335,
    11286,
    1049,
    1448,
    261,
    1696,
    874,
    909,
    6539,
    3965,
    309,
    782,
    724,
    5517,
    28157,
    4363,
    1906,
    977,
    1407,
    254,
    2163,
    2752,
    1471,
    3887,
    4178,
    4452,
    8686,
    765,
    1635,
    1473,
    3613,
    509,
    978,
    747,
    11349,
    1886,
    1020,
    75,
    936,
    176,
    1241,
    948,
    2399,
    1717,
    850,
    3110,
    15997,
    891,
    218,
    768,
    10101,
    13354,
    14616,
    285,
    792,
    656,
    16218,
    486,
    1165,
    4600,
    7058,
    443,
    3900,
    1115,
    1022,
    13625,
    1911,
    1474,
    11331,
    2670,
    8052,
    7355,
    3868,
    4142,
    692,
    1859,
    3090,
    7686,
    345,
    729,
    11408,
    2436,
    1856,
    1936,
    3405,
    4910,
    5349,
    863,
    9714,
    7459,
    961,
    3429,
    18480,
    16384,
    7910,
    771,
    562,
    123,
    4206,
    807,
    1994,
    29428,
    303,
    2711,
    1193,
    649,
    2813,
    1085,
    1118,
    2214,
    2123,
    2446,
    6515,
    820,
    20210,
    625,
    2119,
    5330,
    1706,
    3630,
    16384,
    5430,
    956,
    2347,
    1262,
    1425,
    28771,
    2011,
    1909,
    7920,
    20231,
    2038,
    1303,
    6712,
    15933,
    977,
    9859,
    2491,
    2249,
    3140,
    2545,
    698,
    1844,
    1568,
    1611,
    1302,
    1695,
    2507,
    1164,
    8706,
    6227,
    13458,
    2707,
    1891,
    2713,
    1567,
    3128,
    312,
    575,
    1219,
    333,
    1840,
    672,
    1150,
    1961,
    4137,
    3547,
    2160,
    1783,
    1743,
    3482,
    13645,
    711,
    1594,
    409,
    16384,
    12341,
    6298,
    330,
    800,
    2084,
    413,
    12274,
    1798,
    16384,
    2803,
    299,
    16989,
    2713,
    13611,
    763,
    566,
    982,
    810,
    7030,
    9450,
    2720,
    314,
    1253,
    4132,
    1926,
    1915,
    818,
    2420,
    10153,
    1569,
    1077,
    756,
    514,
    6966,
    5110,
    4138,
    29794,
    7920,
    11030,
    388,
    1117,
    1077,
    3312,
    3319,
    791,
    34440,
    17367,
    176,
    914,
    939,
    3414,
    5810,
    2040,
    2066,
    16384,
    3043,
    2234,
    7849,
    2192,
    2056,
    1292,
    15451,
    1167,
    1757,
    719,
    12988,
    1525,
    1863,
    463,
    1496,
    10223,
    3804,
    946,
    18935,
    1370,
    1114,
    1513,
    7811,
    1014,
    763,
    569,
    1057,
    16384,
    8717,
    4356,
    6091,
    987,
    1184,
    7888,
    1909,
    1360,
    8230,
    464,
    6646,
    3046,
    934,
    1845,
    3361,
    1198,
    4898,
    11015,
    2964,
    466,
    17937,
    2124,
    1139,
    418,
    7205,
    4531,
    982,
    3363,
    640,
    14148,
    878,
    13952,
    6187,
    2116,
    227,
    12515,
    1398,
    1358,
    1328,
    246,
    16384,
    2405,
    2094,
    450,
    3569,
    25618,
    995,
    1959,
    10843,
    11412,
    380,
    2150,
    3028,
    5390,
    2037,
    1778,
    5680,
    7045,
    7286,
    795,
    1144,
    1152,
    2168,
    3628,
    4184,
    3087,
    2470,
    1908,
    3791,
    3128,
    5585,
    11816,
    1282,
    1101,
    3004,
    1266,
    7684,
    287,
    2047,
    2221,
    714,
    214,
    992,
    1259,
    4371,
    3911,
    1493,
    3175,
    588,
    977,
    1683,
    15252,
    402,
    3294,
    3603,
    266,
    2094,
    3729,
    1313,
    2379,
    882,
    159,
    72,
    1259,
    2993,
    1318,
    3000,
    4644,
    6943,
    2412,
    16384,
    6253,
    4802,
    369,
    1561,
    903,
    7808,
    1121,
    2992,
    1456,
    2251,
    1344,
    2161,
    5872,
    4140,
    281,
    605,
    3660,
    1263,
    11777,
    1594,
    17934,
    2922,
    10956,
    887,
    1260,
    5101,
    2724,
    5062,
    644,
    5843,
    1864,
    1013,
    432,
    7377,
    2322,
    3263,
    499,
    2084,
    691,
    3152,
    1647,
    117,
    323,
    1922,
    2099,
    1284,
    907,
    1831,
    651,
    2103,
    6251,
    1265,
    1007,
    2591,
    16384,
    470,
    9349,
    4471,
    7760,
    8977,
    1828,
    3591,
    1813,
    266,
    1099,
    3969,
    4950,
    1107,
    509,
    4528,
    6893,
    2387,
    2537,
    1800,
    856,
    1492,
    3090,
    647,
    10257,
    1676,
    124,
    5027,
    515,
    697,
    968,
    1594,
    2197,
    364,
    1961,
    2224,
    335,
    974,
    6777,
    2430,
    9450,
    4156,
    4723,
    206,
    5161,
    1304,
    1666,
    2029,
    2105,
    5265,
    2213,
    16384,
    2510,
    15099,
    14554,
    1114,
    543,
    16384,
    1330,
    1921,
    13264,
    1950,
    607,
    352,
    1711,
    8800,
    1687,
    549,
    910,
    1561,
    2949,
    1760,
    721,
    2405,
    979,
    1074,
    2242,
    1821,
    1145,
    512,
    516,
    1531,
    1896,
    2927,
    837,
    23219,
    691,
    746,
    4590,
    3439,
    1345,
    5531,
    7339,
    588,
    3926,
    9200,
    1514,
    869,
    714,
    5614,
    3643,
    5746,
    703,
    11033,
    1283,
    3832,
    19213,
    753,
    2109,
    10175,
    2719,
    686,
    9206,
    1024,
    1561,
    16384,
    749,
    2039,
    8364,
    16714,
    3650,
    629,
    183,
    8528,
    740,
    427,
    16384,
    215,
    1980,
    335,
    3252,
    10048,
    5458,
    1900,
    2062,
    1083,
    1675,
    13674,
    151,
    747,
    996,
    10261,
    889,
    2039,
    2752,
    14613,
    523,
    394,
    1118,
    16384,
    3306,
    2118,
    2457,
    817,
    608,
    4240,
    9497,
    3255,
    1835,
    601,
    1234,
    12599,
    80,
    12718,
    1699,
    1333,
    2324,
    1568,
    127,
    13963,
    4442,
    7531,
    638,
    1524,
    433,
    1832,
    5869,
    1591,
    1159,
    317,
    2531,
    953,
    15243,
    742,
    506,
    1644,
    967,
    7461,
    44541,
    835,
    1007,
    11320,
    18378,
    117,
    1004,
    2892,
    6509,
    390,
    1445,
    1418,
    8164,
    12694,
    735,
    763,
    982,
    118,
    2612,
    10376,
    6721,
    878,
    16384,
    455,
    479,
    7155,
    702,
    2001,
    2442,
    11333,
    25514,
    961,
    1725,
    87,
    3164,
    94,
    2676,
    2566,
    9332,
    11059,
    5050,
    5394,
    2845,
    1033,
    647,
    1014,
    12861,
    2685,
    1287,
    1435,
    2549,
    1567,
    4269,
    420,
    815,
    223,
    3250,
    4178,
    4404,
    1440,
    6781,
    812,
    1060,
    7461,
    110,
    11446,
    9487,
    4646,
    2398,
    4897,
    333,
    2780,
    8664,
    4975,
    35220,
    2387,
    1049,
    928,
    565,
    4409,
    710,
    506,
    5854,
    21303,
    726,
    2370,
    457,
    6391,
    1834,
    13019,
    1478,
    1420,
    289,
    3168,
    8191,
    1115,
    6200,
    2363,
    777,
    3336,
    2692,
    868,
    13087,
    2657,
    2697,
    2066,
    4104,
    16220,
    1480,
    7194,
    1623,
    4106,
    4552,
    11109,
    499,
    998,
    4005,
    2752,
    800,
    1587,
    919,
    360,
    13549,
    548,
    1326,
    1339,
    3407,
    319,
    1273,
    3415,
    1853,
    1543,
    13402,
    6172,
    1293,
    611,
    1213,
    1146,
    1164,
    1310,
    268,
    594,
    2959,
    1561,
    279,
    5886,
    10452,
    4931,
    3058,
    542,
    530,
    1688,
    14982,
    4899,
    21021,
    5025,
    198,
    2612,
    1922,
    4781,
    6782,
    16384,
    88,
    622,
    6404,
    5735,
    4950,
    124,
    11203,
    1598,
    16384,
    1862,
    3589,
    5776,
    1350,
    17713,
    3749,
    12099,
    4997,
    1732,
    2540,
    2977,
    1374,
    450,
    480,
    4673,
    1860,
    510,
    12721,
    7003,
    372,
    4902,
    536,
    362,
    1028,
    1023,
    614,
    1752,
    2029,
    667,
    1756,
    27621,
    308,
    1996,
    666,
    2967,
    922,
    1112,
    866,
    466,
    1110,
    11116,
    3125,
    328,
    1361,
    7165,
    2750,
    1699,
    369,
    2735,
    2027,
    182,
    6033,
    5229,
    6818,
    1808,
    10125,
    3489,
    11693,
    1348,
    1772,
    1207,
    12838,
    1357,
    1363,
    1994,
    11628,
    4883,
    2383,
    1344,
    1519,
    1058,
    11759,
    1166,
    905,
    1868,
    18592,
    357,
    1020,
    3529,
    10250,
    4033,
    2598,
    10712,
    3792,
    945,
    1109,
    1597,
    881,
    3153,
    2015,
    633,
    528,
    15013,
    765,
    13866,
    686,
    10812,
    2723,
    7799,
    3357,
    7218,
    488,
    352,
    3232,
    593,
    4073,
    3268,
    1700,
    4709,
    945,
    5758,
    776,
    7625,
    2748,
    114,
    366,
    2186,
    11453,
    890,
    4455,
    5539,
    4354,
    28608,
    7293,
    241,
    4338,
    600,
    1463,
    4262,
    7905,
    2448,
    3193,
    2038,
    351,
    1297,
    5248,
    1284,
    3347,
    383,
    3000,
    734,
    4461,
    846,
    3744,
    927,
    15601,
    30733,
    16384,
    5980,
    441,
    3571,
    345,
    3507,
    1977,
    244,
    3780,
    465,
    730,
    1051,
    1700,
    5452,
    1748,
    3997,
    1238,
    6294,
    1365,
    6501,
    5605,
    995,
    280,
    796,
    3325,
    440,
    1691,
    3310,
    47638,
    825,
    1639,
    1469,
    14609,
    5447,
    3988,
    1754,
    3551,
    822,
    408,
    1121,
    1189,
    1241,
    1865,
    2689,
    511,
    16384,
    3819,
    4354,
    2319,
    1239,
    5006,
    368,
    16384,
    482,
    12276,
    3231,
    933,
    266,
    1755,
    2040,
    1776,
    286,
    12646,
    799,
    2803,
    1096,
    3632,
    5638,
    2582,
    3312,
    3889,
    1122,
    4145,
    1135,
    3000,
    16384,
    260,
    9054,
    1440,
    300,
    2862,
    673,
    16384,
    21082,
    4054,
    2516,
    1414,
    2022,
    795,
    4012,
    11264,
    473,
    3690,
    826,
    1992,
    4412,
    7421,
    2565,
    1984,
    147,
    1534,
    1065,
    5004,
    2473,
    4231,
    1762,
    22162,
    4005,
    263,
    1522,
    5155,
    4025,
    4102,
    3676,
    1408,
    861,
    3824,
    4771,
    4598,
    4705,
    30051,
    963,
    10419,
    12287,
    4589,
    5363,
    684,
    1555,
    5985,
    1384,
    820,
    1357,
    16384,
    4247,
    2799,
    1232,
    16384,
    2597,
    716,
    1712,
    765,
    4352,
    1415,
    1803,
    4066,
    1208,
    13286,
    2626,
    1038,
    1064,
    481,
    2942,
    1473,
    2421,
    4669,
    1412,
    1031,
    1834,
    4106,
    372,
    767,
    1574,
    7067,
    3884,
    16385,
    1902,
    436,
    3854,
    532,
    1192,
    6698,
    3404,
    3077,
    3531,
    16384,
    14153,
    14509,
    3620,
    12260,
    533,
    571,
    11153,
    6350,
    1335,
    1857,
    303,
    1682,
    313,
    321,
    459,
    13986,
    8908,
    851,
    1035,
    378,
    3934,
    647,
    3242,
    1446,
    9142,
    5415,
    13607,
    826,
    4748,
    1999,
    3258,
    17623,
    1386,
    12828,
    818,
    13065,
    2967,
    1757,
    4068,
    16384,
    736,
    1643,
    178,
    221,
    14812,
    873,
    657,
    872,
    1503,
    512,
    1850,
    2548,
    3146,
    3638,
    1350,
    2487,
    923,
    948,
    1194,
    6120,
    708,
    399,
    6082,
    1105,
    16384,
    850,
    1519,
    296,
    1893,
    3042,
    10519,
    4693,
    1689,
    7890,
    2556
  ],
  "output_lens": [
    20,
    24,
    8,
    4,
    18,
    13,
    17,
    7,
    15,
    8,
    12,
    5,
    8,
    5,
    16,
    2,
    13,
    13,
    27,
    22,
    1,
    14,
    1,
    1,
    29,
    4,
    21,
    67,
    71,
    19,
    13,
    26,
    3,
    73,
    0,
    4,
    1,
    2,
    3,
    9,
    3,
    22,
    5,
    7,
    17,
    1,
    46,
    7,
    4,
    24,
    7,
    7,
    3,
    1,
    0,
    15,
    69,
    15,
    1,
    10,
    1,
    6,
    12,
    49,
    1,
    54,
    37,
    18,
    1,
    53,
    13,
    1,
    28,
    4,
    5,
    14,
    2,
    41,
    17,
    6,
    21,
    31,
    28,
    14,
    26,
    2,
    0,
    6,
    5,
    2,
    1,
    1,
    33,
    18,
    18,
    14,
    7,
    6,
    5,
    6,
    19,
    7,
    22,
    1,
    11,
    7,
    1,
    2,
    1,
    7,
    20,
    16,
    14,
    0,
    1,
    12,
    3,
    30,
    3,
    28,
    13,
    7,
    4,
    11,
    13,
    0,
    68,
    3,
    19,
    60,
    60,
    22,
    43,
    23,
    15,
    3,
    1,
    1,
    8,
    14,
    8,
    21,
    11,
    5,
    48,
    7,
    0,
    24,
    3,
    1,
    16,
    22,
    1,
    17,
    20,
    15,
    8,
    1,
    49,
    15,
    10,
    1,
    1,
    33,
    12,
    2,
    5,
    12,
    4,
    7,
    43,
    3,
    26,
    3,
    7,
    5,
    14,
    2,
    4,
    1,
    6,
    12,
    13,
    45,
    4,
    33,
    22,
    3,
    17,
    15,
    11,
    21,
    51,
    9,
    6,
    68,
    25,
    62,
    8,
    86,
    13,
    9,
    7,
    6,
    8,
    10,
    0,
    15,
    15,
    11,
    1,
    5,
    2,
    5,
    3,
    1,
    14,
    1,
    26,
    12,
    8,
    25,
    0,
    8,
    8,
    4,
    1,
    4,
    5,
    19,
    36,
    4,
    38,
    4,
    6,
    33,
    4,
    2,
    39,
    15,
    11,
    23,
    1,
    1,
    6,
    9,
    1,
    26,
    3,
    7,
    15,
    27,
    5,
    7,
    21,
    9,
    47,
    22,
    8,
    9,
    27,
    8,
    18,
    8,
    19,
    8,
    5,
    4,
    5,
    1,
    7,
    6,
    1,
    4,
    2,
    11,
    18,
    16,
    24,
    2,
    78,
    2,
    81,
    2,
    14,
    24,
    31,
    10,
    8,
    18,
    5,
    26,
    9,
    17,
    17,
    28,
    20,
    1,
    6,
    43,
    3,
    25,
    9,
    3,
    1,
    11,
    2,
    28,
    12,
    4,
    11,
    12,
    13,
    40,
    9,
    85,
    18,
    1,
    4,
    1,
    43,
    11,
    40,
    17,
    12,
    2,
    9,
    34,
    10,
    7,
    20,
    1,
    21,
    23,
    37,
    0,
    13,
    2,
    36,
    1,
    2,
    32,
    14,
    13,
    1,
    7,
    1,
    75,
    6,
    25,
    19,
    13,
    17,
    1,
    12,
    5,
    13,
    5,
    1,
    2,
    2,
    25,
    3,
    13,
    4,
    27,
    6,
    6,
    1,
    8,
    8,
    10,
    5,
    2,
    9,
    6,
    7,
    4,
    5,
    7,
    17,
    16,
    41,
    77,
    0,
    42,
    14,
    2,
    3,
    61,
    3,
    6,
    12,
    1,
    12,
    2,
    9,
    1,
    3,
    7,
    11,
    15,
    54,
    1,
    1,
    1,
    5,
    25,
    5,
    27,
    66,
    42,
    5,
    1,
    1,
    74,
    8,
    26,
    12,
    43,
    21,
    13,
    8,
    10,
    1,
    3,
    52,
    8,
    1,
    8,
    2,
    31,
    34,
    37,
    1,
    28,
    7,
    3,
    4,
    10,
    5,
    32,
    2,
    13,
    6,
    1,
    4,
    15,
    25,
    14,
    13,
    9,
    10,
    5,
    12,
    16,
    11,
    2,
    25,
    27,
    29,
    5,
    23,
    19,
    6,
    27,
    15,
    16,
    6,
    4,
    32,
    3,
    11,
    1,
    5,
    0,
    1,
    6,
    5,
    25,
    26,
    1,
    56,
    22,
    69,
    1,
    1,
    11,
    8,
    52,
    5,
    6,
    1,
    39,
    21,
    4,
    9,
    2,
    2,
    18,
    0,
    14,
    1,
    9,
    4,
    14,
    4,
    1,
    4,
    1,
    3,
    20,
    30,
    25,
    3,
    16,
    2,
    3,
    17,
    9,
    9,
    6,
    6,
    9,
    32,
    2,
    12,
    1,
    19,
    27,
    28,
    18,
    5,
    1,
    5,
    31,
    4,
    19,
    8,
    1,
    3,
    25,
    52,
    12,
    17,
    12,
    4,
    30,
    32,
    4,
    9,
    1,
    4,
    41,
    1,
    3,
    23,
    2,
    24,
    10,
    17,
    12,
    15,
    10,
    17,
    32,
    1,
    3,
    1,
    13,
    3,
    23,
    5,
    8,
    6,
    2,
    37,
    6,
    9,
    16,
    12,
    1,
    6,
    11,
    1,
    3,
    1,
    6,
    13,
    22,
    13,
    1,
    6,
    29,
    4,
    19,
    5,
    11,
    9,
    26,
    2,
    20,
    6,
    9,
    1,
    8,
    1,
    16,
    2,
    42,
    15,
    19,
    5,
    3,
    1,
    9,
    64,
    2,
    18,
    5,
    13,
    3,
    5,
    30,
    14,
    5,
    3,
    21,
    3,
    14,
    7,
    118,
    28,
    10,
    10,
    7,
    22,
    8,
    3,
    1,
    27,
    2,
    10,
    25,
    29,
    5,
    1,
    5,
    11,
    27,
    22,
    16,
    1,
    4,
    118,
    71,
    6,
    17,
    12,
    4,
    14,
    15,
    2,
    1,
    1,
    16,
    34,
    33,
    3,
    9,
    21,
    5,
    8,
    10,
    40,
    6,
    14,
    13,
    8,
    2,
    8,
    29,
    8,
    6,
    15,
    2,
    3,
    30,
    13,
    34,
    4,
    1,
    6,
    5,
    14,
    4,
    11,
    3,
    7,
    54,
    5,
    2,
    1,
    14,
    5,
    1,
    3,
    3,
    15,
    12,
    71,
    24,
    10,
    22,
    30,
    6,
    6,
    20,
    0,
    4,
    30,
    47,
    3,
    40,
    6,
    11,
    6,
    1,
    17,
    1,
    4,
    3,
    1,
    7,
    3,
    62,
    16,
    3,
    16,
    19,
    1,
    15,
    11,
    5,
    6,
    8,
    11,
    26,
    4,
    35,
    1,
    2,
    3,
    2,
    22,
    6,
    4,
    7,
    3,
    27,
    14,
    5,
    127,
    40,
    18,
    5,
    1,
    11,
    23,
    6,
    32,
    24,
    34,
    12,
    15,
    5,
    18,
    1,
    3,
    9,
    35,
    13,
    22,
    19,
    1,
    1,
    14,
    37,
    16,
    3,
    4,
    18,
    11,
    3,
    23,
    10,
    10,
    3,
    32,
    14,
    7,
    19,
    18,
    0,
    1,
    1,
    51,
    11,
    9,
    6,
    33,
    17,
    36,
    3,
    0,
    3,
    12,
    4,
    4,
    7,
    28,
    22,
    17,
    13,
    41,
    4,
    28,
    23,
    1,
    26,
    5,
    56,
    60,
    7,
    1,
    9,
    14,
    23,
    29,
    11,
    24,
    3,
    1,
    6,
    78,
    36,
    4,
    6,
    49,
    3,
    3,
    0,
    3,
    1,
    11,
    15,
    11,
    1,
    25,
    1,
    1,
    24,
    3,
    7,
    13,
    20,
    4,
    1,
    5,
    25,
    6,
    3,
    37,
    10,
    8,
    31,
    25,
    11,
    15,
    5,
    4,
    3,
    11,
    0,
    1,
    48,
    23,
    24,
    18,
    3,
    53,
    6,
    23,
    22,
    43,
    22,
    6,
    1,
    4,
    17,
    3,
    12,
    19,
    3,
    10,
    14,
    58,
    2,
    3,
    1,
    1,
    3,
    16,
    26,
    14,
    28,
    0,
    15,
    6,
    1,
    10,
    62,
    12,
    10,
    3,
    17,
    8,
    40,
    3,
    6,
    1,
    33,
    22,
    1,
    13,
    15,
    13,
    1,
    12,
    3,
    22,
    2,
    1,
    29,
    4,
    4,
    9,
    1,
    13,
    3,
    2,
    9,
    2,
    3,
    3,
    1,
    2,
    5,
    10,
    7,
    13,
    1,
    1,
    1,
    1,
    7,
    4,
    10,
    6,
    34,
    3,
    3,
    1,
    15,
    1,
    1,
    44,
    1,
    0,
    8,
    16,
    3,
    1,
    2,
    45,
    8,
    7,
    1,
    1,
    1,
    14,
    6,
    1,
    0,
    25,
    5,
    20,
    5,
    3,
    3,
    8,
    4,
    10,
    10,
    14,
    38,
    4,
    0,
    11,
    5,
    16,
    5,
    40,
    10,
    7,
    1,
    3,
    2,
    3,
    2,
    0,
    5,
    4,
    29,
    21,
    34,
    5,
    1,
    3,
    8,
    48,
    21,
    1,
    6,
    2,
    3,
    12,
    1,
    16,
    1,
    19,
    23,
    21,
    1,
    49,
    76,
    39,
    40,
    55,
    19,
    4,
    4,
    1,
    1,
    1,
    6,
    14,
    36,
    1,
    4,
    8,
    14,
    1,
    24,
    28,
    29,
    7,
    5,
    20,
    34,
    21,
    3,
    60,
    43,
    1,
    0,
    58,
    22,
    24,
    20,
    1,
    29,
    19,
    4,
    36,
    4,
    1,
    3,
    6,
    8,
    11,
    16,
    5,
    9,
    1,
    10,
    80,
    1,
    4,
    19,
    8,
    11,
    10,
    37,
    16,
    1,
    8,
    39,
    9,
    10,
    17,
    1,
    1,
    30,
    7,
    2,
    9,
    11,
    31,
    12,
    28,
    2,
    16,
    2,
    4,
    1,
    45,
    56,
    13,
    1,
    41,
    1,
    11,
    11,
    1,
    8,
    8,
    7,
    66,
    13,
    12,
    24,
    3,
    41,
    6,
    127,
    8,
    21,
    14,
    15,
    3,
    12,
    6,
    5,
    37,
    1,
    63,
    3,
    13,
    3,
    11,
    3,
    39,
    12,
    23,
    2,
    1,
    19,
    7,
    32,
    2,
    35,
    15,
    15,
    2,
    32,
    26,
    12,
    24,
    3,
    22,
    1,
    16,
    30,
    3,
    6,
    23,
    18,
    66,
    3,
    11,
    14,
    16,
    12,
    1,
    6,
    5,
    76,
    1,
    18,
    30,
    24,
    9,
    8,
    22,
    11,
    1,
    24,
    8,
    5,
    0,
    2,
    7,
    17,
    15,
    4,
    11,
    4,
    9,
    14,
    1,
    5,
    15,
    5,
    10,
    1,
    3,
    39,
    1,
    0,
    1,
    8,
    0,
    43,
    11,
    10,
    51,
    4,
    18,
    7,
    5,
    0,
    9,
    1,
    0,
    6,
    5,
    2,
    46,
    12,
    17,
    16,
    5,
    57,
    4,
    31,
    2,
    9,
    16,
    0,
    28,
    2,
    52,
    12,
    20,
    8,
    9,
    1,
    2,
    1,
    4,
    11,
    5,
    16,
    43,
    20,
    3,
    5,
    2,
    4,
    22,
    35,
    21,
    17,
    0,
    9,
    47,
    42,
    6,
    3,
    1,
    71,
    12,
    4,
    14,
    9,
    23,
    2,
    36,
    24,
    0,
    40,
    6,
    20,
    1,
    7,
    12,
    42,
    5,
    1,
    23,
    2,
    27,
    26,
    39,
    98,
    1,
    1,
    18,
    2,
    22,
    4,
    1,
    1,
    0,
    33,
    77,
    22,
    1,
    76,
    1,
    3,
    25,
    3,
    9,
    9,
    4,
    18,
    3,
    11,
    24,
    9,
    40,
    13,
    2,
    27,
    31,
    15,
    24,
    21,
    11,
    13,
    7,
    18,
    4,
    12,
    10,
    2,
    11,
    7,
    6,
    1,
    31,
    20,
    13,
    28,
    12,
    5,
    22,
    7,
    41,
    6,
    5,
    6,
    7,
    9,
    1,
    1,
    7,
    5,
    15,
    15,
    1,
    4,
    15,
    36,
    1,
    4,
    22,
    7,
    22,
    3,
    2,
    19,
    3,
    1,
    17,
    20,
    25,
    1,
    16,
    21,
    14,
    22,
    10,
    22,
    31,
    1,
    42,
    31,
    23,
    4,
    7,
    19,
    9,
    22,
    31,
    14,
    18,
    1,
    14,
    1,
    1,
    21,
    30,
    18,
    16,
    3,
    52,
    5,
    18,
    9,
    8,
    20,
    15,
    28,
    21,
    28,
    1,
    13,
    0,
    7,
    17,
    1,
    1,
    51,
    9,
    1,
    1,
    48,
    57,
    0,
    3,
    0,
    1,
    13,
    5,
    10,
    5,
    8,
    22,
    7,
    3,
    38,
    19,
    1,
    5,
    10,
    14,
    7,
    30,
    7,
    13,
    8,
    13,
    3,
    19,
    26,
    21,
    2,
    1,
    36,
    13,
    21,
    34,
    4,
    10,
    47,
    8,
    10,
    9,
    12,
    30,
    3,
    6,
    15,
    1,
    47,
    3,
    3,
    21,
    13,
    3,
    18,
    32,
    1,
    11,
    5,
    3,
    13,
    64,
    42,
    40,
    36,
    26,
    20,
    34,
    1,
    2,
    28,
    2,
    14,
    3,
    12,
    45,
    15,
    59,
    0,
    9,
    10,
    10,
    2,
    10,
    3,
    1,
    6,
    1,
    1,
    29,
    6,
    5,
    3,
    5,
    63,
    21,
    10,
    3,
    1,
    49,
    19,
    2,
    3,
    3,
    2,
    3,
    8,
    28,
    48,
    12,
    2,
    5,
    0,
    5,
    25,
    5,
    13,
    5,
    1,
    4,
    51,
    33,
    6,
    21,
    28,
    36,
    9,
    12,
    6,
    3,
    8,
    13,
    14,
    3,
    16,
    6,
    17,
    8,
    2,
    23,
    2,
    25,
    31,
    21,
    15,
    1,
    43,
    1,
    25,
    2,
    1,
    42,
    3,
    12,
    1,
    0,
    3,
    4,
    5,
    15,
    15,
    26,
    27,
    1,
    28,
    15,
    5,
    1,
    9,
    14,
    7,
    21,
    1,
    2,
    5,
    40,
    9,
    13,
    18,
    8,
    24,
    11,
    29,
    17,
    21,
    1,
    22,
    10,
    6,
    9,
    3,
    1,
    17,
    28,
    42,
    12,
    11,
    12,
    7,
    6,
    32,
    2,
    24,
    33,
    0,
    10,
    40,
    2,
    9,
    52,
    1,
    23,
    8,
    1,
    1,
    9,
    31,
    24,
    14,
    1,
    7,
    7,
    1,
    3,
    29,
    4,
    2,
    1,
    19,
    7,
    50,
    14,
    14,
    23,
    25,
    2,
    12,
    27,
    7,
    1,
    3,
    14,
    36,
    15,
    17,
    11,
    1,
    25,
    14,
    19,
    32,
    4,
    21,
    1,
    24,
    1,
    6,
    0,
    17,
    24,
    8,
    22,
    21,
    1,
    15,
    25,
    13,
    11,
    10,
    1,
    28,
    3,
    2,
    14,
    23,
    4,
    1,
    2,
    3,
    1,
    12,
    9,
    69,
    59,
    14,
    5,
    7,
    11,
    7,
    49,
    3,
    22,
    11,
    16,
    5,
    58,
    22,
    7,
    12,
    1,
    21,
    0,
    17,
    21,
    2,
    17,
    5,
    24,
    3,
    0,
    5,
    37,
    5,
    3,
    4,
    34,
    7,
    1,
    3,
    19,
    1,
    1,
    5,
    15,
    1,
    28,
    2,
    4,
    34,
    56,
    5,
    13,
    23,
    21,
    5,
    1,
    1,
    1,
    12,
    49,
    48,
    0,
    10,
    1,
    1,
    2,
    1,
    12,
    9,
    1,
    14,
    23,
    5,
    1,
    16,
    55,
    35,
    37,
    52,
    25,
    7,
    67,
    11,
    11,
    3,
    12,
    5,
    5,
    31,
    49,
    18,
    7,
    5,
    30,
    18,
    3,
    10,
    0,
    11,
    3,
    6,
    13,
    18,
    6,
    0,
    9,
    25,
    6,
    3,
    36,
    1,
    3,
    1,
    18,
    51,
    67,
    12,
    28,
    28,
    3,
    11,
    5,
    23,
    23,
    5,
    32,
    6,
    2,
    40,
    4,
    29,
    21,
    24,
    19,
    31,
    3,
    68,
    15,
    2,
    45,
    45,
    53,
    38,
    12,
    81,
    1,
    1,
    22,
    26,
    7,
    1,
    16,
    2,
    11,
    24,
    13,
    12,
    16,
    1,
    7,
    29,
    23,
    34,
    22,
    10,
    6,
    12,
    8,
    50,
    53,
    17,
    15,
    38,
    3,
    10,
    32,
    30,
    7,
    57,
    5,
    16,
    30,
    14,
    3,
    1,
    6,
    8,
    7,
    5,
    1,
    6,
    31,
    20,
    1,
    1,
    0,
    14,
    1,
    27,
    2,
    33,
    29,
    11,
    18,
    12,
    9,
    18,
    33,
    3,
    20,
    3,
    3,
    14,
    1,
    1,
    10,
    16,
    55,
    7,
    1,
    14,
    2,
    61,
    7,
    12,
    4,
    37,
    1,
    4,
    1,
    45,
    4,
    6,
    28,
    34,
    15,
    31,
    46,
    4,
    11,
    3,
    35,
    16,
    43,
    7,
    59,
    13,
    20,
    16,
    1,
    5,
    43,
    3,
    1,
    16,
    7,
    27,
    3,
    83,
    25,
    1,
    3,
    30,
    4,
    64,
    5,
    1,
    18,
    7,
    14,
    11,
    7,
    28,
    23,
    54,
    10,
    6,
    23,
    8,
    1,
    22,
    10,
    16,
    36,
    3,
    1,
    1,
    9,
    3,
    14,
    5,
    16,
    6,
    8,
    1,
    31,
    26,
    10,
    4,
    3,
    55,
    1,
    79,
    1,
    45,
    0,
    3,
    27,
    18,
    1,
    14,
    1,
    12,
    5,
    9,
    5,
    3,
    8,
    4,
    18,
    2,
    1,
    1,
    1,
    3,
    5,
    1,
    32,
    16,
    39,
    1,
    18,
    68,
    7,
    8,
    26,
    7,
    28,
    14,
    8,
    2,
    51,
    4,
    3,
    1,
    3,
    7,
    2,
    17,
    19,
    2,
    27,
    26,
    47,
    1,
    29,
    1,
    34,
    13,
    46,
    3,
    23,
    15,
    5,
    8,
    3,
    92,
    3,
    5,
    11,
    19,
    4,
    5,
    38,
    8,
    8,
    64,
    2,
    1,
    16,
    27,
    0,
    8,
    25,
    14,
    1,
    3,
    26,
    1,
    11,
    26,
    66,
    2,
    77,
    19,
    2,
    48,
    29,
    18,
    34,
    35,
    8,
    6,
    24,
    10,
    42,
    9,
    29,
    3,
    8,
    5,
    3,
    66,
    9,
    10,
    5,
    7,
    4,
    2,
    10,
    46,
    27,
    7,
    5,
    14,
    24,
    15,
    8,
    6,
    3,
    8,
    5,
    9,
    9,
    3,
    42,
    23,
    3,
    8,
    12,
    1,
    2,
    4,
    14,
    27,
    2,
    13,
    1,
    23,
    1,
    24,
    12,
    10,
    17,
    13,
    37,
    10,
    1,
    2,
    12,
    5,
    31,
    1,
    4,
    5,
    29,
    3,
    11,
    11,
    13,
    2,
    28,
    14,
    16,
    4,
    1,
    17,
    22,
    17,
    19,
    1,
    3,
    13,
    17,
    3,
    16,
    1,
    2,
    1,
    30,
    17,
    101,
    31,
    10,
    0,
    1,
    31,
    17,
    12,
    15,
    10,
    3,
    5,
    7,
    28,
    8,
    1,
    13,
    8,
    8,
    21,
    5,
    9,
    30,
    10,
    21,
    7,
    3,
    19,
    12,
    2,
    7,
    20,
    35,
    43,
    27,
    10,
    9,
    11,
    13,
    5,
    17,
    20,
    4,
    6,
    15,
    17,
    48,
    3,
    34,
    12,
    32,
    24,
    55,
    41,
    2,
    13,
    1,
    18,
    12,
    2,
    17,
    7,
    1,
    1,
    14,
    24,
    10,
    7,
    6,
    1,
    26,
    2,
    5,
    15,
    26,
    2,
    17,
    8,
    4,
    8,
    7,
    28,
    2,
    29,
    2,
    29,
    5,
    20,
    11,
    14,
    5,
    16,
    43,
    18,
    13,
    0,
    28,
    5,
    9,
    7,
    11,
    55,
    1,
    11,
    3,
    2,
    11,
    12,
    3,
    5,
    1,
    8,
    5,
    47,
    9,
    25,
    56,
    1,
    17,
    27,
    1,
    1,
    15,
    12,
    1,
    55,
    5,
    33,
    19,
    33,
    14,
    34,
    19,
    9,
    4,
    43,
    25,
    12,
    4,
    0,
    7,
    8,
    26,
    2,
    12,
    5,
    4,
    1,
    2,
    55,
    1,
    2,
    6,
    46,
    13,
    4,
    40,
    9,
    17,
    10,
    1,
    1,
    12,
    1,
    29,
    28,
    15,
    1,
    14,
    22,
    1,
    1,
    1,
    1,
    3,
    13,
    1,
    5,
    24,
    8,
    0,
    10,
    11,
    8,
    34,
    5,
    4,
    3,
    2,
    1,
    18,
    1,
    12,
    4,
    6,
    21,
    36,
    4,
    34,
    24,
    1,
    21,
    1,
    3,
    33,
    5,
    5,
    6,
    69,
    6,
    5,
    47,
    4,
    30,
    5,
    3,
    1,
    0,
    16,
    1,
    7,
    17,
    31,
    1,
    1,
    22,
    3,
    11,
    1,
    11,
    13,
    47,
    12,
    23,
    17,
    22,
    1,
    3,
    47,
    9,
    5,
    10,
    5,
    1,
    36,
    31,
    17,
    1,
    10,
    4,
    2,
    22,
    1,
    32,
    21,
    6,
    15,
    9,
    23,
    26,
    13,
    6,
    8,
    5,
    1,
    34,
    4,
    74,
    5,
    14,
    8,
    5,
    18,
    16,
    18,
    2,
    3,
    2,
    3,
    12,
    1,
    5,
    10,
    1,
    11,
    6,
    47,
    6,
    5,
    1,
    4,
    1,
    18,
    27,
    18,
    8,
    6,
    6,
    27,
    24,
    38,
    3,
    7,
    30,
    9,
    3,
    11,
    14,
    24,
    3,
    31,
    18,
    4,
    29,
    6,
    2,
    6,
    23,
    21,
    26,
    17,
    4,
    1,
    1,
    22,
    51,
    5,
    4,
    24,
    1,
    6,
    19,
    19,
    33,
    18,
    55,
    1,
    4,
    5,
    20,
    10,
    3,
    38,
    1,
    2,
    62,
    30,
    17,
    28,
    1,
    45,
    9,
    1,
    2,
    14,
    2,
    9,
    27,
    3,
    32,
    13,
    19,
    1,
    23,
    42,
    14,
    3,
    16,
    6,
    27,
    3,
    2,
    7,
    8,
    10,
    70,
    10,
    8,
    3,
    52,
    31,
    6,
    1,
    1,
    25,
    1,
    20,
    20,
    13,
    67,
    4,
    11,
    7,
    4,
    10,
    52,
    3,
    77,
    13,
    0,
    1,
    8,
    15,
    5,
    12,
    0,
    1,
    1,
    4,
    1,
    4,
    5,
    9,
    13,
    1,
    11,
    2,
    30,
    25,
    10,
    1,
    7,
    0,
    17,
    31,
    16,
    115,
    4,
    1,
    18,
    28,
    5,
    35,
    13,
    8,
    1,
    6,
    6,
    32,
    3,
    12,
    7,
    24,
    1,
    12,
    16,
    3,
    54,
    6,
    3,
    26,
    20,
    1,
    13,
    14,
    6,
    9,
    7,
    8,
    1,
    12,
    13,
    1,
    5,
    7,
    8,
    3,
    16,
    3,
    9,
    6,
    1,
    7,
    4,
    26,
    9,
    28,
    29,
    13,
    16,
    31,
    7,
    3,
    59,
    14,
    16,
    17,
    5,
    9,
    37,
    9,
    37,
    5,
    5,
    3,
    4,
    13,
    31,
    6,
    39,
    9,
    3,
    1,
    14,
    2,
    12,
    1,
    34,
    5,
    18,
    4,
    23,
    1,
    1,
    28,
    5,
    1,
    3,
    7,
    1,
    21,
    17,
    39,
    2,
    3,
    9,
    2,
    26,
    3,
    2,
    2,
    5,
    68,
    5,
    12,
    6,
    6,
    13,
    1,
    5,
    12,
    8,
    37,
    5,
    6,
    11,
    0,
    11,
    13,
    52,
    13,
    22,
    29,
    105,
    33,
    8,
    29,
    26,
    5,
    9,
    32,
    2,
    39,
    4,
    11,
    6,
    5,
    18,
    25,
    1,
    8,
    12,
    13,
    8,
    8,
    1,
    4,
    5,
    9,
    19,
    9,
    27,
    5,
    19,
    6,
    40,
    5,
    1,
    61,
    36,
    8,
    3,
    28,
    4,
    2,
    1,
    3,
    4,
    1,
    9,
    29,
    16,
    1,
    29,
    45,
    20,
    1,
    18,
    14,
    32,
    10,
    19,
    5,
    6,
    4,
    11,
    3,
    21,
    38,
    21,
    23,
    5,
    15,
    17,
    25,
    44,
    17,
    9,
    3,
    20,
    0,
    1,
    4,
    9,
    13,
    13,
    13,
    11,
    14,
    2,
    17,
    0,
    4,
    1,
    1,
    53,
    7,
    2,
    11,
    68,
    18,
    31,
    11,
    8,
    21,
    26,
    3,
    37,
    8,
    1,
    4,
    1,
    7,
    1,
    28,
    20,
    6,
    8,
    5,
    19,
    20,
    0,
    4,
    6,
    2,
    8,
    18,
    1,
    26,
    21,
    3,
    14,
    30,
    1,
    3,
    4,
    17,
    15,
    8,
    9,
    20,
    13,
    9,
    1,
    40,
    2,
    1,
    1,
    27,
    8,
    13,
    5,
    54,
    17,
    3,
    7,
    1,
    7,
    49,
    3,
    12,
    33,
    38,
    10,
    23,
    40,
    9,
    37,
    34,
    42,
    39,
    5,
    1,
    36,
    8,
    15,
    6,
    10,
    23,
    28,
    2,
    5,
    28,
    27,
    12,
    3,
    4,
    0,
    25,
    7,
    1,
    39,
    44,
    60,
    17,
    18,
    1,
    22,
    15,
    19,
    2,
    5,
    36,
    29,
    12,
    14,
    12,
    31,
    14,
    2,
    1,
    3,
    9,
    35,
    35,
    13,
    12,
    30,
    11,
    2,
    13,
    9,
    6,
    3,
    1,
    7,
    4,
    0,
    22,
    1,
    1,
    15,
    7,
    16,
    7,
    1,
    3,
    0,
    3,
    17,
    16,
    8,
    47,
    11,
    19,
    9,
    70,
    17,
    7,
    1,
    14,
    1,
    3,
    10,
    29,
    7,
    11,
    6,
    15,
    9,
    6,
    9,
    21,
    9,
    22,
    1,
    66,
    3,
    3,
    6,
    3,
    8,
    11,
    27,
    9,
    2,
    9,
    4,
    1,
    3,
    2,
    18,
    27,
    23,
    27,
    11,
    9,
    5,
    1,
    29,
    18,
    1,
    7,
    39,
    16,
    4,
    37,
    15,
    12,
    7,
    2,
    1,
    8,
    4,
    1,
    8,
    20,
    17,
    17,
    2,
    7,
    4,
    27,
    12,
    3,
    1,
    41,
    7,
    4,
    5,
    45,
    18,
    2,
    4,
    5,
    2,
    61,
    30,
    20,
    17,
    4,
    18,
    9,
    32,
    2,
    3,
    5,
    8,
    55,
    0,
    6,
    11,
    30,
    2,
    6,
    2,
    7,
    5,
    2,
    21,
    30,
    2,
    10,
    34,
    21,
    1,
    6,
    5,
    8,
    11,
    10,
    16,
    16,
    8,
    30,
    6,
    18,
    9,
    1,
    4,
    6,
    3,
    5,
    4,
    6,
    19,
    1,
    19,
    19,
    14,
    1,
    36,
    5,
    14,
    8,
    8,
    13,
    19,
    12,
    2,
    12,
    20,
    6,
    34,
    11,
    1,
    11,
    25,
    21,
    1,
    29,
    40,
    6,
    6,
    2,
    9,
    10,
    0,
    9,
    26,
    12,
    34,
    2,
    33,
    1,
    0,
    4,
    1,
    5,
    32,
    10,
    11,
    34,
    10,
    3,
    20,
    13,
    15,
    9,
    33,
    2,
    9,
    17,
    13,
    6,
    1,
    16,
    60,
    27,
    12,
    59,
    52,
    44,
    4,
    13,
    0,
    8,
    30,
    1,
    38,
    2,
    1,
    0,
    10,
    11,
    1,
    17,
    2,
    7,
    2,
    20,
    50,
    37,
    17,
    5,
    11,
    6,
    15,
    2,
    22,
    2,
    1,
    11,
    13,
    26,
    11,
    42,
    6,
    3,
    10,
    9,
    21,
    5,
    2,
    16,
    12,
    1,
    9,
    7,
    1,
    13,
    38,
    7,
    27,
    9,
    2,
    34,
    0,
    3,
    6,
    1,
    1,
    22,
    3,
    22,
    39,
    3,
    27,
    37,
    11,
    44,
    1,
    15,
    10,
    17,
    6,
    19,
    3,
    7,
    49,
    54,
    11,
    1,
    18,
    19,
    8,
    1,
    5,
    20,
    17,
    11,
    24,
    12,
    37,
    18,
    4,
    37,
    15,
    40,
    23,
    13,
    23,
    22,
    9,
    5,
    32,
    16,
    54,
    66,
    47,
    2,
    11,
    24,
    1,
    2,
    9,
    63,
    2,
    42,
    2,
    1,
    25,
    18,
    22,
    9,
    0,
    8,
    15,
    1,
    3,
    2,
    3,
    43,
    1,
    40,
    47,
    16,
    69,
    17,
    5,
    3,
    36,
    7,
    20,
    4,
    14,
    2,
    8,
    85,
    12,
    5,
    3,
    20,
    9,
    2,
    2,
    1,
    2,
    24,
    10,
    24,
    1,
    1,
    23,
    0,
    55,
    6,
    6,
    26,
    3,
    14,
    5,
    1,
    37,
    7,
    24,
    27,
    18,
    15,
    3,
    8,
    11,
    35,
    11,
    10,
    5,
    32,
    43,
    1,
    7,
    49,
    8,
    4,
    26,
    8,
    22,
    14,
    6,
    3,
    2,
    1,
    15,
    5,
    21,
    2,
    1,
    6,
    30,
    13,
    3,
    2,
    1,
    11,
    1,
    3,
    12,
    1,
    4,
    1,
    12,
    2,
    1,
    7,
    20,
    31,
    23,
    3,
    21,
    6,
    3,
    15,
    5,
    1,
    28,
    3,
    4,
    8,
    18,
    7,
    33,
    15,
    1,
    8,
    15,
    59,
    0,
    22,
    11,
    42,
    1,
    12,
    3,
    1,
    40,
    39,
    9,
    10,
    2,
    6,
    11,
    1,
    11,
    12,
    20,
    3,
    23,
    83,
    1,
    30,
    3,
    5,
    9,
    3,
    29,
    17,
    7,
    13,
    14,
    21,
    35,
    25,
    24,
    42,
    6,
    18,
    14,
    5,
    2,
    14,
    3,
    41,
    11,
    12,
    17,
    8,
    8,
    0,
    29,
    80,
    1,
    7,
    12,
    3,
    37,
    20,
    13,
    5,
    10,
    9,
    19,
    1,
    7,
    14,
    3,
    30,
    16,
    9,
    51,
    32,
    15,
    16,
    1,
    58,
    10,
    19,
    2,
    1,
    25,
    5,
    16,
    21,
    14,
    40,
    1,
    1,
    11,
    1,
    9,
    11,
    29,
    13,
    56,
    1,
    8,
    3,
    3,
    2,
    12,
    46,
    2,
    0,
    5,
    21,
    2,
    1,
    1,
    5,
    1,
    7,
    3,
    26,
    18,
    18,
    1,
    4,
    1,
    21,
    1,
    10,
    10,
    7,
    13,
    12,
    0,
    1,
    1,
    7,
    23,
    10,
    12,
    21,
    11,
    1,
    23,
    7,
    17,
    3,
    15,
    8,
    8,
    35,
    0,
    0,
    22,
    28,
    16,
    6,
    39,
    22,
    19,
    14,
    10,
    25,
    10,
    14,
    9,
    6,
    11,
    3,
    27,
    36,
    5,
    13,
    2,
    22,
    0,
    61,
    44,
    25,
    12,
    10,
    10,
    1,
    18,
    19,
    1,
    0,
    1,
    26,
    22,
    3,
    1,
    15,
    1,
    7,
    16,
    1,
    6,
    35,
    27,
    9,
    2,
    13,
    6,
    18,
    11,
    9,
    18,
    8,
    16,
    4,
    5,
    1,
    27,
    11,
    23,
    36,
    60,
    10,
    22,
    15,
    26,
    44,
    11,
    7,
    1,
    53,
    8,
    2,
    24,
    41,
    23,
    2,
    41,
    23,
    14,
    11,
    8,
    68,
    22,
    64,
    27,
    13,
    9,
    23,
    19,
    37,
    7,
    24,
    16,
    1,
    5,
    20,
    16,
    21,
    7,
    18,
    33,
    19,
    35,
    1,
    38,
    10,
    1,
    57,
    6,
    1,
    7,
    16,
    12,
    15,
    13,
    10,
    1,
    22,
    5,
    19,
    5,
    2,
    44,
    11,
    24,
    2,
    23,
    18,
    2,
    1,
    6,
    33,
    61,
    1,
    16,
    9,
    17,
    22,
    32,
    22,
    23,
    1,
    43,
    8,
    7,
    19,
    4,
    1,
    3,
    4,
    10,
    29,
    5,
    9,
    8,
    10,
    32,
    1,
    4,
    29,
    10,
    21,
    17,
    17,
    9,
    71,
    6,
    16,
    3,
    3,
    33,
    25,
    57,
    5,
    12,
    5,
    66,
    7,
    3,
    22,
    2,
    10,
    28,
    13,
    31,
    11,
    1,
    31,
    6,
    19,
    17,
    12,
    32,
    33,
    20,
    9,
    1,
    10,
    13,
    2,
    10,
    8,
    1,
    7,
    51,
    7,
    2,
    38,
    1,
    19,
    7,
    5,
    42,
    5,
    2,
    17,
    1,
    2,
    9,
    8,
    17,
    81,
    11,
    3,
    7,
    2,
    5,
    65,
    19,
    14,
    19,
    27,
    1,
    36,
    41,
    13,
    5,
    1,
    14,
    0,
    7,
    1,
    4,
    3,
    5,
    3,
    1,
    15,
    6,
    1,
    7,
    26,
    84,
    16,
    57,
    9,
    9,
    4,
    9,
    42,
    3,
    6,
    8,
    15,
    2,
    10,
    19,
    5,
    8,
    1,
    11,
    1,
    6,
    3,
    4,
    15,
    70,
    6,
    17,
    1,
    10,
    1,
    49,
    21,
    7,
    5,
    18,
    2,
    30,
    19,
    46,
    1,
    18,
    10,
    6,
    15,
    3,
    2,
    36,
    21,
    3,
    46,
    1,
    2,
    41,
    1,
    20,
    16,
    21,
    1,
    4,
    12,
    20,
    24,
    20,
    23,
    3,
    14,
    13,
    17,
    20,
    1,
    1,
    5,
    7,
    1,
    6,
    29,
    2,
    3,
    6,
    12,
    2,
    1,
    23,
    39,
    7,
    1,
    14,
    27,
    6,
    12,
    6,
    35,
    14,
    3,
    10,
    34,
    19,
    22,
    1,
    56,
    21,
    31,
    22,
    20,
    13,
    4,
    1,
    16,
    3,
    5,
    8,
    4,
    23,
    1,
    13,
    6,
    20,
    0,
    4,
    5,
    1,
    1,
    4,
    1,
    6,
    16,
    18,
    12,
    10,
    5,
    6,
    28,
    16,
    20,
    1,
    42,
    5,
    4,
    25,
    13,
    27,
    6,
    24,
    1,
    1,
    20,
    5,
    9,
    25,
    5,
    11,
    1,
    74,
    24,
    1,
    1,
    48,
    35,
    21,
    1,
    9,
    26,
    13,
    2,
    18,
    8,
    1,
    3,
    16,
    0,
    13,
    1,
    1,
    2,
    0,
    37,
    20,
    15,
    1,
    4,
    3,
    2,
    1,
    11,
    54,
    28,
    26,
    20,
    8,
    1,
    4,
    17,
    17,
    43,
    6,
    3,
    7,
    5,
    13,
    15,
    23,
    2,
    2,
    26,
    19,
    21,
    8,
    9,
    7,
    1,
    3,
    14,
    8,
    15,
    28,
    1,
    25,
    12,
    65,
    16,
    14,
    1,
    48,
    36,
    28,
    30,
    12,
    5,
    2,
    25,
    55,
    18,
    21,
    21,
    3,
    44,
    6,
    4,
    0,
    2,
    6,
    7,
    8,
    3,
    70,
    6,
    24,
    11,
    1,
    6,
    7,
    0,
    21,
    20,
    30,
    22,
    11,
    0,
    26,
    0,
    4,
    16,
    14,
    65,
    15,
    1,
    68,
    11,
    23,
    1,
    3,
    15,
    1,
    1,
    22,
    21,
    5,
    3,
    1,
    1,
    4,
    19,
    7,
    14,
    9,
    35,
    7,
    43,
    17,
    27,
    17,
    1,
    3,
    49,
    17,
    34,
    1,
    35,
    69,
    12,
    1,
    21,
    7,
    13,
    26,
    12,
    1,
    40,
    11,
    5,
    31,
    2,
    8,
    5,
    1,
    11,
    2,
    5,
    4,
    1,
    20,
    44,
    21,
    26,
    1,
    12,
    8,
    9,
    24,
    10,
    33,
    10,
    64,
    21,
    20,
    22,
    1,
    13,
    21,
    23,
    9,
    35,
    38,
    20,
    31,
    9,
    2,
    9,
    5,
    3,
    6,
    6,
    13,
    3,
    8,
    4,
    123,
    8,
    9,
    1,
    1,
    11,
    43,
    20,
    55,
    15,
    12,
    3,
    1,
    3,
    3,
    51,
    1,
    1,
    34,
    1,
    19,
    18,
    7,
    21,
    7,
    15,
    31,
    5,
    1,
    3,
    1,
    11,
    43,
    0,
    1,
    4,
    15,
    6,
    0,
    6,
    1,
    1,
    4,
    0,
    45,
    20,
    25,
    4,
    2,
    7,
    2,
    2,
    6,
    27,
    24,
    7,
    20,
    29,
    19,
    5,
    25,
    11,
    14,
    1,
    10,
    17,
    28,
    15,
    13,
    20,
    14,
    8,
    6,
    2,
    22,
    11,
    39,
    24,
    32,
    1,
    2,
    9,
    2,
    17,
    2,
    2,
    7,
    7,
    11,
    6,
    3,
    29,
    1,
    13,
    8,
    27,
    19,
    1,
    5,
    9,
    29,
    2,
    1,
    8,
    9,
    19,
    13,
    1,
    5,
    13,
    3,
    3,
    22,
    21,
    28,
    4,
    20,
    15,
    2,
    15,
    20,
    12,
    40,
    1,
    7,
    2,
    4,
    9,
    57,
    13,
    14,
    28,
    122,
    10,
    2,
    8,
    1,
    16,
    1,
    7,
    1,
    1,
    3,
    45,
    6,
    13,
    4,
    4,
    2,
    4,
    19,
    3,
    2,
    4,
    14,
    5,
    0,
    31,
    30,
    0,
    8,
    8,
    18,
    23,
    32,
    2,
    20,
    7,
    11,
    1,
    13,
    17,
    1,
    0,
    7,
    17,
    1,
    3,
    18,
    4,
    8,
    3,
    41,
    3,
    34,
    24,
    15,
    28,
    4,
    26,
    18,
    23,
    8,
    29,
    6,
    3,
    27,
    9,
    9,
    8,
    17,
    7,
    1,
    19,
    5,
    12,
    6,
    25,
    12,
    37,
    2,
    16,
    32,
    13,
    5,
    3,
    14,
    81,
    1,
    4,
    6,
    1,
    24,
    3,
    9,
    25,
    15,
    1,
    11,
    2,
    4,
    1,
    11,
    45,
    51,
    25,
    8,
    9,
    6,
    7,
    9,
    23,
    5,
    39,
    1,
    62,
    1,
    0,
    0,
    10,
    0,
    0,
    23,
    0,
    0,
    0,
    14,
    10,
    10,
    4,
    34,
    2,
    3,
    9,
    11,
    18,
    10,
    6,
    10,
    36,
    55,
    0,
    15,
    0,
    43,
    5,
    2,
    0,
    0,
    0,
    0,
    25,
    13,
    3,
    43,
    21,
    5,
    1,
    3,
    1,
    21,
    11,
    4,
    11,
    1,
    2,
    6,
    22,
    31,
    6,
    92,
    10,
    23,
    22,
    4,
    39,
    7,
    10,
    11,
    18,
    5,
    0,
    7,
    8,
    4,
    3,
    12,
    0,
    23,
    3,
    11,
    20,
    0,
    0,
    2,
    0,
    1,
    3,
    1,
    8,
    32,
    0,
    0,
    0,
    2,
    0,
    0,
    0,
    0,
    1,
    23,
    3,
    0,
    4,
    29,
    6,
    1,
    33,
    14,
    0,
    13,
    39,
    9,
    1,
    5,
    7,
    3,
    6,
    11,
    52,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    8,
    36,
    1,
    1,
    4,
    63,
    0,
    0,
    4,
    0,
    0,
    0,
    0,
    4,
    69,
    1,
    6,
    11,
    12,
    37,
    72,
    9,
    1,
    7,
    0,
    0,
    17,
    9,
    64,
    13,
    17,
    18,
    4,
    17,
    8,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    2,
    1,
    3,
    1,
    1,
    12,
    0,
    0,
    8,
    6,
    61,
    17,
    8,
    1,
    14,
    17,
    2,
    6,
    3,
    0,
    0,
    0,
    0,
    0,
    3,
    2,
    1,
    41,
    15,
    1,
    9,
    32,
    0,
    0,
    0,
    3,
    6,
    21,
    6,
    11,
    0,
    0,
    0,
    0,
    15,
    6,
    0,
    0,
    0,
    17,
    1,
    15,
    74,
    1,
    0,
    7,
    27,
    0,
    0,
    0,
    0,
    4,
    0,
    0,
    1,
    24,
    0,
    10,
    28,
    0,
    0,
    0,
    0,
    13,
    20,
    8,
    36,
    0,
    0,
    1,
    20,
    5,
    2,
    13,
    14,
    34,
    2,
    8,
    11,
    10,
    15,
    7,
    4,
    12,
    8,
    0,
    14,
    0,
    0,
    0,
    20,
    12,
    1,
    0,
    35,
    0,
    71,
    1,
    1,
    0,
    0,
    1,
    16,
    0,
    12,
    0,
    0,
    0,
    0,
    9,
    16,
    12,
    8,
    2,
    10,
    6,
    4,
    8,
    34,
    86,
    2,
    6,
    2,
    9,
    23,
    15,
    3,
    6,
    2,
    3,
    4,
    14,
    16,
    54,
    9,
    3,
    8,
    3,
    15,
    7,
    6,
    33,
    13,
    1,
    7,
    2,
    9,
    1,
    141,
    50,
    6,
    1,
    4,
    30,
    3,
    14,
    13,
    36,
    46,
    18,
    11,
    3,
    40,
    2,
    10,
    7,
    17,
    12,
    2,
    5,
    39,
    3,
    13,
    15,
    38,
    20,
    5,
    20,
    1,
    1,
    3,
    4,
    28,
    13,
    85,
    2,
    21,
    6,
    2,
    10,
    27,
    81,
    3,
    2,
    2,
    25,
    40,
    26,
    26,
    14,
    13,
    12,
    2,
    2,
    7,
    1,
    1,
    1,
    7,
    22,
    20,
    5,
    1,
    22,
    1,
    6,
    8,
    10,
    1,
    17,
    3,
    5,
    7,
    1,
    9,
    13,
    6,
    17,
    3,
    23,
    27,
    1,
    0,
    1,
    19,
    2,
    4,
    16,
    2,
    8,
    1,
    2,
    2,
    5,
    23,
    10,
    1,
    2,
    15,
    24,
    24,
    31,
    5,
    3,
    51,
    1,
    4,
    8,
    19,
    5,
    54,
    27,
    0,
    3,
    20,
    77,
    14,
    1,
    3,
    17,
    8,
    2,
    8,
    9,
    3,
    5,
    1,
    13,
    14,
    4,
    1,
    21,
    1,
    5,
    7,
    6,
    1,
    5,
    5,
    2,
    61,
    42,
    2,
    18,
    6,
    4,
    12,
    22,
    10,
    2,
    8,
    16,
    1,
    7,
    9,
    11,
    41,
    23,
    5,
    15,
    17,
    19,
    4,
    18,
    5,
    9,
    1,
    1,
    12,
    2,
    22,
    13,
    1,
    34,
    3,
    1,
    16,
    26,
    14,
    18,
    2,
    1,
    6,
    14,
    28,
    1,
    38,
    1,
    29,
    41,
    4,
    4,
    3,
    24,
    1,
    1,
    8,
    12,
    5,
    15,
    0,
    4,
    26,
    46,
    19,
    15,
    5,
    3,
    1,
    9,
    23,
    6,
    6,
    3,
    39,
    5,
    25,
    29,
    13,
    13,
    44,
    12,
    28,
    2,
    1,
    36,
    5,
    16,
    11,
    14,
    1,
    13,
    24,
    14,
    9,
    13,
    18,
    5,
    1,
    8,
    63,
    28,
    2,
    11,
    3,
    22,
    3,
    15,
    19,
    21,
    23,
    38,
    1,
    1,
    6,
    17,
    2,
    15,
    19,
    19,
    19,
    4,
    7,
    6,
    22,
    4,
    20,
    17,
    28,
    18,
    5,
    1,
    5,
    7,
    13,
    38,
    19,
    81,
    16,
    15,
    15,
    1,
    23,
    42,
    35,
    44,
    4,
    19,
    26,
    18,
    26,
    17,
    20,
    26,
    3,
    29,
    3,
    8,
    7,
    38,
    19,
    45,
    24,
    17,
    4,
    12,
    28,
    12,
    7,
    1,
    11,
    23,
    9,
    0,
    7,
    55,
    0,
    14,
    12,
    0,
    0,
    1,
    0,
    0,
    33,
    3,
    38,
    1,
    9,
    15,
    0,
    0,
    0,
    1,
    0,
    0,
    0,
    0,
    0,
    1,
    0,
    16,
    22,
    13,
    14,
    0,
    9,
    1,
    26,
    1,
    0,
    0,
    0,
    0,
    0,
    0,
    8,
    0,
    13,
    0,
    0,
    0,
    28,
    4,
    13,
    33,
    0,
    0,
    14,
    10,
    7,
    29,
    32,
    31,
    50,
    3,
    53,
    12,
    1,
    6,
    2,
    57,
    0,
    0,
    0,
    0,
    16,
    0,
    27,
    7,
    10,
    0,
    0,
    0,
    0,
    3,
    19,
    1,
    7,
    13,
    14,
    5,
    3,
    23,
    28,
    40,
    10,
    3,
    15,
    3,
    27,
    0,
    0,
    0,
    47,
    99,
    7,
    2,
    0,
    0,
    25,
    1,
    0,
    9,
    1,
    0,
    22,
    2,
    54,
    0,
    8,
    0,
    0,
    19,
    20,
    9,
    0,
    0,
    17,
    0,
    2,
    0,
    7,
    20,
    33,
    6,
    28,
    8,
    5,
    12,
    1,
    3,
    3,
    23,
    26,
    1,
    27,
    0,
    0,
    2,
    20,
    0,
    0,
    0,
    0,
    0,
    0,
    1,
    8,
    30,
    10,
    60,
    1,
    5,
    0,
    0,
    3,
    0,
    24,
    9,
    7,
    0,
    0,
    1,
    58,
    1,
    0,
    0,
    0,
    0,
    30,
    1,
    1,
    11,
    22,
    1,
    0,
    0,
    0,
    7,
    37,
    0,
    0,
    0,
    6,
    4,
    10,
    12,
    7,
    1,
    0,
    4,
    0,
    22,
    13,
    20,
    44,
    41,
    8,
    28,
    12,
    3,
    12,
    11,
    5,
    10,
    3,
    1,
    59,
    20,
    0,
    1,
    4,
    40,
    1,
    3,
    32,
    0,
    0,
    0,
    27,
    0,
    0,
    1,
    3,
    7,
    2,
    1,
    1,
    11,
    27,
    7,
    4,
    13,
    30,
    2,
    9,
    14,
    2,
    27,
    5,
    1,
    11,
    1,
    1,
    19,
    1,
    9,
    28,
    9,
    19,
    4,
    3,
    0,
    6,
    1,
    26,
    4,
    0,
    14,
    18,
    3,
    20,
    3,
    0,
    3,
    10,
    26,
    69,
    25,
    45,
    22,
    1,
    9,
    11,
    1,
    0,
    9,
    0,
    5,
    0,
    0,
    0,
    3,
    6,
    0,
    1,
    0,
    0,
    0,
    0,
    0,
    0,
    4,
    5,
    1,
    15,
    1,
    29,
    7,
    54,
    10,
    24,
    0,
    0,
    0,
    0,
    0,
    3,
    11,
    0,
    0,
    0,
    1,
    10,
    0,
    27,
    20,
    3,
    0,
    1,
    0,
    0,
    8,
    0,
    0,
    0,
    20,
    39,
    0,
    0,
    3,
    6,
    0,
    4,
    0,
    4,
    0,
    0,
    0,
    0,
    1,
    5,
    38,
    0,
    0,
    0,
    12,
    57,
    0,
    12,
    1,
    1,
    9,
    1,
    16,
    22,
    13,
    25,
    73,
    6,
    3,
    6,
    8,
    4,
    0,
    32,
    15,
    21,
    9,
    4,
    11,
    60,
    3,
    19,
    0,
    1,
    44,
    2,
    10,
    4,
    8,
    8,
    9,
    1,
    22,
    28,
    36,
    11,
    0,
    1,
    2,
    42,
    39,
    4,
    1,
    8,
    1,
    34,
    3,
    34,
    1,
    0,
    0,
    0,
    0,
    0,
    19,
    0,
    0,
    14,
    0,
    0,
    1,
    12,
    1,
    22,
    4,
    5,
    20,
    48,
    15,
    14,
    30,
    0,
    11,
    4,
    0,
    0,
    20,
    3,
    10,
    1,
    25,
    5,
    2,
    50,
    1,
    1,
    30,
    14,
    0,
    3,
    39,
    36,
    3,
    9,
    37,
    1,
    18,
    27,
    1,
    8,
    7,
    0,
    0,
    0,
    14,
    70,
    4,
    5,
    0,
    0,
    0,
    0,
    0,
    0,
    9,
    0,
    0,
    0,
    7,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    4,
    23,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    2,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    14,
    16,
    0,
    0,
    29,
    0,
    0,
    1,
    12,
    7,
    14,
    0,
    0,
    0,
    0,
    0,
    16,
    0,
    2,
    18,
    0,
    13,
    0,
    0,
    12,
    11,
    54,
    4,
    3,
    6,
    59,
    12,
    20,
    0,
    0,
    25,
    9,
    14,
    5,
    3,
    0,
    0,
    0,
    0,
    22,
    10,
    0,
    0,
    16,
    12,
    49,
    9,
    11,
    2,
    2,
    12,
    0,
    15,
    15,
    5,
    1,
    9,
    39,
    1,
    2,
    0,
    0,
    0,
    1,
    0,
    0,
    30,
    8,
    30,
    1,
    0,
    0,
    0,
    0,
    0,
    3,
    1,
    0,
    0,
    0,
    0,
    19,
    5,
    0,
    0,
    5,
    38,
    15,
    2,
    0,
    1,
    67,
    20,
    2,
    63,
    3,
    9,
    13,
    7,
    2,
    17,
    2,
    10,
    53,
    11,
    1,
    28,
    3,
    25,
    1,
    40,
    13,
    10,
    8,
    29,
    5,
    7,
    3,
    14,
    50,
    8,
    10,
    33,
    10,
    14,
    23,
    13,
    11,
    1,
    1,
    18,
    18,
    11,
    14,
    2,
    31,
    0,
    6,
    9,
    2,
    44,
    10,
    17,
    8,
    0,
    7,
    11,
    8,
    21,
    20,
    27,
    14,
    1,
    6,
    5,
    60,
    2,
    23,
    32,
    8,
    47,
    11,
    4,
    4,
    11,
    29,
    21,
    71,
    15,
    6,
    5,
    19,
    23,
    11,
    15,
    0,
    6,
    44,
    1,
    7,
    1,
    2,
    69,
    7,
    3,
    0,
    6,
    2,
    11,
    2,
    18,
    25,
    6,
    0,
    5,
    7,
    31,
    24,
    0,
    5,
    35,
    15,
    1,
    90,
    19,
    0,
    11,
    8,
    6,
    35,
    16,
    2,
    22,
    15,
    6,
    31,
    24,
    9,
    1,
    13,
    2,
    2,
    6,
    26,
    3,
    23,
    11,
    7,
    15,
    4,
    9,
    1,
    11,
    0,
    0,
    1,
    14,
    11,
    0,
    0,
    0,
    0,
    3,
    24,
    6,
    20,
    3,
    11,
    3,
    4,
    6,
    25,
    19,
    1,
    23,
    0,
    28,
    1,
    9,
    1,
    1,
    11,
    13,
    13,
    1,
    26,
    10,
    16,
    35,
    3,
    5,
    0,
    10,
    11,
    0,
    0,
    0,
    19,
    51,
    25,
    2,
    22,
    41,
    1,
    0,
    0,
    0,
    13,
    0,
    0,
    0,
    0,
    0,
    0,
    13,
    1,
    0,
    8,
    9,
    0,
    20,
    1,
    25,
    18,
    3,
    17,
    9,
    0,
    0,
    0,
    0,
    0,
    17,
    3,
    23,
    0,
    21,
    0,
    0,
    7,
    1,
    23,
    13,
    32,
    20,
    33,
    1,
    23,
    7,
    8,
    10,
    68,
    24,
    13,
    9,
    12,
    10,
    1,
    19,
    0,
    0,
    36,
    17,
    0,
    23,
    37,
    18,
    2,
    1,
    9,
    8,
    0,
    0,
    52,
    0,
    0,
    0,
    29,
    0,
    0,
    0,
    0,
    1,
    8,
    15,
    54,
    0,
    0,
    5,
    3,
    1,
    0,
    5,
    38,
    12,
    1,
    27,
    4,
    0,
    3,
    23,
    26,
    2,
    0,
    1,
    0,
    2,
    25,
    0,
    0,
    0,
    14,
    9,
    9,
    0,
    0,
    0,
    2,
    9,
    3,
    21,
    3,
    0,
    0,
    13,
    9,
    0,
    9,
    16,
    0,
    5,
    4,
    9,
    37,
    2,
    8,
    8,
    21,
    0,
    3,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    1,
    1,
    0,
    0,
    8,
    0,
    0,
    2,
    8,
    20,
    14,
    32,
    25,
    8,
    20,
    0,
    25,
    21,
    5,
    1,
    0,
    0,
    0,
    0,
    0,
    0,
    12,
    8,
    0,
    3,
    6,
    0,
    27,
    11,
    0,
    10,
    0,
    16,
    4,
    2,
    0,
    7,
    0,
    0,
    7,
    0,
    0,
    9,
    29,
    3,
    11,
    11,
    0,
    6,
    0,
    1,
    5,
    0,
    0,
    0,
    0,
    0,
    3,
    7,
    6,
    3,
    3,
    14,
    5,
    1,
    18,
    23,
    1,
    1,
    12,
    10,
    19,
    13,
    3,
    35,
    22,
    50,
    4,
    1,
    1,
    11,
    8,
    9,
    5,
    27,
    15,
    27,
    30,
    4,
    13,
    1,
    1,
    4,
    2,
    52,
    9,
    14,
    3,
    6,
    1,
    11,
    16,
    13,
    24,
    35,
    1,
    5,
    52,
    1,
    15,
    3,
    16,
    12,
    1,
    1,
    14,
    0,
    0,
    0,
    0,
    5,
    11,
    10,
    12,
    7,
    4,
    12,
    6,
    0,
    26,
    0,
    0,
    0,
    0,
    6,
    0,
    6,
    38,
    3,
    13,
    3,
    0,
    0,
    0,
    0,
    30,
    5,
    13,
    24,
    47,
    3,
    11,
    69,
    6,
    1,
    32,
    0,
    0,
    0,
    2,
    65,
    56,
    16,
    5,
    2,
    5,
    0,
    3,
    28,
    0,
    17,
    0,
    0,
    0,
    0,
    20,
    0,
    0,
    0,
    9,
    29,
    12,
    4,
    24,
    3,
    0,
    6,
    0,
    0,
    0,
    0,
    0,
    0,
    9,
    0,
    0,
    17,
    2,
    42,
    16,
    5,
    1,
    0,
    36,
    0,
    0,
    1,
    1,
    5,
    14,
    14,
    44,
    0,
    22,
    8,
    0,
    55,
    7,
    4,
    16,
    0,
    0,
    0,
    4,
    4,
    9,
    2,
    2,
    21,
    3,
    1,
    0,
    0,
    0,
    0,
    0,
    39,
    0,
    0,
    1,
    13,
    0,
    2,
    8,
    72,
    0,
    8,
    59,
    1,
    0,
    40,
    0,
    0,
    34,
    15,
    22,
    1,
    36,
    42,
    0,
    6,
    0,
    0,
    10,
    0,
    0,
    0,
    0,
    0,
    0,
    4,
    0,
    0,
    0,
    0,
    11,
    0,
    0,
    1,
    0,
    14,
    25,
    0,
    0,
    17,
    2,
    8,
    36,
    3,
    12,
    5,
    2,
    19,
    39,
    5,
    1,
    54,
    0,
    4,
    3,
    10,
    10,
    8,
    5,
    6,
    1,
    3,
    6,
    3,
    0,
    0,
    3,
    8,
    33,
    0,
    0,
    1,
    0,
    5,
    7,
    30,
    0,
    0,
    0,
    0,
    22,
    7,
    6,
    23,
    12,
    1,
    26,
    1,
    5,
    4,
    41,
    29,
    38,
    37,
    0,
    0,
    23,
    6,
    0,
    0,
    14,
    19,
    12,
    12,
    0,
    7,
    19,
    0,
    0,
    19,
    25,
    3,
    12,
    9,
    59,
    0,
    0,
    0,
    0,
    16,
    23,
    2,
    1,
    18,
    3,
    5,
    1,
    14,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    5,
    0,
    11,
    58,
    0,
    5,
    9,
    3,
    13,
    21,
    1,
    5,
    4,
    25,
    24,
    16,
    1,
    36,
    4,
    12,
    5,
    16,
    1,
    1,
    0,
    0,
    0,
    0,
    9,
    24,
    28,
    4,
    0,
    42,
    1,
    1,
    25,
    13,
    0,
    0,
    0,
    31,
    2,
    4,
    29,
    28,
    3,
    1,
    1,
    9,
    0,
    5,
    2,
    14,
    10,
    13,
    12,
    9,
    68,
    2,
    9,
    7,
    28,
    0,
    0,
    33,
    9,
    12,
    5,
    3,
    16,
    12,
    28,
    18,
    0,
    5,
    28,
    1,
    16,
    6,
    14,
    18,
    56,
    22,
    11,
    0,
    5,
    1,
    9,
    5,
    31,
    5,
    30,
    9,
    34,
    1,
    5,
    11,
    9,
    21,
    1,
    38,
    30,
    1,
    0,
    53,
    8,
    9,
    2,
    1,
    26,
    2,
    47,
    0,
    1,
    6,
    9,
    0,
    6,
    74,
    6,
    1,
    0,
    0,
    11,
    83,
    9,
    2,
    14,
    8,
    3,
    15,
    16,
    6,
    1,
    41,
    0,
    2,
    5,
    53,
    13,
    0,
    0,
    28,
    56,
    17,
    24,
    38,
    5,
    0,
    20,
    2,
    60,
    1,
    0,
    0,
    0,
    8,
    38,
    15,
    1,
    78,
    10,
    1,
    29,
    2,
    25,
    60,
    12,
    10,
    15,
    9,
    7,
    21,
    1,
    11,
    11,
    0,
    41,
    2,
    34,
    5,
    8,
    11,
    0,
    9,
    0,
    0,
    0,
    6,
    2,
    14,
    119,
    0,
    0,
    0,
    11,
    0,
    0,
    12,
    0,
    0,
    0,
    11,
    0,
    0,
    0,
    0,
    0,
    0,
    30,
    2,
    29,
    42,
    44,
    0,
    0,
    2,
    12,
    12,
    0,
    0,
    0,
    5,
    7,
    0,
    1,
    0,
    22,
    0,
    1,
    7,
    25,
    3,
    30,
    7,
    21,
    0,
    0,
    18,
    0,
    31,
    0,
    0,
    5,
    14,
    44,
    11,
    36,
    25,
    10,
    4,
    11,
    11,
    18,
    17,
    0,
    1,
    5,
    1,
    11,
    22,
    5,
    18,
    7,
    17,
    21,
    1,
    0,
    0,
    0,
    0,
    13,
    84,
    3,
    1,
    0,
    2,
    0,
    0,
    0,
    0,
    3,
    0,
    0,
    10,
    0,
    7,
    0,
    0,
    18,
    7,
    11,
    11,
    11,
    12,
    0,
    0,
    5,
    8,
    21,
    6,
    14,
    25,
    15,
    0,
    18,
    10,
    1,
    0,
    23,
    4,
    16,
    3,
    9,
    18,
    12,
    0,
    42,
    6,
    14,
    25,
    1,
    3,
    1,
    2,
    1,
    9,
    1,
    0,
    85,
    14,
    0,
    0,
    0,
    35,
    29,
    60,
    3,
    7,
    2,
    0,
    27,
    12,
    0,
    0,
    4,
    2,
    53,
    3,
    1,
    39,
    10,
    14,
    2,
    0,
    9,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    3,
    0,
    0,
    0,
    0,
    0,
    0,
    3,
    2,
    1,
    5,
    26,
    1,
    2,
    19,
    0,
    12,
    18,
    0,
    9,
    36,
    1,
    1,
    4,
    1,
    11,
    8,
    84,
    10,
    0,
    0,
    0,
    0,
    0,
    2,
    0,
    0,
    30,
    1,
    13,
    127,
    2,
    5,
    14,
    3,
    0,
    0,
    44,
    0,
    0,
    0,
    0,
    0,
    0,
    1,
    22,
    26,
    0,
    0,
    17,
    0,
    0,
    0,
    0,
    5,
    43,
    0,
    8,
    33,
    15,
    14,
    21,
    8,
    0,
    0,
    0,
    47,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    26,
    1,
    28,
    12,
    21,
    0,
    33,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    22,
    0,
    0,
    0,
    0,
    0,
    5,
    1,
    0,
    9,
    0,
    54,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    4,
    17,
    22,
    57,
    13,
    3,
    1,
    2,
    0,
    0,
    0,
    120,
    1,
    23,
    8,
    0,
    0,
    0,
    0,
    11,
    15,
    0,
    15,
    2,
    0,
    0,
    32,
    4,
    0,
    0,
    0,
    0,
    2,
    6,
    0,
    0,
    0,
    0,
    0,
    9,
    0,
    1,
    9,
    0,
    0,
    3,
    9,
    3,
    2,
    34,
    12,
    20,
    1,
    3,
    5,
    22,
    7,
    60,
    32,
    29,
    51,
    8,
    28,
    135,
    18,
    0,
    0,
    0,
    0,
    3,
    28,
    4,
    6,
    0,
    0,
    18,
    8,
    0,
    0,
    8,
    0,
    1,
    3,
    0,
    0,
    0,
    0,
    0,
    19,
    40,
    2,
    1,
    7,
    43,
    4,
    1,
    10,
    3,
    27,
    8,
    8,
    26,
    9,
    1,
    10,
    6,
    11,
    3,
    9,
    13,
    8,
    11,
    0,
    5,
    9,
    14,
    19,
    5,
    22,
    21,
    3,
    3,
    15,
    1,
    51,
    25,
    16,
    5,
    7,
    11,
    5,
    3,
    33,
    2,
    19,
    0,
    0,
    62,
    2,
    15,
    0,
    2,
    0,
    0,
    0,
    14,
    15,
    32,
    13,
    61,
    0,
    0,
    0,
    0,
    19,
    14,
    9,
    4,
    4,
    42,
    10,
    0,
    3,
    1,
    2,
    16,
    30,
    42,
    0,
    0,
    69,
    58,
    4,
    0,
    0,
    24,
    0,
    10,
    7,
    11,
    0,
    2,
    29,
    0,
    9,
    3,
    0,
    35,
    0,
    0,
    0,
    1,
    2,
    9,
    10,
    7,
    1,
    16,
    13,
    0,
    0,
    55,
    5,
    7,
    26,
    0,
    0,
    1,
    9,
    21,
    5,
    8,
    21,
    0,
    0,
    7,
    1,
    28,
    2,
    0,
    0,
    0,
    30,
    3,
    3,
    11,
    0,
    2,
    15,
    12,
    0,
    0,
    0,
    0,
    6,
    38,
    12,
    5,
    0,
    0,
    0,
    22,
    15,
    3,
    11,
    15,
    23,
    2,
    0,
    13,
    2,
    24,
    5,
    10,
    1,
    29,
    2,
    39,
    7,
    0,
    21,
    5,
    16,
    1,
    23,
    58,
    11,
    42,
    24,
    0,
    16,
    0,
    18,
    6,
    26,
    46,
    13,
    0,
    10,
    6,
    1,
    0,
    0,
    1,
    0,
    0,
    0,
    0,
    0,
    0,
    1,
    0,
    0,
    0,
    0,
    0,
    6,
    0,
    0,
    0,
    12,
    0,
    0,
    0,
    0,
    12,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    4,
    0,
    0,
    45,
    3,
    27,
    0,
    0,
    27,
    4,
    10,
    22,
    0,
    0,
    0,
    13,
    0,
    7,
    0,
    0,
    0,
    3,
    9,
    13,
    7,
    23,
    34,
    12,
    0,
    0,
    0,
    0,
    21,
    16,
    22,
    11,
    0,
    0,
    0,
    41,
    57,
    0,
    5,
    0,
    0,
    0,
    9,
    8,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    28,
    7,
    0,
    0,
    0,
    0,
    4,
    0,
    0,
    0,
    0,
    19,
    29,
    12,
    4,
    56,
    1,
    5,
    19,
    14,
    12,
    1,
    62,
    2,
    6,
    4,
    4,
    5,
    3,
    23,
    14,
    31,
    20,
    9,
    14,
    9,
    0,
    0,
    0,
    0,
    0,
    0,
    34,
    44,
    2,
    11,
    6,
    33,
    1,
    53,
    31,
    1,
    30,
    18,
    4,
    0,
    4,
    4,
    20,
    4,
    8,
    87,
    58,
    17,
    16,
    3,
    2,
    1,
    6,
    6,
    5,
    5,
    12,
    4,
    3,
    8,
    52,
    4,
    32,
    10,
    32,
    21,
    1,
    57,
    0,
    20,
    7,
    4,
    2,
    16,
    56,
    30,
    1,
    32,
    0,
    28,
    1,
    14,
    12,
    1,
    12,
    4,
    1,
    12,
    6,
    24,
    27,
    1,
    8,
    8,
    3,
    14,
    19,
    1,
    11,
    10,
    5,
    5,
    6,
    10,
    5,
    12,
    22,
    8,
    2,
    16,
    19,
    4,
    29,
    5,
    21,
    4,
    8,
    23,
    4,
    21,
    7,
    136,
    4,
    1,
    8,
    3,
    49,
    17,
    4,
    23,
    14,
    18,
    12,
    5,
    22,
    9,
    7,
    1,
    6,
    38,
    1,
    2,
    18,
    16,
    15,
    12,
    14,
    18,
    11,
    15,
    1,
    10,
    8,
    23,
    4,
    23,
    11,
    17,
    1,
    27,
    12,
    14,
    3,
    29,
    5,
    3,
    38,
    25,
    44,
    3,
    3,
    32,
    10,
    2,
    1,
    11,
    16,
    27,
    13,
    1,
    53,
    5,
    3,
    7,
    7,
    13,
    28,
    0,
    2,
    9,
    7,
    5,
    18,
    10,
    18,
    3,
    9,
    55,
    2,
    26,
    1,
    3,
    6,
    2,
    10,
    6,
    14,
    4,
    1,
    3,
    11,
    1,
    7,
    2,
    17,
    20,
    19,
    61,
    2,
    16,
    3,
    30,
    10,
    14,
    3,
    7,
    13,
    4,
    10,
    3,
    6,
    58,
    4,
    1,
    16,
    19,
    1,
    12,
    5,
    9,
    3,
    32,
    6,
    41,
    9,
    23,
    1,
    25,
    31,
    5,
    3,
    26,
    1,
    1,
    48,
    12,
    24,
    2,
    3,
    7,
    2,
    12,
    3,
    1,
    38,
    1,
    10,
    66,
    1,
    15,
    1,
    12,
    5,
    4,
    9,
    19,
    13,
    14,
    0,
    22,
    1,
    2,
    42,
    1,
    4,
    5,
    6,
    10,
    33,
    34,
    79,
    2,
    1,
    33,
    26,
    13,
    2,
    7,
    1,
    18,
    13,
    28,
    0,
    0,
    0,
    0,
    0,
    0,
    17,
    0,
    0,
    0,
    12,
    0,
    0,
    17,
    11,
    0,
    1,
    0,
    0,
    1,
    10,
    3,
    14,
    3,
    42,
    2,
    10,
    3,
    5,
    46,
    37,
    2,
    8,
    6,
    2,
    1,
    3,
    10,
    13,
    0,
    0,
    0,
    8,
    9,
    8,
    0,
    3,
    6,
    1,
    0,
    14,
    30,
    1,
    13,
    64,
    1,
    7,
    21,
    39,
    13,
    1,
    8,
    0,
    2,
    2,
    0,
    14,
    63,
    0,
    3,
    13,
    24,
    3,
    4,
    2,
    25,
    15,
    0,
    0,
    0,
    1,
    0,
    0,
    0,
    7,
    2,
    37,
    0,
    0,
    0,
    0,
    13,
    14,
    12,
    0,
    0,
    3,
    0,
    18,
    53,
    0,
    0,
    0,
    33,
    9,
    12,
    18,
    0,
    0,
    7,
    1,
    0,
    36,
    43,
    10,
    9,
    34,
    3,
    5,
    9,
    3,
    0,
    7,
    19,
    5,
    0,
    0,
    44,
    15,
    0,
    0,
    1,
    18,
    14,
    3,
    0,
    3,
    0,
    8,
    24,
    1,
    1,
    1,
    2,
    27,
    1,
    6,
    2,
    17,
    1,
    2,
    48,
    10,
    3,
    1,
    14,
    46,
    0,
    0,
    0,
    0,
    0,
    4,
    5,
    11,
    0,
    0,
    3,
    1,
    40,
    11,
    25,
    9,
    8,
    6,
    9,
    11,
    4,
    13,
    3,
    10,
    30,
    10,
    0,
    16,
    27,
    0,
    0,
    0,
    0,
    13,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    61,
    0,
    11,
    8,
    13,
    8,
    0,
    0,
    3,
    26,
    0,
    5,
    4,
    0,
    0,
    7,
    18,
    10,
    46,
    11,
    0,
    0,
    0,
    0,
    0,
    19,
    16,
    1,
    0,
    0,
    5,
    1,
    0,
    9,
    0,
    0,
    1,
    0,
    0,
    4,
    3,
    0,
    0,
    41,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    17,
    0,
    0,
    0,
    1,
    4,
    22,
    19,
    3,
    3,
    1,
    3,
    11,
    14,
    1,
    14,
    13,
    1,
    3,
    24,
    3,
    13,
    55,
    0,
    0,
    18,
    1,
    5,
    0,
    0,
    0,
    0,
    22,
    0,
    0,
    4,
    24,
    4,
    0,
    0,
    5,
    0,
    15,
    1,
    0,
    0,
    0,
    0,
    0,
    0,
    5,
    13,
    0,
    0,
    6,
    1,
    0,
    5,
    1,
    0,
    0,
    0,
    9,
    54,
    6,
    26,
    22,
    9,
    4,
    7,
    5,
    46,
    8,
    34,
    3,
    15,
    18,
    0,
    3,
    0,
    17,
    1,
    4,
    9,
    4,
    12,
    8,
    3,
    2,
    0,
    7,
    17,
    0,
    0,
    0,
    46,
    12,
    39,
    1,
    6,
    0,
    54,
    31,
    0,
    10,
    2,
    0,
    0,
    0,
    0,
    0,
    43,
    0,
    13,
    0,
    2,
    0,
    10,
    84,
    14,
    40,
    18,
    0,
    0,
    0,
    44,
    0,
    0,
    0,
    0,
    4,
    21,
    6,
    1,
    3,
    0,
    0,
    0,
    6,
    0,
    0,
    13,
    1,
    0,
    14,
    49,
    13,
    0,
    1,
    45,
    5,
    14,
    22,
    0,
    0,
    9,
    12,
    0,
    0,
    0,
    0,
    5,
    0,
    0,
    0,
    0,
    8,
    0,
    0,
    0,
    0,
    0,
    3,
    39,
    1,
    0,
    0,
    51,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    17,
    5,
    24,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    41,
    2,
    18,
    5,
    2,
    0,
    2,
    0,
    0,
    0,
    0,
    0,
    0,
    1,
    0,
    11,
    8,
    19,
    47,
    0,
    0,
    0,
    0,
    1,
    1,
    0,
    0,
    0,
    23,
    0,
    0,
    2,
    16,
    0,
    21,
    7,
    123,
    0,
    32,
    25,
    0,
    0,
    44,
    5,
    4,
    3,
    36,
    2,
    8,
    9,
    10,
    36,
    5,
    0,
    0,
    0,
    6,
    4,
    4,
    3,
    1,
    4,
    15,
    13,
    1,
    0,
    0,
    3,
    0,
    0,
    0,
    0,
    4,
    29,
    16,
    0,
    6,
    7,
    1,
    2,
    1,
    5,
    12,
    1,
    6,
    10,
    2,
    8,
    16,
    26,
    2,
    25,
    18,
    40,
    2,
    4,
    5,
    1,
    1,
    29,
    19,
    45,
    5,
    48,
    1,
    27,
    24,
    8,
    51,
    9,
    22,
    24,
    3,
    10,
    68,
    2,
    6,
    5,
    1,
    3,
    22,
    9,
    40,
    1,
    12,
    18,
    21,
    4,
    5,
    23,
    9,
    46,
    58,
    12,
    1,
    19,
    5,
    0,
    0,
    3,
    0,
    0,
    2,
    1,
    1,
    10,
    0,
    0,
    0,
    0,
    6,
    7,
    1,
    0,
    0,
    0,
    17,
    0,
    0,
    6,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    1,
    0,
    0,
    0,
    1,
    1,
    40,
    3,
    49,
    10,
    0,
    38,
    1,
    15,
    24,
    0,
    14,
    77,
    30,
    9,
    38,
    9,
    26,
    53,
    12,
    42,
    0,
    5,
    0,
    0,
    0,
    36,
    1,
    6,
    0,
    5,
    10,
    10,
    17,
    0,
    0,
    0,
    2,
    9,
    18,
    19,
    35,
    1,
    0,
    0,
    12,
    10,
    11,
    9,
    0,
    0,
    13,
    17,
    0,
    0,
    68,
    25,
    66,
    4,
    0,
    0,
    0,
    0,
    13,
    15,
    0,
    0,
    1,
    0,
    0,
    0,
    1,
    14,
    16,
    7,
    9,
    16,
    6,
    3,
    31,
    3,
    21,
    5,
    3,
    4,
    6,
    7,
    2,
    32,
    16,
    0,
    1,
    4,
    37,
    0,
    0,
    1,
    32,
    0,
    0,
    47,
    21,
    6,
    6,
    0,
    0,
    16,
    0,
    19,
    0,
    0,
    104,
    0,
    18,
    0,
    0,
    0,
    0,
    0,
    14,
    0,
    0,
    0,
    6,
    38,
    0,
    0,
    0,
    14,
    1,
    3,
    2,
    44,
    1,
    3,
    23,
    11,
    12,
    0,
    0,
    0,
    0,
    0,
    18,
    25,
    7,
    16,
    0,
    0,
    3,
    10,
    0,
    2,
    16,
    0,
    0,
    31,
    0,
    16,
    14,
    12,
    19,
    9,
    0,
    0,
    1,
    7,
    11,
    25,
    3,
    26,
    62,
    5,
    3,
    49,
    4,
    14,
    15,
    4,
    6,
    3,
    23,
    17,
    38,
    9,
    4,
    11,
    63,
    16,
    15,
    1,
    8,
    0,
    19,
    35,
    7,
    14,
    46,
    0,
    6,
    0,
    0,
    0,
    47,
    60,
    0,
    0,
    1,
    77,
    0,
    0,
    1,
    2,
    0,
    0,
    0,
    2,
    2,
    0,
    0,
    16,
    3,
    10,
    0,
    10,
    0,
    0,
    21,
    9,
    5,
    1,
    12,
    5,
    7,
    2,
    7,
    2,
    16,
    1,
    75,
    1,
    0,
    0,
    3,
    0,
    0,
    0,
    0,
    0,
    1,
    8,
    0,
    0,
    31,
    0,
    3,
    22,
    28,
    4,
    32,
    2,
    2,
    16,
    5,
    8,
    2,
    23,
    46,
    2,
    16,
    19,
    9,
    10,
    3,
    34,
    2,
    0,
    0,
    0,
    11,
    30,
    0,
    0,
    0,
    0,
    20,
    4,
    0,
    0,
    0,
    0,
    0,
    10,
    3,
    0,
    10,
    48,
    1,
    13,
    1,
    22,
    6,
    14,
    6,
    0,
    3,
    86,
    0,
    0,
    7,
    11,
    2,
    42,
    0,
    15,
    18,
    0,
    0,
    0,
    0,
    68,
    0,
    15,
    10,
    0,
    6,
    10,
    20,
    16,
    15,
    0,
    9,
    9,
    15,
    38,
    7,
    1,
    2,
    0,
    0,
    1,
    7,
    0,
    0,
    8,
    16,
    0,
    9,
    8,
    37,
    23,
    1,
    7,
    49,
    0,
    0,
    74,
    73,
    0,
    0,
    10,
    26,
    5,
    14,
    1,
    6,
    56,
    5,
    0,
    0,
    0,
    2,
    18,
    18,
    0,
    11,
    4,
    1,
    0,
    0,
    0,
    0,
    0,
    5,
    5,
    9,
    4,
    39,
    0,
    6,
    11,
    0,
    31,
    8,
    28,
    0,
    0,
    10,
    14,
    0,
    1,
    15,
    53,
    10,
    25,
    8,
    1,
    4,
    0,
    34,
    26,
    2,
    3,
    35,
    0,
    0,
    0,
    0,
    19,
    2,
    0,
    0,
    3,
    3,
    2,
    0,
    0,
    0,
    4,
    19,
    5,
    7,
    13,
    46,
    0,
    7,
    1,
    18,
    4,
    0,
    4,
    0,
    12,
    50,
    0,
    66,
    19,
    5,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    4,
    19,
    24,
    25,
    4,
    0,
    0,
    0,
    2,
    6,
    0,
    11,
    0,
    0,
    0,
    7,
    42,
    2,
    3,
    6,
    0,
    0,
    0,
    3,
    0,
    0,
    7,
    0,
    0,
    0,
    0,
    7,
    0,
    0,
    0,
    13,
    0,
    0,
    42,
    39,
    19,
    12,
    1,
    0,
    0,
    0,
    0,
    0,
    3,
    22,
    31,
    94,
    21,
    7,
    1,
    6,
    4,
    43,
    0,
    0,
    0,
    30,
    0,
    0,
    0,
    0,
    0,
    0,
    2,
    1,
    0,
    0,
    0,
    0,
    0,
    0,
    18,
    32,
    45,
    27,
    35,
    20,
    3,
    16,
    31,
    50,
    48,
    1,
    15,
    2,
    5,
    4,
    38,
    11,
    0,
    0,
    0,
    28,
    5,
    2,
    0,
    12,
    9,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    6,
    12,
    11,
    0,
    0,
    3,
    22,
    0,
    0,
    0,
    0,
    0,
    8,
    8,
    10,
    21,
    1,
    3,
    9,
    1,
    6,
    4,
    39,
    12,
    14,
    25,
    6,
    14,
    0,
    0,
    14,
    2,
    0,
    5,
    0,
    0,
    6,
    2,
    0,
    34,
    6,
    1,
    11,
    0,
    0,
    0,
    0,
    0,
    0,
    85,
    4,
    4,
    0,
    3,
    36,
    26,
    4,
    5,
    7,
    54,
    0,
    0,
    0,
    0,
    24,
    3,
    10,
    1,
    13,
    0,
    3,
    27,
    1,
    0,
    0,
    43,
    2,
    12,
    9,
    4,
    4,
    1,
    28,
    0,
    0,
    0,
    0,
    0,
    11,
    10,
    24,
    11,
    6,
    3,
    3,
    9,
    1,
    9,
    0,
    0,
    36,
    4,
    29,
    13,
    7,
    10,
    16,
    2,
    19,
    13,
    19,
    21,
    7,
    1,
    6,
    10,
    4,
    51,
    15,
    2,
    26,
    21,
    3,
    5,
    7,
    18,
    9,
    7,
    7,
    1,
    11,
    0,
    0,
    15,
    7,
    2,
    1,
    2,
    33,
    0,
    11,
    10,
    0,
    0,
    33,
    42,
    16,
    46,
    17,
    7,
    6,
    3,
    9,
    17,
    35,
    4,
    12,
    1,
    1,
    20,
    51,
    2,
    13,
    26,
    1,
    7,
    13,
    3,
    6,
    4,
    4,
    1,
    2,
    42,
    2,
    18,
    1,
    17,
    0,
    0,
    1,
    9,
    0,
    22,
    8,
    1,
    1,
    6,
    17,
    2,
    3,
    17,
    1,
    20,
    8,
    4,
    3,
    23,
    33,
    3,
    16,
    15,
    22,
    9,
    14,
    3,
    2,
    52,
    4,
    1,
    29,
    4,
    1,
    27,
    1,
    3,
    9,
    2,
    2,
    1,
    24,
    2,
    10,
    0,
    0,
    0,
    51,
    11,
    0,
    0,
    0,
    3,
    0,
    0,
    0,
    0,
    8,
    1,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    15,
    0,
    0,
    15,
    17,
    14,
    0,
    0,
    4,
    13,
    59,
    26,
    0,
    0,
    12,
    1,
    1,
    14,
    0,
    7,
    5,
    28,
    6,
    21,
    8,
    9,
    15,
    2,
    9,
    0,
    0,
    0,
    0,
    2,
    25,
    7,
    0,
    6,
    6,
    14,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    21,
    22,
    0,
    0,
    0,
    3,
    0,
    0,
    1,
    2,
    46,
    25,
    21,
    0,
    3,
    23,
    5,
    2,
    8,
    10,
    1,
    38,
    0,
    26,
    4,
    3,
    6,
    15,
    27,
    7,
    17,
    4,
    4,
    8,
    43,
    1,
    13,
    58,
    14,
    6,
    24,
    10,
    7,
    16,
    35,
    1,
    22,
    5,
    1,
    1,
    1,
    17,
    11,
    6,
    7,
    3,
    8,
    4,
    3,
    5,
    13,
    25,
    9,
    7,
    1,
    16,
    1,
    0,
    0,
    10,
    12,
    20,
    30,
    8,
    0,
    12,
    30,
    1,
    4,
    12,
    1,
    11,
    0,
    11,
    1,
    25,
    0,
    21,
    0,
    0,
    0,
    41,
    9,
    4,
    27,
    12,
    2,
    2,
    48,
    19,
    44,
    0,
    0,
    5,
    150,
    4,
    0,
    0,
    10,
    2,
    10,
    13,
    19,
    1,
    24,
    4,
    5,
    20,
    0,
    35,
    16,
    0,
    0,
    0,
    1,
    50,
    4,
    33,
    14,
    48,
    28,
    15,
    5,
    60,
    16,
    2,
    3,
    6,
    11,
    0,
    0,
    21,
    1,
    3,
    2,
    0,
    0,
    0,
    59,
    23,
    1,
    0,
    10,
    2,
    0,
    0,
    8,
    9,
    0,
    51,
    2,
    10,
    4,
    2,
    35,
    69,
    5,
    6,
    1,
    74,
    9,
    11,
    4,
    29,
    1,
    7,
    6,
    8,
    7,
    14,
    4,
    11,
    4,
    15,
    9,
    18,
    12,
    3,
    1,
    2,
    5,
    10,
    23,
    9,
    2,
    1,
    6,
    7,
    2,
    102,
    30,
    2,
    9,
    10,
    1,
    3,
    18,
    1,
    9,
    31,
    9,
    22,
    15,
    48,
    1,
    2,
    19,
    3,
    65,
    2,
    10,
    10,
    20,
    14,
    18,
    14,
    11,
    1,
    18,
    8,
    6,
    1,
    0,
    16,
    18,
    1,
    2,
    28,
    3,
    20,
    13,
    5,
    3,
    12,
    30,
    17,
    1,
    7,
    6,
    10,
    4,
    3,
    3,
    15,
    32,
    1,
    0,
    13,
    6,
    4,
    2,
    7,
    4,
    6,
    10,
    5,
    50,
    7,
    4,
    32,
    15,
    1,
    3,
    9,
    21,
    55,
    6,
    0,
    0,
    5,
    0,
    6,
    20,
    23,
    30,
    0,
    9,
    1,
    28,
    0,
    12,
    0,
    29,
    0,
    6,
    46,
    61,
    6,
    4,
    29,
    18,
    27,
    7,
    5,
    11,
    21,
    4,
    12,
    12,
    16,
    3,
    12,
    2,
    5,
    2,
    47,
    0,
    12,
    1,
    13,
    1,
    36,
    4,
    14,
    1,
    1,
    14,
    0,
    0,
    0,
    33,
    9,
    4,
    9,
    9,
    16,
    1,
    6,
    15,
    9,
    35,
    17,
    11,
    3,
    35,
    2,
    7,
    4,
    9,
    1,
    4,
    6,
    31,
    2,
    7,
    1,
    1,
    3,
    18,
    4,
    14,
    4,
    9,
    0,
    9,
    6,
    5,
    2,
    6,
    3,
    36,
    8,
    41,
    11,
    29,
    10,
    1,
    0,
    2,
    19,
    17,
    9,
    26,
    11,
    10,
    11,
    1,
    6,
    3,
    10,
    29,
    23,
    28,
    1,
    13,
    7,
    16,
    88,
    15,
    58,
    65,
    1,
    2,
    40,
    13,
    7,
    1,
    28,
    12,
    22,
    28,
    11,
    16,
    13,
    7,
    45,
    1,
    9,
    53,
    6,
    20,
    1,
    14,
    16,
    6,
    18,
    24,
    8,
    26,
    6,
    1,
    31,
    6,
    1,
    1,
    10,
    22,
    7,
    63,
    29,
    0,
    2,
    0,
    0,
    2,
    29,
    19,
    2,
    18,
    24,
    7,
    41,
    4,
    2,
    2,
    23,
    1,
    17,
    7,
    8,
    54,
    26,
    0,
    10,
    0,
    22,
    1,
    3,
    48,
    11,
    1,
    30,
    22,
    32,
    21,
    18,
    6,
    5,
    1,
    19,
    4,
    5,
    5,
    2,
    29,
    16,
    22,
    44,
    8,
    10,
    22,
    2,
    17,
    7,
    7,
    14,
    1,
    18,
    36,
    11,
    89,
    1,
    17,
    1,
    7,
    4,
    1
  ],
  "ttfts": [
    0.052265542006352916,
    0.06853456099634059,
    0.1164955439962796,
    0.4882100969989551,
    0.7411276060011005,
    0.6281069550022949,
    0.5774895419963286,
    0.5969591309985844,
    0.5919587300013518,
    0.5566056999959983,
    0.5277427370019723,
    0.9522850600042148,
    1.2273776859947247,
    1.1998503630020423,
    1.1715681030036649,
    1.274170545002562,
    1.336851772000955,
    1.327572322996275,
    1.4407573089993093,
    1.5782937839976512,
    1.5776117899949895,
    1.607483398001932,
    1.5847441669975524,
    1.5078704560000915,
    1.792740643999423,
    1.72843308699521,
    1.6752028799965046,
    1.6240745519971824,
    1.688847771998553,
    1.7932761200063396,
    1.709018820001802,
    1.5480406880014925,
    1.5110467139966204,
    1.4693565749985282,
    0.0,
    2.759842369996477,
    2.874621727001795,
    2.736381558002904,
    2.716160073003266,
    2.6672495789971435,
    2.6652469640030176,
    2.590452328004176,
    2.5331250029994408,
    2.76568251199933,
    2.6269090280038654,
    2.5976786379978876,
    2.566402456002834,
    2.5499154299977818,
    2.5028758960033883,
    2.289975901003345,
    2.407284360000631,
    2.3847938839971903,
    2.3619311720030964,
    2.2970060239968006,
    0.0,
    2.57075564799743,
    2.5673017969966168,
    2.609023024000635,
    2.586421779997181,
    2.4700870550004765,
    2.4562447059943224,
    2.341308426002797,
    2.427706262002175,
    2.36443871800293,
    2.2512054600010742,
    2.1790655069999048,
    2.070570765994489,
    2.1700221549981507,
    2.268713174002187,
    2.2009489929987467,
    2.181487472997105,
    2.1096148659999017,
    2.213544156002172,
    2.1498844559973804,
    2.1314262309970218,
    2.22515576799924,
    2.0549384859987185,
    2.046889042998373,
    1.970603857000242,
    2.041171803997713,
    2.0331857570054126,
    1.9879366690001916,
    2.09443454000575,
    1.9112706500018248,
    1.9846461920024012,
    2.179487055997015,
    0.0,
    1.9725221539993072,
    2.385039697997854,
    2.2456494819998625,
    2.118539076000161,
    2.2437421920039924,
    2.220730438006285,
    2.2176977419949253,
    2.3042387390014483,
    2.196663169997919,
    2.2570961309975246,
    2.5092202659943723,
    2.7536038539983565,
    2.68181454599835,
    2.909737904003123,
    2.6308706920026452,
    2.629905638001219,
    2.66738081999938,
    2.6455675449979026,
    2.792264427000191,
    2.6356252199984738,
    2.7622885500022676,
    3.049582736006414,
    3.164834476992837,
    3.435257889999775,
    3.53695878200233,
    3.5132634110050276,
    0.0,
    4.025540238995745,
    4.011162447997776,
    4.1130155680002645,
    3.777005570002075,
    4.0691643960017245,
    4.0931578329982585,
    4.178369983994344,
    4.176741470000707,
    4.119563020001806,
    3.919635703001404,
    3.893043212003249,
    0.0,
    4.7176264010049636,
    4.867645040998468,
    4.981689300999278,
    4.935011339999619,
    4.819721170999401,
    4.767334766002023,
    4.7551777889966615,
    5.163208546997339,
    5.158393468998838,
    5.10625270500168,
    5.222576973996183,
    5.180252342004678,
    5.162778240002808,
    5.154218420000689,
    5.110528051001893,
    5.118305014002544,
    4.96027451500413,
    5.058073723004782,
    5.326096696000604,
    5.1823226749984315,
    0.0,
    5.777708129993698,
    5.689851147995796,
    5.657501301000593,
    5.621741193994239,
    5.874518133998208,
    5.80454623499827,
    5.792023682995932,
    5.755285223000101,
    5.694473374998779,
    5.756170116997964,
    5.700217612000415,
    5.69961722100561,
    5.694562120996125,
    5.705028249998577,
    5.9806874389978475,
    5.974286748998566,
    6.026060263997351,
    6.0062366889978875,
    5.9512282949945075,
    5.942083485002513,
    5.992627095001808,
    5.977931291999994,
    5.9850388210034,
    5.911144686993794,
    6.006823121999332,
    5.927649953002401,
    5.9434348690047045,
    5.8685958290006965,
    5.83178340500308,
    5.725816272999509,
    5.875217556997086,
    5.858453465996718,
    5.7770116009996855,
    5.7426487440025085,
    5.66145748100098,
    5.763700900999538,
    5.676901730999816,
    5.648864858005254,
    5.63093644699984,
    5.501757762001944,
    5.614651495001453,
    5.5153947889994015,
    5.582683362998068,
    5.562380283998209,
    5.535106226001517,
    5.790144370999769,
    5.742312668997329,
    5.739889306998521,
    5.7546175259994925,
    5.481052142997214,
    5.566140244998678,
    5.824467230995651,
    5.931103372000507,
    5.898964739993971,
    5.712100374999864,
    5.799326177999319,
    5.89670189699973,
    5.743894688996079,
    5.7392129439977,
    0.0,
    6.404249316998175,
    6.433115754000028,
    6.533347444004903,
    6.652860524998687,
    6.631731798006513,
    6.590867453000101,
    6.57452167700103,
    6.537428718998854,
    6.517235242994502,
    6.473883342994668,
    6.572352180002781,
    6.827441507004551,
    6.819204029998218,
    6.7472226780009805,
    6.541062656993745,
    0.0,
    7.515264761001163,
    7.418051282998931,
    7.411744873003045,
    7.540560173001722,
    7.517257678002352,
    7.470744336002099,
    7.432635850003862,
    7.419362585998897,
    7.369994123997458,
    7.4683182219960145,
    7.383749856999202,
    7.322755435001454,
    7.430005365997204,
    7.507242152001709,
    7.366552930005128,
    7.19389662300091,
    7.275046902002941,
    7.20059135700285,
    7.1311948010043125,
    7.357239584001945,
    7.272008229003404,
    7.209258618000604,
    7.180018295999616,
    7.297831645002589,
    7.371844067994971,
    7.3462382669968065,
    7.287933734005492,
    7.381090292001318,
    7.347751912006061,
    7.333594233998156,
    7.186648121998587,
    7.3030964260033215,
    7.239380099003029,
    7.352076914001373,
    7.220876163999492,
    7.08172417699825,
    7.079231828996853,
    7.493318804998125,
    7.279847840000002,
    7.179195546006667,
    7.155389088002266,
    6.938552386003721,
    6.854706625999825,
    6.966041637999297,
    6.961126556998352,
    6.879004214999441,
    7.000177297995833,
    7.065012671999284,
    7.053011153002444,
    7.259342473997094,
    7.345674413998495,
    7.2856867000009515,
    7.268874235000112,
    7.195239925000351,
    7.191300243001024,
    7.259247282003344,
    7.477049553002871,
    7.4687460719942464,
    7.590801218000706,
    7.558768639995833,
    7.650602825997339,
    7.725287067994941,
    7.66186489899701,
    7.564636470000551,
    7.651473694997549,
    7.7422391479994985,
    7.838762998995662,
    7.814808426999662,
    7.799325056999805,
    8.080473644004087,
    8.07950511600211,
    8.043121784001414,
    8.108061059996544,
    8.088270971995371,
    7.888857689998986,
    8.107241767000232,
    7.902622898000118,
    8.040586104994873,
    8.280543626002327,
    8.269048859001487,
    8.186558051995235,
    8.288645100001304,
    8.073981746994832,
    7.779997871999512,
    7.733696265997423,
    7.71770285799721,
    7.824951068003429,
    7.7657529979987885,
    7.822746327001369,
    7.7539093010054785,
    7.739128105000418,
    7.731863111999701,
    7.728168444999028,
    7.754941159997543,
    7.71915298300155,
    7.6980881490017055,
    7.673032865001005,
    7.573238967001089,
    7.538710888002242,
    7.658541404998687,
    7.60085971600347,
    7.868556374996842,
    7.787129648997507,
    7.692863641001168,
    7.692305591001059,
    7.584810493004625,
    7.674344910999935,
    7.753326209000079,
    7.750604102002399,
    7.683048015001987,
    7.624180141996476,
    7.849903688998893,
    0.0,
    8.401691945000493,
    8.446539502998348,
    8.296796157999779,
    8.403689553997538,
    8.51112976299919,
    8.644482846997562,
    8.751430303003872,
    8.739328962998115,
    8.705205482001475,
    8.650907543997164,
    8.640333166993514,
    8.56232941000053,
    8.677966386007029,
    8.585492107005848,
    8.436505525001849,
    8.337061331003497,
    8.365121462004026,
    8.54475269500108,
    8.523826076998375,
    8.517097683004977,
    8.502330950999749,
    8.480659166998521,
    8.44837963500322,
    8.513410430001386,
    8.483230889003607,
    8.601608368997404,
    8.536106030995143,
    8.616214590998425,
    8.599886429001344,
    8.550474741998187,
    8.654115476005245,
    8.640750180995383,
    8.620184091996634,
    8.613784569999552,
    8.517003311004373,
    8.51254448400141,
    8.151459576001798,
    8.066389886997058,
    8.159628966001037,
    8.107181280000077,
    8.216664536994358,
    8.086452010000357,
    8.021907761001785,
    8.01712527500058,
    7.881353438999213,
    7.666364310003701,
    7.589868280003429,
    7.845447125000646,
    0.0,
    8.346815023003728,
    8.334223274003307,
    8.170994517000508,
    8.378467754999292,
    8.324546802999976,
    8.340496393997455,
    8.192506758998206,
    8.154580069000076,
    8.109923431002244,
    8.103312972998538,
    8.078991064998263,
    8.19689605700114,
    8.114547943994694,
    8.242995412998425,
    8.087390175001929,
    7.932612782002252,
    7.78539539600024,
    7.803585524001392,
    7.710031106995302,
    7.706604432998574,
    7.710978465001972,
    7.7073647340002935,
    7.959265552999568,
    7.925802621000912,
    7.910352065002371,
    7.887272637999558,
    7.962934739000048,
    7.955233237000357,
    7.8317880390022765,
    8.117489436001051,
    8.027166819992999,
    8.044220137999218,
    8.03130744000373,
    8.084716490004212,
    8.260130656999536,
    8.315509549996932,
    8.295812077005394,
    8.253987349999079,
    8.2327901940007,
    8.10142229199846,
    8.045403344993247,
    8.070268335999572,
    8.066411980995326,
    8.005571941997914,
    7.998091884997848,
    8.258251400002337,
    8.39460129699728,
    8.521419120996143,
    8.651270969996403,
    8.531556783003907,
    8.643691641002079,
    8.60979066100117,
    8.674446320001152,
    8.782345192994399,
    8.817896218002716,
    8.944818017000216,
    8.871287740003027,
    9.251884229001007,
    9.170047067003907,
    9.289535386000352,
    9.2046876730019,
    9.123821147994022,
    9.09834138500446,
    9.084271747000457,
    8.933768866001628,
    8.907150052997167,
    9.184375024000474,
    9.107355341002403,
    9.080100993996894,
    9.142420167001546,
    9.395740485997521,
    9.316060907003703,
    9.278366455997457,
    9.261133383006381,
    9.337755587002903,
    9.269995574002678,
    9.2179436180013,
    9.21246443799464,
    9.029715299002419,
    8.993803982004465,
    9.253728891999344,
    9.228028318000725,
    9.224098542996217,
    9.205935035999573,
    9.159082288999343,
    8.943753256004129,
    8.878829628003587,
    8.832582209004613,
    8.84554797400051,
    8.94885312399856,
    0.0,
    9.444315336004365,
    9.552610234997701,
    9.503695040999446,
    9.625909953996597,
    9.606144987999869,
    9.92098132999672,
    9.887256121000974,
    9.764607011995395,
    9.75946038300026,
    9.714011056996242,
    9.962173893,
    9.96221052799956,
    10.082497477000288,
    10.082121934996394,
    10.36470037500112,
    10.347812388004968,
    10.311944926004799,
    10.403403804004483,
    10.643541530997027,
    10.589264656002342,
    10.70287659099995,
    10.647285708000709,
    10.642506778000097,
    10.634380255003634,
    0.0,
    11.068626150001364,
    11.102868324000156,
    11.080281141999876,
    11.011076491995482,
    11.173123505999683,
    11.123547024995787,
    11.086102743000083,
    11.075070447004691,
    11.156784776001587,
    11.272003994003171,
    11.382236220000777,
    11.378481308995106,
    11.342931049999606,
    11.198720501000935,
    11.182656683995447,
    11.190379382998799,
    11.235321607004153,
    11.230471481001587,
    11.419542887997522,
    11.387243376993865,
    11.335834190998867,
    11.320572459000687,
    11.269679901997733,
    11.24079033099406,
    11.203872076002881,
    11.17744723999931,
    10.992099305003649,
    11.240829791000579,
    11.328807196994603,
    11.288728741994419,
    11.185293154994724,
    11.28832643700298,
    11.222126594999281,
    11.019840466993628,
    11.066159249996417,
    11.025813783002377,
    10.97348184499424,
    10.604214767001395,
    10.58543242399901,
    10.6318217789958,
    10.518611966996104,
    10.279116309000528,
    10.383763855003053,
    10.459153700998286,
    10.444997773003706,
    10.570767827994132,
    10.485305921996769,
    10.768713794001087,
    10.75628233399766,
    10.810911312000826,
    10.801315402997716,
    10.834868620993802,
    10.729432768996048,
    10.721966342003725,
    10.837177205001353,
    10.738695464002376,
    10.650176650000503,
    10.514915753003152,
    10.442423223001242,
    10.692680221000046,
    10.643968816999404,
    10.616485025995644,
    10.618026222997287,
    10.555381004000083,
    10.432955787000537,
    10.481125012003758,
    10.454933638000512,
    10.387330463003309,
    10.512210493005114,
    10.475783646994387,
    10.388672707005753,
    10.330410148002557,
    10.395051831001183,
    10.337148159000208,
    10.260993659001542,
    10.258556945002056,
    10.238369916005468,
    10.231403712998144,
    10.305492883002444,
    10.22621544499998,
    10.133752672998526,
    10.082711873001244,
    10.201086242006568,
    10.145384709001519,
    10.124378175001766,
    9.980814326001564,
    10.040674149000552,
    9.91343495399633,
    9.789899465999042,
    9.782025899003202,
    9.89155478199973,
    9.873868059003144,
    9.863352773994848,
    9.836207690001174,
    9.913931208000577,
    9.905727716999536,
    9.863028838997707,
    9.830097958001716,
    9.895028083003126,
    9.88614734300063,
    9.837504025999806,
    9.83506363399647,
    10.11487394800497,
    10.079570850997698,
    10.052257769995776,
    10.044005782998283,
    10.019798576999165,
    9.992514051002217,
    10.059083364998514,
    10.001418052997906,
    9.933111313999689,
    10.044591191996005,
    10.099626925999473,
    10.07081704900338,
    9.880989132994728,
    9.99493174500094,
    9.97197651899478,
    10.014196385003743,
    9.994605814004899,
    10.077963530995476,
    10.071247601001232,
    9.881762911994883,
    9.85839125199709,
    9.82227678399795,
    9.668203777000599,
    9.64999950699712,
    9.596786116999283,
    9.638676909999049,
    9.622450133996608,
    9.5382118599955,
    9.462084002996562,
    9.57985198499955,
    9.373220738998498,
    9.249966748997394,
    9.027363205997972,
    8.984107820004283,
    9.10607957100001,
    9.00157608200243,
    9.066563779000717,
    9.151950236002449,
    9.088457439000194,
    9.044760404001863,
    9.039282288998947,
    8.977328365996073,
    8.870149072994536,
    8.986653538006067,
    8.978579123999225,
    8.827348653998342,
    9.078000615001656,
    9.053733341002953,
    9.050156571996922,
    8.96394468699873,
    8.92570066400367,
    9.086460443999385,
    9.340785372005485,
    9.23774272800074,
    9.427023726006155,
    9.383258508001745,
    9.333794785001373,
    9.247588660000474,
    9.143221910999273,
    9.07792158700613,
    9.077647327001614,
    8.942197994001617,
    8.923411771997053,
    8.914519105994259,
    9.019749801002035,
    8.960456852997595,
    8.873297801997978,
    8.987986964995798,
    8.96874747700349,
    8.939246298999933,
    8.882997100001376,
    8.839271715005452,
    8.8098386749989,
    9.058494388998952,
    9.124761131002742,
    9.184699935001845,
    9.18047129399929,
    8.953136791998986,
    8.99240232000011,
    8.91487796400179,
    8.880984465002257,
    8.875952776004851,
    8.6817251189932,
    8.968884018002427,
    8.83417180399556,
    8.674449618003564,
    8.94143970499863,
    9.002350502996705,
    8.958987434001756,
    8.870619038003497,
    8.816382569995767,
    8.936875054001575,
    8.87718463499914,
    9.145761343999766,
    8.928569616000459,
    9.020412929996382,
    9.001296091999393,
    8.912981280998792,
    8.841558382002404,
    8.95802444099536,
    8.918600375996903,
    8.957277277004323,
    9.245976199003053,
    8.903393822998623,
    8.89410059000511,
    9.011457859000075,
    8.941617009004403,
    8.86790356200072,
    8.972987952998665,
    8.784999768999114,
    8.775804994998907,
    8.737555499996233,
    8.670680194001761,
    8.765645785002562,
    8.737742900993908,
    0.0,
    9.250994244001049,
    9.133259626003564,
    9.180341762999888,
    8.948849567001162,
    8.903545759996632,
    9.095240087997809,
    9.031143520005571,
    9.026232511998387,
    9.1216869389973,
    9.101840464994893,
    9.227755947998958,
    9.01392373300041,
    8.870885085001646,
    8.860448797000572,
    8.747709338997083,
    9.050892398001452,
    9.028210778000357,
    9.144425989004958,
    9.109603122000408,
    9.079900822995114,
    9.01618176500051,
    8.915880184998969,
    8.866798980998283,
    9.101342643996759,
    8.964199018999352,
    9.07375840300665,
    8.951043410997954,
    8.9198902729986,
    9.0293762959991,
    9.02642654099327,
    8.881079225997382,
    8.994598276003671,
    8.90872615599801,
    8.896051732997876,
    9.03469801400206,
    9.003264710001531,
    9.022920289004105,
    8.949413075002667,
    8.819406648995937,
    8.756305834998784,
    8.743996008000977,
    8.621725632001471,
    8.617081799995503,
    8.73606989599648,
    8.711834670997632,
    8.803112918001716,
    8.788786360993981,
    8.641719703002309,
    8.638134833003278,
    8.890063895996718,
    8.885909126001934,
    8.820695499998692,
    8.813277934998041,
    8.807524900003045,
    8.922674078996351,
    8.902548017998924,
    8.92822380799771,
    8.919590089004487,
    9.033290210005362,
    9.013994964996527,
    8.928596956000547,
    9.020517989003565,
    9.151152981998166,
    9.234710458993504,
    9.19194640400383,
    9.170155277999584,
    9.227004325002781,
    9.16931776599813,
    9.221063410994248,
    9.215876563001075,
    9.173451697002747,
    9.11588980200031,
    9.025682006002171,
    9.013072783003736,
    9.136365129001206,
    9.070449679995363,
    9.17591071499919,
    8.906262897005945,
    8.901474319005501,
    8.844469231000403,
    8.70249054300075,
    8.80843410899979,
    8.746191382000688,
    8.696498638004414,
    0.0,
    9.380306828003086,
    9.364967826993961,
    9.337575388002733,
    9.32405586099776,
    9.276133523999306,
    9.38251918899914,
    9.506084050997742,
    9.467892055996344,
    9.451933739997912,
    9.449939964993973,
    0.0,
    10.226778707998164,
    10.227951239001413,
    10.334875935004675,
    10.265507336996961,
    10.325739127998531,
    10.287402932000987,
    10.294099466998887,
    10.254242399998475,
    10.35869375999755,
    10.289709514996503,
    10.287927231001959,
    10.251534025999717,
    10.545512345001043,
    10.766612604995316,
    10.74208799200278,
    10.719471475997125,
    10.815839975999552,
    10.758655814999656,
    10.727578806006932,
    10.701116937001643,
    10.68167109299975,
    10.636021718004486,
    10.740859256002295,
    10.70980429300107,
    10.703670294999029,
    10.681287166997208,
    10.754996823998226,
    10.874497274002351,
    11.102111761996639,
    10.962107107996417,
    11.095532077000826,
    10.975214979000157,
    10.941180658999656,
    11.008756727998843,
    10.792374197000754,
    10.790245080999739,
    0.0,
    11.238629942003172,
    11.19451342800312,
    11.260618385997077,
    11.184426478001114,
    11.124743436004792,
    11.054069466001238,
    10.932413292997808,
    10.906272407002689,
    10.997913418999815,
    11.112125444997218,
    11.11062476200459,
    11.009253525997337,
    10.86517879999883,
    10.796781035001914,
    10.893733338001766,
    10.850541749001422,
    10.753648762998637,
    10.869619233002595,
    10.990450882003643,
    10.96352251800272,
    10.951248151002801,
    10.880951173996436,
    10.989786131001892,
    11.129846798998187,
    11.110519613001088,
    11.094806063003489,
    11.252336552999623,
    11.201981104997685,
    11.252240668000013,
    11.510490277993085,
    11.483922154999163,
    0.0,
    12.043561794002017,
    12.004742660006741,
    12.031098853003641,
    12.264387575996807,
    12.502232449005533,
    12.497826234997774,
    12.44167815700348,
    12.340319882001495,
    12.235406944004353,
    12.17724452100083,
    12.141908475998207,
    12.50179660700087,
    12.487118868004472,
    12.458647701998416,
    12.683156985993264,
    12.561114217998693,
    12.540739424002822,
    12.497518926000339,
    12.57415902500361,
    12.703874008999264,
    12.684317603998352,
    12.886197965002793,
    12.756196921996889,
    12.721593964997737,
    12.832421548002458,
    12.853753377996327,
    12.839052112998615,
    12.728020176997234,
    12.83975124599965,
    12.763665952996234,
    12.865618203002668,
    12.94957641899964,
    0.0,
    13.377887971000746,
    13.421839048998663,
    13.353250073996605,
    13.223702087001584,
    13.294525741002872,
    13.423119844999746,
    13.53498281299835,
    13.523426437001035,
    13.5620976029968,
    13.528379174000293,
    13.747463466999761,
    13.69785525000043,
    13.714053180003248,
    13.779280176000611,
    13.753590503998566,
    13.73156266099977,
    13.711687847004214,
    13.888979459996335,
    13.673744084997452,
    13.626880008996523,
    13.837451011000667,
    13.834655517995998,
    13.771638179001457,
    13.721005257000797,
    13.919795351001085,
    13.698915511995438,
    13.961513679998461,
    13.960682842996903,
    14.035231368994573,
    13.928211320999253,
    14.049447072997282,
    14.1539957519999,
    14.099637763996725,
    14.090828339001746,
    13.978111781994812,
    14.09921560800285,
    14.07601195499592,
    14.139764041006856,
    14.098081967000326,
    14.086466319000465,
    14.053811033001693,
    14.248701880998851,
    14.227925459999824,
    14.171770551998634,
    14.240542660001665,
    14.21655896199809,
    14.260118747006345,
    14.212303439002426,
    14.050846022997575,
    14.177116314996965,
    14.292708604996733,
    14.332438186000218,
    14.208236832004332,
    14.190363282999897,
    14.237506502999167,
    14.471267895001802,
    14.543158216998563,
    14.823668067001563,
    14.806246538006235,
    14.72032257400133,
    14.67187664299854,
    0.0,
    15.257470857999579,
    15.352049696004542,
    15.277286846998322,
    15.23093257599976,
    15.320185946002312,
    15.290993624999828,
    15.150465157996223,
    15.100430167003651,
    15.185622356002568,
    15.307830990001094,
    15.255543919003685,
    15.369952422995993,
    15.369422618001408,
    15.354380951001076,
    0.0,
    15.78303535899613,
    15.653603833001398,
    15.564627972999006,
    15.496220467997773,
    15.4938052840007,
    15.3390091340043,
    15.38166247499612,
    15.614715616000467,
    15.508526790996257,
    15.425366175004456,
    15.484580184005608,
    15.401650239997252,
    15.435184759997355,
    0.0,
    15.810685577998811,
    15.741106412999216,
    15.856853006000165,
    15.936246895995282,
    15.85958593399846,
    16.112162304001686,
    16.028959713003132,
    15.95523722699727,
    15.754622053995263,
    15.724053196994646,
    15.796363642999495,
    15.756741842000338,
    0.0,
    16.352644513994164,
    16.298084494999785,
    16.443693536995852,
    16.569457685996895,
    16.524848847999237,
    16.463745688997733,
    16.32393899100134,
    16.426609030997497,
    16.326331586002198,
    16.28089681399433,
    16.26976109599491,
    16.215432276003412,
    16.144558948995837,
    16.252485427001375,
    16.550088848001906,
    16.418824094005686,
    16.317254076995596,
    16.41479406599683,
    16.361916469002608,
    16.394029130999115,
    16.477343368998845,
    16.459219372001826,
    16.535565115998907,
    16.667053017001308,
    16.7451697449942,
    16.69071694100421,
    16.660947480006143,
    16.57957266700396,
    16.54229067900451,
    16.509405996999703,
    16.489328197996656,
    16.431165197995142,
    16.65328697899531,
    16.91750103099912,
    16.771265162999043,
    16.72611524200329,
    16.781144129003223,
    17.062439758003165,
    16.917194099005428,
    17.023823242998333,
    16.934933363998425,
    16.86303613599739,
    16.75841425300314,
    16.86347976599791,
    16.67439228299918,
    16.617765251998208,
    16.641178257996216,
    16.601863734998915,
    16.593611176002014,
    16.502374459996645,
    16.939396753004985,
    16.910232263995567,
    16.908564101999218,
    16.84450680200098,
    0.0,
    17.095236683999246,
    17.078513248998206,
    17.011672152002575,
    17.1051409770007,
    17.098443663999205,
    16.94354169300641,
    17.004974121999112,
    17.096046701997693,
    17.204362301999936,
    17.08201113800169,
    17.058096739994653,
    17.003505111999402,
    17.308596447997843,
    17.3588318339971,
    17.342990646000544,
    17.287186700996244,
    17.236343502998352,
    17.204384671997104,
    17.43437338199874,
    17.229852333999588,
    17.176354057002754,
    17.115121290997195,
    17.12886312699993,
    17.0543417939989,
    16.969663242001843,
    16.881393912997737,
    17.06889873399632,
    16.882139658999222,
    16.812812888994813,
    16.792518292997556,
    16.762018733999867,
    16.864994844996545,
    16.98932166799932,
    16.909697035996942,
    17.021628372000123,
    17.129807813995285,
    17.116045434995613,
    17.080331447999924,
    17.359765207002056,
    17.632549706002465,
    17.631315024002106,
    17.714709239000513,
    17.84153180400608,
    17.83981458600465,
    17.966502103001403,
    17.794748573003744,
    17.868481726000027,
    17.781756648997543,
    17.779397136000625,
    17.84624061800423,
    17.877252399994177,
    17.83638231900113,
    17.792700489997515,
    17.62969236200297,
    17.614430180001364,
    17.565791144006653,
    17.641679994005244,
    17.634040901000844,
    17.72332961399661,
    17.494650727996486,
    17.34505469999567,
    17.257382495998172,
    17.514717992999067,
    17.3775893810016,
    17.29642527100077,
    17.230389746000583,
    17.34597813999426,
    17.400289324999903,
    17.49064316199656,
    17.35310519199993,
    17.24855431600008,
    17.18584193800052,
    16.940039064000302,
    16.840080381000007,
    16.788163334000274,
    16.725341930003196,
    16.442340171997785,
    16.470467285995255,
    16.444150229996012,
    16.562657560003572,
    16.674012356001185,
    16.603555993002374,
    16.729599686004804,
    16.844104249998054,
    16.9373450589992,
    17.126206930995977,
    17.014949056996556,
    17.03753999699984,
    16.977905762003502,
    17.16866941400076,
    17.362786636993405,
    17.361612164997496,
    17.273973912000656,
    17.376270032997127,
    17.294553227002325,
    17.236753700002737,
    17.22251406799478,
    17.154254446002597,
    17.389387750001333,
    17.350997223002196,
    17.310020614000678,
    17.395350863000203,
    17.647258821001742,
    17.618658838000556,
    17.87859560600191,
    17.991908269003034,
    17.979295664998062,
    17.860394197996357,
    17.777977209996607,
    17.69711966199975,
    17.791221892002795,
    17.619494835002115,
    17.69358807500248,
    17.666047195001738,
    17.78652268699807,
    17.74519195099856,
    17.831934884001384,
    17.8258864849995,
    17.95726255900081,
    18.03450470599637,
    18.15435078700102,
    18.13117535699712,
    17.817724911001278,
    17.844205216002592,
    17.822677178999584,
    17.819569315004628,
    18.03350077199866,
    18.011811989999842,
    17.94297826300317,
    17.87242465599411,
    17.96651088100043,
    18.177972065997892,
    18.298703942004067,
    18.40306029900239,
    0.0,
    19.30702455699793,
    19.210406284000783,
    19.27315271000407,
    19.21029258100316,
    19.083168741999543,
    19.19130456900166,
    19.13269192699954,
    19.080005775002064,
    19.18250942799932,
    19.057286848998046,
    19.018259955002577,
    19.266903636998904,
    19.21118884599855,
    19.177037991998077,
    18.910401686000114,
    18.73537385399686,
    18.7842622629978,
    18.68275877799897,
    0.0,
    19.00476239699492,
    18.984233782000956,
    0.0,
    20.28289443700487,
    20.154254986002343,
    20.62919986599445,
    20.604104222999013,
    20.682630462993984,
    20.64253457700397,
    20.905396106994885,
    21.177091320001637,
    0.0,
    21.634607004998543,
    21.752329468996322,
    0.0,
    22.315835281995533,
    22.380045402001997,
    22.270794085998205,
    22.0496501680027,
    22.00619723199634,
    21.893930819998786,
    21.888812970995787,
    21.972945413006528,
    21.912739218998468,
    21.90898329400079,
    21.57910777899815,
    21.535699460000615,
    21.614625497000816,
    21.006341227999656,
    0.0,
    22.21454989700578,
    22.2119470539983,
    22.185283072998573,
    22.42892839200067,
    22.42679759600287,
    22.44457710000279,
    22.23756056399725,
    22.449058903999685,
    22.34189344799961,
    22.57812556600402,
    22.576910882002267,
    22.59053596099693,
    22.75368212900503,
    22.747962859000836,
    22.636402833995817,
    22.710056596995855,
    22.648641353996936,
    22.709728719004488,
    22.657384811005613,
    22.751962917005585,
    22.73589760300092,
    22.64549861299747,
    22.73113481800101,
    22.729625995998504,
    0.0,
    23.294963407999603,
    23.411410216998775,
    23.33677762800653,
    23.331021264006267,
    23.570860478001123,
    23.556575440001325,
    23.44953902799898,
    23.442324627001653,
    23.517782650997106,
    23.581597192998743,
    23.53991732600116,
    23.53031498400378,
    23.597514811997826,
    23.58726125499379,
    23.441608704997634,
    0.0,
    24.274017151998123,
    24.37464660599653,
    24.251893402004498,
    24.23091145500075,
    24.22183227200003,
    24.33180230900325,
    24.183649780999986,
    24.119513273995835,
    24.318993836997834,
    24.268633857995155,
    24.219613749002747,
    24.183296821000113,
    24.2537508519963,
    24.149405782998656,
    24.12779193100141,
    24.090090026002144,
    24.066884860003483,
    23.996578989004774,
    23.96047728400299,
    23.919819167997048,
    23.875780159003625,
    23.833080388998496,
    23.881125814994448,
    0.0,
    24.3491682390013,
    24.391668733995175,
    24.31473594599811,
    24.398011817000224,
    24.503740284999367,
    24.494121087998792,
    24.48030894999829,
    24.46819446799782,
    24.356833252997603,
    24.196261421995587,
    24.09474298600253,
    24.068938352997066,
    24.294867366996186,
    24.16969924500154,
    24.168131168997206,
    24.135233674998744,
    24.04617480900197,
    24.1395199710023,
    24.077343977005512,
    24.03637686000002,
    24.019970523993834,
    24.070869298004254,
    23.85294745599822,
    23.94608178600174,
    23.94151930500084,
    23.895605550002074,
    24.01426060999802,
    24.29097131999879,
    24.561830942002416,
    24.643841414996132,
    24.616969436996442,
    24.68444386700139,
    24.665092292001646,
    24.614272027996776,
    24.535878028997104,
    24.50049410200154,
    24.691954968002392,
    24.678754326996568,
    24.616839994996553,
    24.594177543003752,
    24.63249168600305,
    24.87972685600107,
    24.81894306900358,
    24.73830554899905,
    24.80815728900052,
    24.906330956000602,
    24.83596883600694,
    24.523601545006386,
    24.653650047002884,
    24.560038759002055,
    24.4584015740038,
    24.44696480600396,
    24.21895726799994,
    24.195017666999775,
    24.1749728759969,
    24.148860825996962,
    24.058047687998624,
    24.040259514003992,
    24.244533788005356,
    24.128263539001637,
    24.23885782099387,
    24.420137036002416,
    24.49378765999427,
    24.27435259900085,
    24.325372318002337,
    24.291570736000722,
    24.264901182999893,
    24.21167882000009,
    24.315240234995144,
    24.263760076995823,
    24.366653667995706,
    24.232238797005266,
    24.181044026001473,
    24.140525856004388,
    24.240424257994164,
    24.2269473310007,
    24.20959701600077,
    24.308448301999306,
    24.399955673994555,
    24.397508220004966,
    24.46888261699496,
    24.413103507002234,
    24.50566505700408,
    24.690348715004802,
    24.63449815000058,
    24.62032664099388,
    24.533579836999706,
    24.649738207001064,
    24.619711596998968,
    24.566402753000148,
    24.555967252999835,
    24.78367639100179,
    24.73080860399932,
    24.724283450996154,
    24.690910853998503,
    24.622834648995195,
    24.61851371800003,
    24.726981410000008,
    24.65266685399547,
    24.56323604899808,
    24.652085098998214,
    24.627485876997525,
    24.726683371001855,
    24.557740244999877,
    24.44373309799994,
    24.42676153100183,
    24.37141559200245,
    24.598906840998097,
    24.582464703002188,
    24.54987604299822,
    24.49415024799964,
    24.600251687996206,
    24.59476860900031,
    24.549786231000326,
    24.65847099999519,
    0.0,
    25.236948563993792,
    25.330487238999922,
    25.325572038003884,
    25.416651084997284,
    25.404036851003184,
    25.3768941810049,
    25.453127043001587,
    25.67819899800088,
    25.58993978500075,
    25.6619228510026,
    0.0,
    26.29611620200012,
    0.0,
    27.658585251003387,
    27.615084586002922,
    27.612842996000836,
    27.54319094800303,
    27.371880550999776,
    27.646935553995718,
    27.89826521299983,
    27.968596703001822,
    27.841026613001304,
    27.92411393999646,
    27.98843783800112,
    27.949876140002743,
    28.06302566499653,
    28.160378207998292,
    28.142654834002315,
    28.098827100999188,
    28.058324552999693,
    28.016602969000814,
    28.008571422004024,
    27.741644239999005,
    27.70564128899423,
    27.633745977000217,
    27.604381987002853,
    27.579325339000206,
    27.565079147003416,
    27.540120869998646,
    27.602929079002934,
    27.592738542996813,
    27.53758701100014,
    27.25590353000007,
    27.329580209996493,
    27.29370878700138,
    27.33675358900655,
    27.462387902000046,
    27.758095488003164,
    27.7004074150027,
    27.58692087700183,
    27.71627110499685,
    27.986855462004314,
    27.934240775000944,
    28.210555537996697,
    28.264590635000786,
    28.226714132993948,
    28.113357768997957,
    28.091532866004854,
    27.932993116002763,
    28.018801546997565,
    28.12231379700097,
    28.067086037997797,
    28.032689731000573,
    28.1555586139948,
    28.12826296399726,
    28.119226301001618,
    28.136689626997395,
    28.259390384002472,
    28.208485113005736,
    28.189274103999196,
    28.261804716006736,
    28.20458700999734,
    28.19643042700045,
    28.0625682050013,
    28.109474569995655,
    28.092201293999096,
    27.962422098004026,
    28.021475997004018,
    28.017813411002862,
    28.10589064600208,
    28.045668418999412,
    27.88949683899409,
    28.17439635300252,
    28.126841443998273,
    28.53794490799919,
    28.456657660994097,
    0.0,
    29.419003956005326,
    29.526026289000583,
    29.522385971002223,
    29.510766198996862,
    29.39671180300502,
    29.4642939990008,
    29.435649674996966,
    29.39899654799956,
    29.551804260001518,
    29.368669645999034,
    29.610994966998987,
    29.590632077997725,
    29.68580162100261,
    29.66027072900033,
    29.567512581998017,
    29.67216837599699,
    29.651171809993684,
    29.697786204000295,
    29.617858274003083,
    29.60215670800244,
    29.657862985004613,
    29.586393321995274,
    29.580111465998925,
    29.546980075996544,
    29.53365744199982,
    29.463857451999502,
    29.45624139499705,
    29.726885902004142,
    29.648340993000602,
    29.74081102800119,
    29.703812234998622,
    29.61530788100208,
    29.879820344998734,
    0.0,
    31.133974402000604,
    31.07146516600187,
    31.161119996002526,
    31.13063038999826,
    30.97769271099969,
    30.888361363999138,
    30.940320463996613,
    30.812335560003703,
    30.805158395000035,
    30.827667378995102,
    30.693618246994447,
    30.622281724005006,
    30.874558915005764,
    30.573503599000105,
    31.003941481001675,
    31.072456141002476,
    31.01862990000518,
    31.006318095001916,
    30.949526085001708,
    31.044651381998847,
    31.024295157003507,
    30.997831470995152,
    31.093357849000313,
    31.047607569998945,
    31.044109125999967,
    31.15236541700142,
    31.280101850999927,
    31.109113125996373,
    31.065252897999017,
    31.158562760996574,
    31.142820027002017,
    31.260294476000126,
    31.283730974995706,
    31.234207649999007,
    31.303994201000023,
    31.40905649599881,
    31.67955390100542,
    31.580990290000045,
    31.57360327400238,
    31.623034944997926,
    31.50030687099934,
    31.359262687998125,
    0.0,
    32.314169621000474,
    32.2830360690059,
    32.31316339899786,
    32.25913475499692,
    32.18917596699612,
    32.11783347200253,
    32.482338249996246,
    32.33336904399766,
    32.30631329999596,
    32.28803088199493,
    32.24542293500417,
    32.239662239997415,
    32.47008353599813,
    32.571857206996356,
    32.383073901997705,
    32.08378796499892,
    31.990349036001135,
    31.948183708002034,
    32.208456894994015,
    32.10972224599391,
    32.09424783600116,
    31.961283060001733,
    32.032285149005475,
    32.012916255000164,
    32.28162021800381,
    32.36106087799999,
    32.3360207500009,
    32.322695199996815,
    32.546482146004564,
    32.49564153499523,
    32.45178786700126,
    32.594858711003326,
    32.638424812001176,
    32.637616678002814,
    32.895638956993935,
    33.08959532400331,
    33.2082427130008,
    33.10996358000557,
    32.744772404003015,
    32.87777851300052,
    33.099700577004114,
    33.0578067710012,
    33.003984408998804,
    32.95644483599608,
    33.06716316299571,
    33.11122665899893,
    33.10828224699799,
    33.02660525700048,
    0.0,
    33.92800944099872,
    33.92470050700649,
    33.95957623400318,
    33.91065277600137,
    33.91882880299818,
    33.94296565500554,
    33.98941325999476,
    33.94098468100128,
    33.92640374599432,
    33.77986814099859,
    33.75463079200563,
    33.865302231002715,
    33.81178017300408,
    33.67931109899655,
    33.671813360997476,
    33.696160512998176,
    33.687442716996884,
    33.68527777999407,
    33.80674263700348,
    33.76888473600411,
    33.66887320100068,
    33.78567745500186,
    34.00391116899846,
    33.964305346999026,
    34.16243524500169,
    33.96963150899683,
    34.09843204799836,
    33.77658801499638,
    33.72076140300487,
    33.88811681000516,
    34.004504053002165,
    33.904621540001244,
    34.094033948997094,
    33.81981214600091,
    33.916428614000324,
    33.80147210799623,
    33.769724969999515,
    34.05240699899878,
    34.26025885200215,
    34.221527173998766,
    34.50354725299985,
    34.77948166999704,
    34.85523463899881,
    34.811434053000994,
    34.795524397006375,
    34.9046343480004,
    34.89811901599751,
    34.79229490700527,
    34.79542205799953,
    35.059265039999445,
    34.94068110900116,
    35.179569148000155,
    0.0,
    35.657417436996184,
    35.62942729199858,
    35.558056647998455,
    35.54508964099659,
    35.53133131699724,
    35.50630861099489,
    35.34023853099643,
    35.32540074600547,
    35.306095465995895,
    35.33762414500234,
    35.16860476600414,
    35.099063387999195,
    35.17868495300354,
    35.05401130199607,
    35.16854550499556,
    35.304517786004,
    35.403203989997564,
    35.36599031100195,
    35.28953444900253,
    35.24747830999695,
    35.30894524299947,
    35.216715564994956,
    35.14844091000123,
    35.581095018998894,
    35.69839613899967,
    35.93315446100314,
    36.206315882001945,
    36.17913160199532,
    36.281128254995565,
    36.236161430999346,
    36.120113778000814,
    36.104660263998085,
    36.04554761599866,
    35.96282018000056,
    36.0339073110008,
    36.01623696000024,
    36.01457784100057,
    35.958544428001915,
    35.950982023001416,
    35.86339397600386,
    35.76658764999593,
    35.91816246000235,
    35.861830994996126,
    0.0,
    36.378366652003024,
    36.47391097500076,
    36.41612764499587,
    36.36135734800337,
    36.42733084400243,
    36.35685717800516,
    36.348621529999946,
    0.0,
    37.220983163999335,
    37.183488292997936,
    37.08489250999992,
    37.01499343899923,
    37.13906815999508,
    37.13333946200146,
    37.25804759199673,
    37.37509996800509,
    37.34460063200095,
    37.330562936003844,
    37.53504273399449,
    37.46532132100401,
    37.56232853900292,
    37.466810423000425,
    37.421034537001105,
    37.51688998300233,
    37.41628701899754,
    37.532647980995534,
    37.701684785002726,
    37.679292720000376,
    37.59884236999642,
    37.51962245899631,
    37.92674799000088,
    37.774712703998375,
    37.718445694001275,
    37.70817626099597,
    37.48879916300211,
    37.37132222999935,
    37.44684225400124,
    37.382250389004184,
    37.336406349000754,
    0.0,
    37.89728697499959,
    38.06332205700164,
    38.14774620200478,
    38.13259789600124,
    38.079061847995035,
    38.054369356999814,
    37.97451546200318,
    37.97400989799644,
    38.09172056800162,
    38.06189615900075,
    38.16422508400137,
    38.219842348000384,
    38.364360728002794,
    38.30442675499944,
    38.21977434499422,
    38.21356123300211,
    38.29178720899654,
    38.183060555995326,
    38.060677932000544,
    37.97116337799525,
    37.991954143006296,
    37.96870763300103,
    38.00290939700062,
    37.95642169200437,
    37.94079833100113,
    38.23730567400344,
    38.138383673001954,
    38.06962275499973,
    38.05973511200136,
    38.04782330599846,
    38.04118057300366,
    38.30002882100234,
    38.27776143300434,
    38.212145353994856,
    38.184337992999644,
    0.0,
    38.99736054199457,
    39.208039994999126,
    39.113457796003786,
    39.03471155199804,
    38.982856862996414,
    38.92928062199644,
    0.0,
    39.41735913300363,
    39.359890931002155,
    39.190352072997484,
    39.179238300006546,
    39.21089967500302,
    39.17986298700271,
    39.116495571994164,
    39.21777417299745,
    39.152233822998824,
    39.08867524899688,
    39.08536964200175,
    39.33980249600427,
    39.1478298180009,
    39.06249643799674,
    39.134397027002706,
    39.2348111770043,
    39.52680015300575,
    39.60062147000281,
    39.58241672200529,
    39.392266700997425,
    39.3104927020031,
    39.27320637900266,
    39.29087287600123,
    39.19311707300221,
    39.185002552003425,
    39.19244524100213,
    38.97331181100162,
    38.54121609199501,
    38.1864191190034,
    38.03178560300148,
    38.05706397400354,
    37.91177327599871,
    37.87098258599872,
    37.861914954002714,
    38.05701540999871,
    37.926683245001186,
    37.91526654399786,
    38.03359895200265,
    38.02840177199687,
    38.0031666779978,
    37.96719134200248,
    37.9229799800014,
    38.007837810997444,
    37.94022369099548,
    37.785719686005905,
    37.87729556800332,
    37.73204093200184,
    37.69677512299677,
    37.81252085500455,
    37.93084691600234,
    37.9227418570008,
    37.9107594379966,
    37.81335020800179,
    37.936848199999076,
    37.89559357200051,
    37.9700932199994,
    37.95808049500192,
    38.20485121999809,
    37.91464676299802,
    38.1894122540034,
    38.30829590299982,
    38.23245083499933,
    38.50350384399644,
    38.48860745099955,
    38.45903887000168,
    38.533796834002715,
    38.37815165199572,
    38.61441352300608,
    38.56574212400301,
    38.55943385000137,
    38.45615483599977,
    38.56109062900214,
    38.535622980998596,
    38.4758029730001,
    38.416540616999555,
    38.52836613799445,
    38.586439796003106,
    38.73525134399824,
    38.7038864999995,
    38.68310993399791,
    38.79459843100631,
    38.70500387799984,
    38.86050605100172,
    38.7621550249969,
    38.7475433169966,
    38.70690638999804,
    38.96743781599798,
    38.95515236200299,
    38.89910963299917,
    38.83365225500165,
    0.0,
    39.891633282997645,
    39.86963446199661,
    40.10773073200107,
    40.03189200799534,
    40.10423603699746,
    39.987565129005816,
    39.95448378800211,
    40.29004272099701,
    40.27403692399821,
    40.39616599299916,
    40.42139393200341,
    40.40145321199816,
    40.32420776900108,
    40.33929605800222,
    40.297112317006395,
    40.33865083099954,
    40.4657661339952,
    40.5215784059983,
    40.485228803998325,
    40.416790769995714,
    40.462487879005494,
    40.444408933995874,
    40.71765878899896,
    40.695974508998916,
    40.61568796500069,
    40.51668882300146,
    40.437519594997866,
    40.66487980000238,
    40.54422829699615,
    40.50611159700202,
    40.572204059004434,
    40.501878290000604,
    40.46231501800503,
    40.569324185998994,
    40.745694259996526,
    40.670743868999125,
    40.80632066500402,
    40.781785220999154,
    41.15598915499868,
    41.104292946001806,
    41.1595383610038,
    41.12626249699679,
    41.240341500000795,
    41.11520860600285,
    41.073154429999704,
    41.003348496997205,
    41.1916272449962,
    41.15856470600556,
    41.12372095500177,
    41.01782453600026,
    40.98687727700599,
    40.87609825300024,
    40.92234663999989,
    40.90025445300125,
    40.892754388005415,
    40.879047096997965,
    40.818227664000005,
    40.91426311599935,
    40.87736519500322,
    40.756682998006,
    40.73334789499495,
    40.62335652099864,
    40.732694376994914,
    40.709148540998285,
    40.513065612998616,
    40.63416729099845,
    40.61245360799512,
    40.72875026700058,
    40.47501075100445,
    40.51216736599599,
    40.484591508997255,
    40.74473307200242,
    40.67805861299712,
    40.56983087199478,
    40.33114983099949,
    40.310044657002436,
    40.309202414995525,
    40.25335947900021,
    40.256676631994196,
    40.19272281000303,
    40.11079624100239,
    40.07444704899535,
    40.06563363799796,
    40.07248004699795,
    39.9871652580041,
    40.02404538800329,
    39.854753059997165,
    39.7940831770029,
    39.91666922199511,
    39.87042628400377,
    39.91684990599606,
    39.846891660003166,
    39.71103227700223,
    39.82107057399844,
    39.78269454900146,
    39.50948406599491,
    39.60438698100188,
    39.520644433003326,
    39.6257199679967,
    39.61137962900102,
    39.90243490799912,
    39.83148639099818,
    39.78062694700202,
    39.737810336999246,
    39.72867759599467,
    39.691402707001544,
    39.72631970499788,
    39.67121829700045,
    39.796047087998886,
    0.0,
    40.38677200600068,
    40.34753838300094,
    40.47733137499745,
    40.52944948099321,
    40.547089064995816,
    40.53291965199605,
    40.45551723000244,
    40.35581871299655,
    40.58815396900172,
    40.58253218999744,
    40.52638932799891,
    40.41569564399833,
    40.524818197998684,
    40.50812862600287,
    40.88752087600005,
    41.016026930003136,
    40.955978398997104,
    41.06534390199522,
    40.89670260999992,
    40.845409603993176,
    40.75822006100498,
    40.711262025004544,
    40.701487569000165,
    40.69564821599488,
    40.818798382999375,
    41.0437425850032,
    40.9730987619987,
    41.07480341199698,
    40.94693638700119,
    40.780221816996345,
    40.64334202199825,
    40.896921061001194,
    41.049021106999135,
    41.04885391600692,
    41.1227136660018,
    41.126722348999465,
    41.10002422200341,
    41.26373816499836,
    41.1378708869961,
    40.88978211000358,
    40.90554485100438,
    40.88246626700129,
    40.845274505001726,
    41.08220395999524,
    41.08250748700084,
    41.002308900999196,
    40.98549680999713,
    40.69503434000217,
    40.639775859999645,
    40.662123537003936,
    40.48682885699964,
    40.43900713900075,
    40.522630266001215,
    40.79888824900263,
    40.79626738099614,
    40.92254474299989,
    40.880752984994615,
    40.65083594700263,
    40.621341712001595,
    40.699547140000504,
    40.550357606000034,
    40.631575783998414,
    40.57984280399978,
    40.49390646000393,
    40.679340304996,
    40.67731061899394,
    40.67526980499679,
    40.68482069000311,
    40.652637097999104,
    40.93184148999717,
    41.13489066300099,
    41.08000587800052,
    41.03906737199577,
    41.28892729700601,
    41.34369242499815,
    0.0,
    41.863726242001576,
    42.1348407550031,
    42.02877381700091,
    41.98087444699922,
    41.87364143799641,
    41.804444189001515,
    41.926023566004005,
    41.844681477996346,
    41.80484869099746,
    41.79862090599636,
    41.65285138699983,
    41.71495904600306,
    41.989320077998855,
    41.95992749099969,
    42.00334057299915,
    41.953974650998134,
    41.87902758499695,
    41.78735135500028,
    41.848075735004386,
    41.643904828000814,
    41.533154292999825,
    41.81283691700082,
    41.70835932599584,
    41.69919301099435,
    41.6848419300004,
    41.670100301002094,
    41.782417065995105,
    41.732370192003145,
    41.66692395800055,
    41.54793706499913,
    41.98931205400004,
    41.866211906999524,
    41.77579768800206,
    41.60941806500341,
    41.45337433399982,
    41.54528661299992,
    41.57953801700205,
    41.62416009599838,
    41.58227620099933,
    41.45050258000265,
    41.44386287499947,
    41.38860387499881,
    41.48665712100046,
    41.463839507996454,
    41.343153855996206,
    41.399669448997884,
    41.264500562996545,
    41.24374779899517,
    41.19302402900212,
    40.98432441299519,
    40.9527486060033,
    40.941394513996784,
    40.87059306100127,
    40.9257549029935,
    41.20172918999742,
    41.11020957100118,
    41.06463561500277,
    41.01636837999831,
    41.08142420199874,
    40.99856022599852,
    41.22754709899891,
    41.14525175699964,
    41.08086942900263,
    41.14048609899328,
    41.261377133996575,
    41.388236266000604,
    41.59656832699693,
    41.58931411399681,
    41.6856059510028,
    41.66231347400026,
    41.51771700300014,
    41.71614813800261,
    41.66296788299951,
    41.59065761200327,
    41.68341368199617,
    41.678677153002354,
    41.62405857200065,
    42.07097312199767,
    42.02761769200151,
    42.30876246200205,
    42.40215740500571,
    42.156586674005666,
    42.14411248199758,
    42.15360240600421,
    42.135019713001384,
    42.21126739399915,
    42.27853430600226,
    42.14879038800427,
    42.15569770900038,
    42.14873593599623,
    41.941380358999595,
    41.9952530330047,
    41.98837496500346,
    42.10310104499513,
    42.390758508001454,
    42.28124753999873,
    42.18288581300294,
    42.29844150700228,
    42.164896487993246,
    42.27728496599593,
    42.547995389999414,
    42.58428796099906,
    42.656424711,
    42.533520281001984,
    42.531991816998925,
    42.81077574699884,
    42.80619554099394,
    42.79143789599766,
    42.763401848002104,
    42.88361181799701,
    42.83770317499875,
    42.84514200700505,
    0.0,
    43.26113083000382,
    43.401968055994075,
    43.354934299997694,
    43.545041502999084,
    43.45841010299773,
    43.38679300199874,
    43.51021748600033,
    43.50809716399817,
    43.497491891997925,
    43.67410617900168,
    43.66720099100348,
    43.533466581997345,
    43.51443532900157,
    43.479537535997224,
    43.76693203600007,
    43.7437342589983,
    43.71832390999771,
    43.7694028870028,
    43.85047279600258,
    44.0860060299965,
    44.08101731399802,
    44.033492470000056,
    43.99239591100195,
    44.03281204699306,
    44.011976870999206,
    43.99027550000028,
    44.09747648300254,
    44.19013904599706,
    44.178106763996766,
    44.168402248003986,
    44.107468205998885,
    44.10237306199997,
    44.06147157899977,
    44.04627634899953,
    43.88923277100548,
    44.11836938700435,
    43.979021965002175,
    44.0861678509973,
    44.07972839199647,
    43.92355563700403,
    43.910744363005506,
    43.96700510599476,
    43.943907711000065,
    43.89372984000511,
    44.131617650004046,
    44.113212780997856,
    44.09200720600347,
    44.08114242600277,
    44.19382054400194,
    44.189320785000746,
    44.30113273199822,
    44.422600646998035,
    44.38071028000559,
    44.51934263200383,
    44.62994198100205,
    44.540884320005716,
    44.51103468799556,
    44.63845430999936,
    44.523807240999304,
    44.71556602799683,
    44.317237989002024,
    44.275697869001306,
    44.1652254790024,
    44.103863137999724,
    44.16404040299676,
    43.969600835000165,
    43.95863669499522,
    44.042067159003636,
    44.16114817500056,
    44.169772246998036,
    44.44052344800002,
    44.36980198299716,
    44.64164781400177,
    44.72670217799896,
    44.58368236699607,
    44.5625125239967,
    44.46454304399958,
    44.46337880200008,
    44.44568371299829,
    44.43347035800252,
    44.50649349200103,
    44.49444088999735,
    44.4653645340004,
    44.516718057995604,
    44.62051266799972,
    44.512600531001226,
    44.49651247399743,
    44.42856715599919,
    44.41389121300017,
    44.162474400000065,
    44.2182378159996,
    0.0,
    44.410328048004885,
    44.39009566399909,
    44.50322119100019,
    44.798440300997754,
    44.75108871299744,
    44.76373027799855,
    44.63532179300091,
    44.60190854800021,
    44.59182190900174,
    44.57590708999487,
    44.82404523900186,
    44.69865939600277,
    44.746009764996415,
    44.702965582000616,
    44.666054509994865,
    44.76834602600138,
    44.73049673299829,
    44.66796068200347,
    44.664168965995486,
    44.59424371500063,
    44.640724551994936,
    44.62431488099537,
    44.58985063700675,
    44.52280524800153,
    44.523851710997405,
    44.626411705001374,
    44.59259416299756,
    44.605868297003326,
    44.555526664000354,
    44.55282350000198,
    44.65708254399942,
    44.73774089299695,
    44.99220724600309,
    44.91926155499823,
    45.029374241996265,
    44.76299799700064,
    44.91060528899834,
    45.024619737996545,
    45.09385844699864,
    45.21058918500057,
    45.15677990700351,
    45.260636600003636,
    45.17058877400268,
    0.0,
    45.81782845899579,
    45.70983882799919,
    45.62139218700031,
    45.615347681996354,
    45.60960022999643,
    45.59503695099556,
    45.55120079399785,
    45.651013438997325,
    45.52967242000159,
    45.6606224559946,
    45.7663282709982,
    45.98135392199765,
    45.97256836800079,
    45.96864055000333,
    45.935496206002426,
    46.01149466199422,
    45.87150569699588,
    45.73475956200127,
    45.794950806994166,
    45.90982213700045,
    46.16367354799877,
    46.28987017399777,
    46.26906417099963,
    46.40099844100041,
    46.514820303003944,
    46.47890691900102,
    46.47812659200281,
    46.45114989799913,
    46.410907726000005,
    46.471214062999934,
    46.361683714996616,
    46.35882715700427,
    46.48150565999822,
    46.42138090299704,
    46.420297573000425,
    46.43299780599773,
    46.42762629700155,
    46.53479210699879,
    46.50021164699865,
    46.44324701799633,
    0.0,
    46.92870297400077,
    46.84781493400078,
    46.94505153299542,
    47.19247188299778,
    47.04389014800108,
    46.97062928700325,
    47.02947919700091,
    47.028281461003644,
    47.040386200002104,
    46.890297566998925,
    46.87216514899774,
    46.95801593700162,
    46.80786799899943,
    46.798287484001776,
    47.09267621199979,
    47.31485353499738,
    47.41815635200328,
    47.302735065997695,
    47.29784125199512,
    47.45979920300306,
    47.26049256200349,
    47.22943657400174,
    46.704430266996496,
    46.639717111997015,
    46.55363889400178,
    46.496023804997094,
    46.42168420600501,
    46.429446854999696,
    46.33571983400179,
    46.31099777300551,
    46.54197386400483,
    46.50771395400079,
    46.56852205200266,
    46.46745333199942,
    46.39490219299478,
    46.38537656999688,
    0.0,
    47.43743262900534,
    47.40112813000451,
    47.52466436599934,
    47.51668337100273,
    47.316820070001995,
    47.248538342995744,
    47.15767536400381,
    46.912792592003825,
    47.02469487499911,
    47.01100035900163,
    46.88308389500162,
    46.98232779200043,
    47.247869192993676,
    47.49033031400177,
    47.800060722001945,
    47.69443103099911,
    47.69225127900427,
    47.541515172997606,
    47.80742199299857,
    47.765063416001794,
    47.793112363993714,
    47.782628950997605,
    47.6788408440043,
    47.573732208002184,
    47.62164254199888,
    47.59858424199774,
    47.939176097999734,
    48.04854671999783,
    48.038954543997534,
    48.02807251700142,
    48.024319720003405,
    48.09858226399956,
    47.9965850410008,
    47.766912701001274,
    47.675235458002135,
    47.612382685998455,
    47.50957357900188,
    47.502972779999254,
    47.51686987499852,
    47.5149887660009,
    47.483442751996336,
    47.47188714899676,
    47.57343091000075,
    47.588917358996696,
    47.49467860799632,
    47.47321169000497,
    47.4266388420001,
    47.40892512300343,
    47.51123346699751,
    47.44946152299963,
    47.5521229320002,
    47.512743158004014,
    47.61985176400049,
    47.54354133300512,
    47.57884151900362,
    47.706909405998886,
    47.703116920005414,
    47.77252201499505,
    47.867764292001084,
    47.864401736995205,
    47.95925233700109,
    47.74223732399696,
    47.73975397200411,
    47.847803437005496,
    47.91823310300242,
    47.89274683000258,
    47.832624776994635,
    47.80931069199869,
    47.919411924005544,
    48.17562149599689,
    48.1455825899975,
    48.017571720003616,
    47.994821889995364,
    47.80825973000174,
    47.9261323430037,
    47.80924814900209,
    47.78544349200092,
    47.70414174399775,
    47.577097551002225,
    47.688204450998455,
    47.679758131998824,
    47.644670631001645,
    47.716848923002544,
    47.6980012500062,
    47.74766188400099,
    47.73975385000085,
    47.85262345500087,
    47.81354132699926,
    47.640425548001076,
    47.56470432099741,
    47.544769695006835,
    47.45754751799541,
    47.45036608100054,
    47.40303900800063,
    47.3827148249984,
    47.335535808000714,
    47.2036825000032,
    47.253088247998676,
    47.15864627699921,
    47.27795857899764,
    47.22262658800173,
    47.21739799599891,
    47.122262623001006,
    47.338724252993416,
    47.41005158399639,
    47.3855330869992,
    47.61517376299889,
    47.708501898996474,
    47.854439138995076,
    47.94864145400061,
    48.13674589700531,
    48.12985147799918,
    48.02439865699853,
    47.93611873900227,
    47.91158781300328,
    47.692930162003904,
    47.7317426879963,
    47.7226566850004,
    47.81819909699698,
    47.81320824699651,
    47.79997183800151,
    47.91390812300233,
    47.811355859994364,
    47.779198693999206,
    48.03789232300187,
    47.96893970999372,
    47.9650003650022,
    47.95505696699547,
    48.009091735002585,
    47.91101782400074,
    47.87292854699626,
    47.72754669200367,
    47.810797549005656,
    47.936199739000585,
    47.7973961730022,
    47.790375946002314,
    47.8637331409991,
    47.83387718699669,
    47.80289143399568,
    47.97198498699436,
    47.899881864002964,
    48.035451244002616,
    47.937171923003916,
    47.51230701500026,
    47.628465516005235,
    47.5640538689986,
    47.61334333799459,
    47.5780554869998,
    47.48823120700399,
    47.55851323000388,
    47.45103066300362,
    47.614312319004966,
    47.73265859300591,
    47.73054102400056,
    47.67937403600081,
    48.10220828800084,
    48.01631406600063,
    47.92425625299802,
    47.91849788199761,
    47.6935697969966,
    47.7845819350041,
    47.6280267070033,
    47.545916422997834,
    47.476356483995914,
    47.71141722799803,
    48.15958191299433,
    48.12268349700025,
    48.10310806300549,
    48.080811413004994,
    48.17896928000118,
    48.0384400000039,
    47.88593099999707,
    47.687519202998374,
    47.75650901400513,
    47.74494255599711,
    47.87693303999549,
    47.95488982700044,
    47.8553719210031,
    47.744095592999656,
    47.73430744500365,
    48.01669522600423,
    0.0,
    48.523835327003326,
    48.818269048999355,
    48.792547901000944,
    48.891705979003746,
    48.91554691000056,
    0.0,
    49.245680056999845,
    49.35379079500126,
    49.459866810000676,
    49.436510528001236,
    49.532133270004124,
    49.50856991299952,
    49.492779981999774,
    49.61689490999561,
    49.579865484003676,
    49.54727593500138,
    49.56715272599831,
    49.51090245600062,
    49.43683818499994,
    49.39799185400625,
    49.51980457800528,
    49.47912761000043,
    0.0,
    50.06000373000279,
    49.90763994399458,
    49.82030232200486,
    49.90003331499611,
    49.78865800299536,
    49.78630857600365,
    49.73366611899837,
    49.77266878999944,
    49.76073969799472,
    49.848917263996555,
    49.84737232500629,
    49.775225423996744,
    49.771645922999596,
    49.88405047800188,
    49.87366786700295,
    49.77805275600258,
    49.7336436839978,
    49.72553971799789,
    49.724023852002574,
    49.84643719500309,
    50.019185530996765,
    49.90286092700262,
    49.84452056699956,
    49.75151669399929,
    49.73027806299797,
    49.66729373700218,
    49.929169954993995,
    49.568482989001495,
    49.66363888000342,
    49.59268391899968,
    49.44583284200053,
    49.558126884003286,
    49.32418151800084,
    49.440228498999204,
    49.28819442499662,
    49.24985188899882,
    49.34920115899877,
    49.311907821000204,
    49.216164529003436,
    49.16862487400067,
    49.138292745999934,
    49.25063490100001,
    49.17774683299649,
    49.17049922199658,
    49.25339595699916,
    49.238633294000465,
    49.356605270004366,
    49.32499180499872,
    49.27998665099585,
    49.193397087998164,
    49.15511522399902,
    49.096276713004045,
    48.931609552993905,
    48.977501278001,
    48.9742048650005,
    49.042858517997956,
    48.82978940400062,
    48.7816481789996,
    48.66972511699714,
    48.66275677399972,
    48.65157499199995,
    48.77057830100239,
    48.720275359999505,
    48.56910846599931,
    48.54893980400084,
    48.6325268790024,
    48.55529400100204,
    48.282420889001514,
    48.358972379006445,
    48.3334965499962,
    48.59110413699818,
    48.41076811699895,
    48.35246455900051,
    48.19661720700242,
    48.17990165000083,
    48.36584110699914,
    48.333561888000986,
    48.23764609800128,
    48.306163722001656,
    48.301321441002074,
    48.263703270000406,
    48.24101371500001,
    48.235922252999444,
    48.225376336995396,
    48.12837607899564,
    48.123997155002144,
    48.41802532500151,
    48.48064107800019,
    48.45540252199862,
    48.387080109001545,
    48.37402621899673,
    48.5896135619987,
    48.82606515900261,
    48.74328127699846,
    48.71361826700013,
    48.82486949200393,
    48.81070852300036,
    49.08276828700036,
    48.992091218002315,
    48.99178039699473,
    48.972765358994366,
    48.97328895600367,
    49.08014217300661,
    48.989813991996925,
    48.84693767000135,
    48.66762968099647,
    48.56802842899924,
    48.43214499700116,
    48.50080723500287,
    48.46730016299989,
    48.77349681800115,
    48.7598593119983,
    48.78577921199758,
    48.7216399789977,
    48.68302396500076,
    48.77322137100418,
    49.031723498002975,
    49.02318017400103,
    49.1135528349987,
    49.11246528600168,
    49.17440123000415,
    49.168713988998206,
    49.09674965599697,
    0.0,
    49.4299652960035,
    49.32068846099719,
    49.31828240500181,
    49.39873432400054,
    49.35369996300142,
    49.479350540997984,
    49.40627560799476,
    49.5759566409979,
    49.557721002995095,
    49.53071512599854,
    49.64031288600381,
    49.58602278299804,
    49.676856400998076,
    49.620241761003854,
    49.52197847399657,
    49.50521490700339,
    49.578528315003496,
    49.545104866003385,
    49.567992332995345,
    49.54196068300371,
    49.51524581600097,
    49.58482632399682,
    49.83597345399903,
    49.805450906998885,
    49.7789148150041,
    49.89553918000456,
    49.74052458300139,
    49.71397899399744,
    49.66931888600084,
    49.518665921998036,
    49.565679109000484,
    49.87178533599945,
    49.85959902600007,
    49.78033024999604,
    49.705669913004385,
    49.69327955900371,
    49.651085384997714,
    49.74024826200184,
    49.721235154996975,
    49.69132409500162,
    49.68227833600395,
    49.75005896199582,
    49.80688664899935,
    49.819503669998085,
    50.0734568570042,
    50.058811570001126,
    50.01358597899525,
    49.97411592499702,
    50.027807824000774,
    49.959545242003514,
    49.912544195998635,
    49.88625006000075,
    49.843824437004514,
    49.72015067299799,
    49.770721550004964,
    49.75748797399865,
    49.69943736999994,
    49.79740770299395,
    49.69446083900402,
    49.66617397600203,
    49.60869544400339,
    49.71450719400309,
    49.827677070999925,
    49.93830943100329,
    49.925719123995805,
    49.905414448003285,
    49.858937212993624,
    49.900921870001184,
    49.87527226299426,
    49.99716632199852,
    49.93296564800403,
    49.90850976799993,
    49.881565796997165,
    49.869755748004536,
    49.86743869299971,
    49.79370419299812,
    49.739089243994385,
    49.582011101003445,
    49.67778917599935,
    49.90556105500582,
    50.18310532200121,
    50.12375969599816,
    50.086126376998436,
    0.0,
    50.437983845004055,
    50.38329569599591,
    50.50044849000551,
    50.47777853599837,
    50.344514782998885,
    50.47626717700041,
    50.33717251299822,
    50.58923020299699,
    50.46581904900086,
    50.439434780004376,
    0.0,
    51.17851605399483,
    51.100611815003504,
    51.35325582399673,
    51.217233852999925,
    51.152285382006085,
    51.121482413000194,
    51.03294246499718,
    50.99148367899761,
    51.05107333200431,
    51.01990556399687,
    51.01591405199724,
    51.209069848999206,
    50.97484092900413,
    51.0393946280019,
    51.014726100998814,
    50.98140801299451,
    50.82760958599829,
    50.799624753999524,
    50.882402587005345,
    50.86191448899626,
    50.822723504999885,
    50.81125551900186,
    50.662139904998185,
    50.58008724300453,
    50.36647593800444,
    50.43545019400335,
    50.55514652200509,
    50.51613288500084,
    50.76500366399705,
    0.0,
    51.944198802004394,
    52.1760434569951,
    52.26771548100078,
    52.385757873998955,
    52.626714058998914,
    52.75770462399669,
    52.62729125199985,
    52.59879750999971,
    52.511579223995795,
    52.554072114995506,
    52.49202982299903,
    52.44292052799574,
    52.40301639199606,
    52.35342822200619,
    52.34993778800708,
    52.2680671969938,
    52.15620416300226,
    52.06902272199659,
    51.970366819004994,
    51.78401671000029,
    51.74414880800032,
    51.76883252600237,
    51.72126128500531,
    51.66538089799724,
    51.65242087899969,
    51.63814457600529,
    51.5647837409997,
    51.669601284003875,
    51.741331775003346,
    51.6564424169992,
    51.457736236996425,
    51.3090761239946,
    51.228984102999675,
    51.138994036999065,
    51.135677045997,
    51.10303688800195,
    51.0741972820033,
    51.137392041004205,
    51.348915871996724,
    51.13575644299999,
    51.03089433499554,
    50.97365754200291,
    50.96425253000052,
    50.93371562500397,
    50.99383282899362,
    50.964412018001894,
    50.88416957899608,
    50.989114332995086,
    50.9577677749985,
    51.066165368996735,
    51.30022392400133,
    51.40999800300051,
    51.38922229800664,
    51.37924683000165,
    51.33550024499709,
    51.27326256300148,
    51.21366634500009,
    51.30893172499782,
    51.09753174499929,
    51.03617780300556,
    51.009162715003185,
    51.07352908900066,
    50.95368884699565,
    50.874149437004235,
    50.87023721400328,
    0.0,
    51.428134162997594,
    51.4257300820027,
    51.42469857799733,
    51.54455080899788,
    51.36303438300092,
    51.239986323002086,
    51.306894075001765,
    51.27437864600506,
    51.31907754200074,
    51.26372110199736,
    51.218894689998706,
    51.1983656040029,
    51.26450574000046,
    51.148940527004015,
    51.14513507400261,
    51.45308390700666,
    51.537508345005335,
    51.786288153001806,
    51.73017402199912,
    51.832146729997476,
    51.7632403969983,
    51.69931131799967,
    51.6471545280001,
    51.76918414700049,
    51.723774942998716,
    51.664982724003494,
    51.71962406500097,
    51.57612572400103,
    51.78255226100009,
    51.708615322000696,
    51.56395583000267,
    51.53044867100107,
    51.62351896100154,
    51.49631727200176,
    51.48523900099826,
    51.46403074900445,
    51.580098796999664,
    51.862945779001166,
    51.85866725899541,
    0.0,
    52.66926329299895,
    52.285540087999834,
    52.52657742200245,
    52.37112856400199,
    52.62405158099864,
    52.57828490099928,
    52.57582184200146,
    52.56020769099996,
    52.482529557004455,
    0.0,
    52.990306597996096,
    52.95114680799452,
    52.929259810000076,
    53.029971789997944,
    52.94886256900645,
    52.91591003500071,
    52.87309890499455,
    52.866522733995225,
    52.70944043099735,
    52.82785845900071,
    52.67448581699864,
    52.67091708099906,
    52.55927336699824,
    52.67489023700182,
    52.66954894900118,
    52.432192309999664,
    52.54677515599906,
    52.53526607999811,
    52.427786005995586,
    52.701797562003776,
    52.65370417899976,
    52.538092073998996,
    52.581326277999324,
    52.54876306799997,
    52.54232244299783,
    52.51152267099678,
    52.40839747300197,
    52.37489062199893,
    52.39331307599787,
    52.35830759599776,
    52.289338909002254,
    52.28641562499979,
    52.27295052500267,
    52.416420653004025,
    52.52190911400248,
    52.49737648900191,
    52.625082259997725,
    52.70303006799804,
    52.679608124999504,
    52.92406843799836,
    52.92106671100191,
    52.90179197499674,
    52.761071083004936,
    52.802789100001974,
    52.75812577200122,
    52.829824690998066,
    52.80808401800459,
    52.762460604004445,
    53.01940869900136,
    52.90858898100123,
    52.85044386399386,
    52.837112959998194,
    53.117673389999254,
    53.07721283999854,
    53.06615323600272,
    53.046937529004936,
    52.8460773830011,
    52.94805057299527,
    52.85406104299909,
    52.79629154299619,
    52.650457622999966,
    52.71219471200311,
    52.705415805001394,
    52.656703859996924,
    52.652102056999865,
    52.650009039003635,
    52.554371526995965,
    52.65125722999801,
    52.632457608997356,
    52.93702581699472,
    53.046837763999065,
    52.95819942899834,
    52.89884819299914,
    52.84916698199959,
    53.131974357005674,
    52.948813553994114,
    52.936191895001684,
    52.92091797300236,
    53.03080272799707,
    53.20078668100177,
    53.168684096002835,
    53.1990032130052,
    53.186140974998125,
    53.17959431499912,
    53.09066010500101,
    53.19238665600278,
    53.0153323859995,
    52.99081901900354,
    53.11031179100246,
    52.935274090006715,
    53.13465942899347,
    53.42078609299642,
    53.52108428700012,
    53.51252844699775,
    53.614150976995006,
    53.713652143997024,
    53.581270235001284,
    53.770874809000816,
    53.767234236998775,
    53.686951144001796,
    53.67727998900227,
    0.0,
    54.52207122999971,
    54.51128874300048,
    54.63213274000009,
    54.58579312299844,
    54.54458211999736,
    54.51343699800054,
    54.46020120100002,
    54.38259741799993,
    54.76353822099918,
    55.05172297600075,
    55.045166187002906,
    55.2760419039987,
    55.26154017099907,
    55.252653135998116,
    55.24982265500148,
    55.16729159300303,
    55.11015037600009,
    55.2257426799988,
    55.23147675499786,
    55.11260377999861,
    55.37597729299887,
    55.34646809800324,
    55.30555115600146,
    55.32064603899926,
    55.31751361300121,
    55.28826747700077,
    55.25120881699695,
    55.21588920600334,
    55.30261983600212,
    55.28200826999819,
    55.264357686995936,
    55.336386147006124,
    55.2522618120056,
    55.16840165499889,
    55.15208472299855,
    55.23499298800016,
    55.15570149400446,
    55.33681472900207,
    55.2982175319994,
    55.25661989400396,
    55.07418838200101,
    54.874466603003384,
    55.189806826994754,
    55.146063792999485,
    55.109799846002716,
    55.08530870699906,
    55.078183097000874,
    55.18955313200422,
    54.89477937099582,
    54.99480870600382,
    54.95924092099449,
    55.12193887399917,
    55.24268470699462,
    55.23609958399902,
    55.343499957998574,
    55.31587019899598,
    55.749434664001456,
    55.71577095799876,
    55.67365098799928,
    55.60557039899868,
    55.56824366799992,
    55.56327284100553,
    55.49381117900339,
    55.44898976999684,
    55.43156900699978,
    55.38952532500116,
    55.14817395999853,
    0.0,
    56.17319487700297,
    56.27207365999493,
    56.173045864001324,
    56.28613824900094,
    56.253481398001895,
    56.17495705500187,
    56.46270458599611,
    0.0,
    57.04436163700302,
    57.02551208400109,
    57.16162431600242,
    57.255116154999996,
    57.1966091409995,
    57.28846940299991,
    57.4184587560012,
    57.507181446002505,
    57.48301250900113,
    57.41736471100012,
    57.49435154499952,
    57.47478004799632,
    57.37309732099675,
    57.311448004998965,
    57.218722019999404,
    57.29997303599521,
    57.26095930500014,
    57.25015707000421,
    57.08364063600311,
    57.077742909001245,
    57.07618751600239,
    57.18284730100277,
    57.23636399800307,
    57.16982906799967,
    56.947053380004945,
    56.8963149179981,
    56.99918286900356,
    56.97506495400012,
    56.88721954200446,
    0.0,
    57.484288407998974,
    57.38222359700012,
    57.14783715099475,
    57.22389203100465,
    57.2154528040046,
    57.33377621499676,
    0.0,
    57.536236596999515,
    57.51664646700374,
    57.30768660600006,
    57.345802737996564,
    57.465154318000714,
    57.324422095000045,
    57.4146686170061,
    57.69591663000028,
    57.61718496400135,
    57.70747629600373,
    57.77543485000206,
    57.73989498100127,
    57.61301878299855,
    57.6323016430033,
    57.411926005996065,
    57.36509996899986,
    57.633614310005214,
    57.58818053499999,
    57.43762664899987,
    57.38173349599674,
    57.338385575996654,
    57.33271469199826,
    57.470983293002064,
    57.46391387100448,
    57.475387275000685,
    57.44484558900149,
    57.57879178599978,
    57.66626907399768,
    57.6455043859969,
    57.76801701699878,
    58.006115595002484,
    57.97923915700085,
    58.03488387399557,
    57.91284549500415,
    57.887334104998445,
    57.805690701003186,
    57.88087073699717,
    57.832855580003525,
    57.82444874299836,
    57.94221252499847,
    57.85658831299952,
    57.85437320300116,
    57.852852046002226,
    57.83618124500208,
    0.0,
    58.31993736200093,
    58.42591273599828,
    58.391983745998004,
    58.50234295499831,
    58.37644484500197,
    58.47821496499819,
    58.41566846400383,
    58.5235253810024,
    58.45220676300232,
    58.571279763003986,
    58.85791740100103,
    58.8677538569973,
    58.81161545899522,
    59.0968639940038,
    58.95172347599873,
    59.18879608100542,
    59.12579332500172,
    59.37993997800368,
    59.2660636790024,
    59.17845370800205,
    59.165473301996826,
    59.12724347400217,
    59.0893372290011,
    59.30146847200376,
    59.19957260699448,
    59.29765249200136,
    59.28738652500033,
    59.195965509003145,
    59.24120676000166,
    59.24536728599924,
    59.64970959999482,
    59.5261218759988,
    59.47676380399935,
    59.44148948400107,
    59.420963575001224,
    59.5275095850011,
    59.490806895002606,
    59.48313191800116,
    59.54319360500085,
    59.488704458999564,
    59.44260905600095,
    59.51693511099438,
    59.50414570199791,
    59.376015702000586,
    59.22172939700249,
    59.19265101500059,
    59.29431204100547,
    59.29043356799957,
    59.25140959100099,
    59.24732714499987,
    59.13234144599846,
    59.081502554996405,
    59.16260999199585,
    59.15140968700143,
    59.18505023299804,
    59.30226219099859,
    59.13092723199952,
    59.255515877994185,
    59.18318305900175,
    59.160502455000824,
    59.13058660500246,
    59.07894406699779,
    59.460005455999635,
    59.376489310998295,
    59.495865641998535,
    59.43471791299817,
    59.71709949900105,
    0.0,
    60.35586034800508,
    60.319190309994156,
    60.255161397995835,
    60.348454602004495,
    60.34350439300033,
    60.27595152100548,
    60.36437736300286,
    60.320995756999764,
    60.49170206800045,
    60.55474289200356,
    60.543479553001816,
    60.496342672995524,
    60.41223186299612,
    60.51948043300217,
    60.61241993300064,
    60.68659095699695,
    60.601134094999,
    60.43459391500073,
    60.52166689799924,
    60.45285258399963,
    60.424490823003,
    60.51793597300275,
    60.41698241999984,
    60.368086367001524,
    60.28978687900235,
    60.153524387998914,
    60.07041089099948,
    59.872893359002774,
    59.96989086100075,
    59.8376600699994,
    59.80815279200033,
    59.92694481700164,
    59.90979294799763,
    59.865744771006575,
    59.64438947600138,
    59.575680337999074,
    59.515708784994786,
    59.573757736994594,
    0.0,
    60.0546050280027,
    60.014520468997944,
    60.288819615998364,
    60.25673963400186,
    60.3575700989968,
    60.29724525700294,
    60.4180146409999,
    60.47537182299857,
    60.45274013299786,
    60.347181354001805,
    60.32077409300109,
    60.391946080002526,
    60.38452730300196,
    60.3287885680038,
    60.30195887500304,
    60.289434840997274,
    60.410042133997194,
    60.40630220499588,
    60.44045155299682,
    60.419969058995775,
    60.503215443000954,
    60.0254132150003,
    59.91156942900125,
    59.701371830000426,
    59.60545781100518,
    59.54902752400085,
    59.542965660002665,
    59.616198827003245,
    59.85235220699542,
    59.749107436000486,
    59.80715279799915,
    59.80438617399341,
    59.69109566300176,
    59.79847119400074,
    59.791306346000056,
    59.72087706299499,
    59.819314626998676,
    59.76033397800347,
    59.78468475800037,
    59.79181135499675,
    59.77591616000427,
    59.89771333500539,
    59.86242783800117,
    59.97145661999821,
    60.072743049000564,
    60.167128837994824,
    60.20731453800545,
    60.164253139999346,
    60.407916169999226,
    60.39788799899543,
    60.344391500999336,
    60.04516692600009,
    60.297532678996504,
    60.07133046299714,
    60.32347875599953,
    60.10981709000043,
    60.07366193399503,
    60.03927692099387,
    60.10493472500093,
    60.0304819180019,
    59.81176117299765,
    59.93868302699411,
    59.92714445599995,
    59.89110433200403,
    59.88966954199714,
    59.75623341699975,
    60.020977378997486,
    60.167380469996715,
    60.16046525500133,
    60.14521591099765,
    60.08849475499301,
    60.07929955799773,
    60.355307890000404,
    60.447058207995724,
    60.56692591300089,
    60.637671176002186,
    60.57985506700061,
    60.505597700997896,
    60.471021988007124,
    60.33155783999973,
    0.0,
    61.06133996499557,
    61.17299978300434,
    61.150278495995735,
    61.424723589996574,
    61.383481297998514,
    61.223028567001165,
    61.15504713799601,
    61.25185404199874,
    61.24633319200075,
    61.38171067000076,
    61.24237288400036,
    61.645618752998416,
    61.64023129599809,
    61.753795964003075,
    61.6837451340034,
    61.63487865200295,
    61.57974558600108,
    61.57245072299702,
    61.53947446300299,
    61.58846842800267,
    61.563001483002154,
    61.50108214200009,
    61.34273744800157,
    61.179817550000735,
    61.29448347500147,
    61.17403894999734,
    61.28156425700581,
    61.26145484799781,
    61.18787211399467,
    61.175296795001486,
    61.05904392799857,
    61.168200264000916,
    61.15151600000536,
    61.11455218799529,
    61.06081360999815,
    61.27322313699551,
    61.26276307599619,
    61.39173930799734,
    61.25167432300077,
    61.14177494300384,
    61.184714942995925,
    61.30460255100479,
    61.25107503599429,
    61.32485369200003,
    61.305570429998625,
    61.35718054500467,
    61.330649771000026,
    61.59079696900153,
    61.50455616399995,
    61.47673281000607,
    0.0,
    62.529899886998464,
    62.49252940199949,
    62.60459617000015,
    62.596047111997905,
    62.551003309003136,
    62.33683724200091,
    62.248502465001366,
    62.246834952995414,
    62.371181684000476,
    62.320921530001215,
    62.201183151002624,
    62.04828834500222,
    62.312574586001574,
    62.307778007001616,
    62.39889376299834,
    62.2495827199964,
    62.257946133002406,
    62.20351886199933,
    62.15276821500447,
    62.24099395599478,
    62.336888887002715,
    62.33596270799899,
    62.47776784400048,
    62.483861374996195,
    62.45023038900399,
    62.38195874700614,
    62.442379863001406,
    62.56598982299329,
    62.609651423001196,
    62.41068439899391,
    62.39202265500353,
    62.514674852995086,
    62.43045528600487,
    62.68601650900382,
    62.680458278002334,
    62.64129017099913,
    62.65053649499896,
    62.69166264600062,
    62.6806345379955,
    62.66814998299378,
    62.74751766700501,
    62.65367124700424,
    62.64388859299652,
    63.088405146001605,
    62.961683925001125,
    62.957893949998834,
    62.90509874700365,
    63.15521776300011,
    63.144735617999686,
    63.383339583000634,
    63.34654196900374,
    63.32801344299514,
    63.310007330997905,
    0.0,
    64.46931968600256,
    64.5492637760035,
    64.52694917500048,
    64.64239670599636,
    64.59612692899827,
    64.55669166799635,
    64.51426600899867,
    64.45401677100017,
    64.40625229600118,
    64.32625470800122,
    64.28989463199832,
    64.39555734899477,
    64.28197458299837,
    64.32418882899947,
    64.03215443399677,
    63.934498694005015,
    64.22851149299822,
    64.26066474500112,
    64.22861707300035,
    64.13215377900633,
    64.19579477100342,
    64.08195628300018,
    0.0,
    64.69040726700041,
    64.63666146300238,
    64.55681909000123,
    64.54637607899349,
    64.54300887300633,
    64.5033146459973,
    64.52061281799979,
    64.5091059099941,
    64.49484095099615,
    64.57673909499863,
    64.66368532299384,
    64.61151261700434,
    64.68522846900305,
    64.68113874499977,
    64.4882191949946,
    64.58810421400267,
    64.24026469300588,
    0.0,
    0.0,
    65.11354970899993,
    65.41171068099356,
    65.31380452099984,
    65.56021516300098,
    65.4972556749999,
    65.54018030699808,
    65.53847200000018,
    65.44832755599782,
    65.57079187799536,
    65.52745074900304,
    65.64933085599478,
    65.63452022500132,
    65.8481389860026,
    65.945115221999,
    65.90846120599599,
    65.8767980449993,
    65.93503259099816,
    65.8672548619943,
    65.95014011400053,
    66.03556051399937,
    65.99363919699681,
    66.12141552099638,
    0.0,
    66.8180511380051,
    66.85133430199494,
    66.97866500799864,
    66.878031500004,
    66.87487886899908,
    66.84704880299978,
    66.81899299799989,
    66.79496718199516,
    66.88242453499697,
    66.87222090699652,
    0.0,
    67.4943563889974,
    67.59716116500203,
    67.73311594300321,
    67.70400561499991,
    67.74921008000092,
    67.73530842199398,
    67.8657550259959,
    68.12876591599343,
    68.10196710900345,
    68.02165546600008,
    67.97566451799503,
    68.08495174499694,
    68.07490271699498,
    68.02980424799898,
    68.12430863900227,
    68.36097881500609,
    68.28694811299647,
    68.20997767600056,
    68.18210667600215,
    68.26686610800243,
    68.23429016000591,
    68.4487482800032,
    68.55877619400417,
    68.41262104499765,
    68.33360238200112,
    68.07678400399891,
    68.07105614199827,
    68.30418636200193,
    68.25880981500086,
    68.21573737199651,
    68.12809562399343,
    68.06799265299924,
    68.00390405000508,
    67.72935378300463,
    67.68835201000184,
    67.60955564000324,
    67.68321631399886,
    67.66213969900127,
    67.61211752800591,
    67.59388658799435,
    67.6595481139957,
    67.64809781200165,
    67.47861982400354,
    67.62827757900232,
    67.78641692599922,
    67.75069650999649,
    67.67931748899719,
    67.63129622900306,
    67.5890232449965,
    67.53193410400127,
    67.50040642700333,
    67.46478108999872,
    67.55921828999999,
    67.82376652500534,
    67.88976040900161,
    67.9622339109992,
    67.94159574600053,
    67.99685589699948,
    67.91967365999881,
    67.99409449200175,
    67.94976645599672,
    67.92465179999999,
    67.89427630299906,
    67.85420830499788,
    68.09589639100159,
    67.76891624100244,
    67.83168324700091,
    67.72328568700323,
    67.69396917300037,
    67.51355724699533,
    67.51777997399768,
    67.43994136400579,
    67.29654513399873,
    67.29387749799935,
    67.36378957499983,
    67.32899756999541,
    67.13445722200413,
    67.31383451400325,
    67.2590156389997,
    67.3135986740017,
    67.44300893600303,
    67.55410477400437,
    67.49417768900457,
    67.19077943599405,
    67.16581217299972,
    67.08953948099952,
    67.02689025799918,
    67.1305864140013,
    67.10141238399956,
    66.82483547599986,
    66.91680828799872,
    67.0228010360006,
    67.00255558799836,
    66.89827103599964,
    66.84986469399882,
    66.97236230900307,
    66.92806814800133,
    66.91703208599938,
    66.76871824599948,
    66.81704653699853,
    66.81445708199317,
    66.68538265399548,
    66.67368122600601,
    66.474324195995,
    66.58884274100274,
    66.90385705700464,
    66.81912107099924,
    66.78353673200036,
    66.90008306899836,
    66.71823284599668,
    66.66139854300127,
    66.65806019799493,
    66.73971674499626,
    66.6324209250015,
    66.88376794499345,
    66.83490280299884,
    66.90392448299826,
    67.00506319700071,
    66.8057420289988,
    66.71114905700233,
    66.61244006399647,
    66.60054166199552,
    66.58400527200138,
    66.56523187999846,
    66.71692758100107,
    66.96006018999469,
    66.96017149099498,
    66.9565220939985,
    66.94677747300011,
    66.93847234499844,
    67.01685126299708,
    67.13402370100084,
    67.13106258600601,
    67.17412728800264,
    67.14202105300501,
    67.09194373300124,
    67.06857686399599,
    67.15930727899831,
    67.12759850899602,
    67.10677525999927,
    67.1811094030054,
    67.141569075,
    67.13846538600046,
    67.23406858499948,
    67.52363879900076,
    67.50952953800152,
    67.4265835979968,
    67.51707698099926,
    67.49332054299884,
    67.42055494099623,
    67.40332396199665,
    67.3039212339936,
    67.57657154100161,
    67.57003333300236,
    67.35853480499645,
    67.34525954799756,
    67.44689231100347,
    67.39140191900515,
    67.15514020999399,
    67.18015128400293,
    67.15558963500371,
    67.12713860400254,
    67.0524717340013,
    66.96462234800128,
    66.86607383199589,
    66.81746298399958,
    66.73757390299579,
    66.69022053500521,
    66.77565432700067,
    66.68099001500377,
    66.71252470000036,
    66.78359576400544,
    66.76291341199976,
    66.86158123700443,
    66.78721177099942,
    66.77261706800346,
    66.86211513500166,
    66.85736576500494,
    66.82954396100104,
    66.71320627800014,
    66.58090586300386,
    66.55670019699755,
    66.54688643499685,
    66.60312304099352,
    66.59713116599596,
    66.83333908799978,
    66.83244836700032,
    66.57370654700208,
    66.54898828799924,
    66.81070589600131,
    66.78131700700033,
    66.89629864200106,
    66.81905152899708,
    66.92471760499757,
    66.84081856199919,
    66.80582370399497,
    66.27019717700023,
    66.24750889300049,
    66.24806564799655,
    66.19756839099864,
    66.10870887900091,
    66.19699560500158,
    66.09811674999946,
    66.22371406899765,
    66.24875277899991,
    65.93822816600004,
    65.8771633619981,
    65.99640164399898,
    65.94569823799975,
    0.0,
    66.57520635399851,
    66.50717722900299,
    66.38937826999609,
    66.23054567499639,
    66.51961312500498,
    66.61889772699942,
    66.55734719400061,
    66.54120398100349,
    66.62400979299855,
    66.49652414699813,
    66.40605102500558,
    66.3645488789989,
    66.625139097996,
    66.56906618900393,
    66.49560432099679,
    66.60302517200034,
    66.6867001280043,
    66.49672524399648,
    66.59964606499852,
    66.71488153899554,
    66.53908048000449,
    66.79833905200212,
    66.69450893099565,
    66.80951123900013,
    66.94726244000049,
    66.92704930799664,
    66.92606218899891,
    66.82361445100105,
    66.6662429640055,
    66.52764812299574,
    66.54904068500036,
    66.5657261170054,
    66.51248564100388,
    66.46293173099548,
    66.56041743199603,
    66.66468736699608,
    66.61389400800545,
    66.60534963100508,
    66.58768545299972,
    66.54293023799983,
    66.53665467300016,
    66.63309496000147,
    66.62844627199956,
    66.51704513200093,
    66.61647922399425,
    66.51327880399913,
    66.80696175799676,
    66.79155853799602,
    66.87685460300418,
    66.700459108004,
    66.68346495600417,
    66.7347787059989,
    66.8561741529993,
    66.81840686199575,
    66.75348993299849,
    66.72836416700011,
    66.63989868899807,
    66.73998795899388,
    66.68842053299886,
    66.79264584200428,
    66.71334256300179,
    66.6966033810022,
    66.79268077300367,
    66.72040178300085,
    66.63400312999875,
    66.6115759020031,
    66.5516018749986,
    66.63078741900244,
    66.76223680800467,
    66.84430669699941,
    66.58041484300338,
    66.53605887899903,
    66.33651690999977,
    66.33933495300153,
    66.27954564399988,
    66.20125519800058,
    66.1223366980048,
    66.20288751499902,
    66.09880244699889,
    66.09694511399721,
    66.0714917869991,
    66.06580289299745,
    66.09047308600566,
    66.07735092900111,
    66.02699965400097,
    66.02602476999891,
    66.13944154600176,
    66.21719372399821,
    65.94657043600455,
    65.94507365399477,
    66.04081983299693,
    66.0130188500043,
    65.95514156900026,
    66.17420909399516,
    66.41622352000559,
    66.3191557359969,
    66.41477583900269,
    66.38569715800259,
    66.37012150800001,
    66.35350183300034,
    66.25148719499703,
    66.52868707099697,
    66.56278983400261,
    66.53015076500014,
    66.41118841400021,
    66.34615380699688,
    66.32689913300419,
    66.2695490440019,
    66.25085479800327,
    66.16563521300122,
    66.16085908800596,
    66.28430589100026,
    66.5663548090015,
    66.45790780300013,
    66.299912829003,
    66.40153114499844,
    66.45427289600048,
    66.42843115000142,
    66.33355747000314,
    66.44662793099997,
    66.42296348600212,
    66.38729699100077,
    66.4951285490024,
    66.34869952900044,
    66.41280997600552,
    66.38171981499909,
    66.63470134600357,
    66.76883658499719,
    66.71413490900159,
    0.0,
    67.72209992299759,
    67.80469453400292,
    67.75611174599908,
    67.87107892199856,
    68.08743878300447,
    68.38487862599868,
    68.3655086400031,
    68.53838658799941,
    68.62270395899395,
    68.45953044300404,
    68.33688735900068,
    68.26808854900446,
    68.18266184600361,
    68.0353043340001,
    68.03114806699887,
    68.01793963000091,
    68.00565286499477,
    68.04382815199642,
    68.01257378599985,
    68.29002302899607,
    68.18571595599497,
    68.37488813700475,
    68.34905785400042,
    68.15643251499569,
    68.38304143900314,
    68.47900277499866,
    68.6007282999999,
    68.49665960599668,
    68.3687831290008,
    68.66296704999695,
    68.52844773499965,
    68.47185337200062,
    68.20270691699989,
    68.15930318400206,
    68.04077559099824,
    68.09077327400155,
    67.96614553900145,
    67.94349620999856,
    67.9074092620067,
    68.01740295399941,
    68.00896404699597,
    67.77666936800233,
    67.79214355399745,
    67.91393701599736,
    68.16167725500418,
    68.15596909799933,
    68.12454733499908,
    68.2468613730016,
    68.48483429100452,
    68.48178906699468,
    68.52072853299615,
    0.0,
    68.91005207299895,
    68.993331235004,
    69.30716705400118,
    69.28904093599704,
    0.0,
    69.87266125300084,
    70.14740488999814,
    70.09373556799983,
    70.19706975800364,
    70.30608543799462,
    70.52511503800633,
    70.49249426499591,
    70.43486462200235,
    70.5111003100028,
    70.79282536499522,
    70.7863388889964,
    70.70407076399715,
    70.82552235099865,
    70.94155499400222,
    71.2095682729996,
    71.193983363999,
    71.12552710300224,
    71.21266819599987,
    71.1002398099954,
    71.08656501400401,
    71.02957277999667,
    71.28819654100516,
    71.24925454199547,
    71.21713572199951,
    71.28881499999989,
    71.19550453499687,
    71.07713965199946,
    70.93776723599876,
    71.1828938769977,
    71.17862302900176,
    71.15542227399419,
    71.11147411399725,
    71.14994220199878,
    71.08224341800087,
    71.34949927900016,
    71.29810977200395,
    71.14281469699927,
    71.24383403100364,
    71.20860633099801,
    71.08806506600376,
    70.85012723399996,
    70.9746894609998,
    70.94414253700234,
    71.14877804499702,
    71.13071057900379,
    71.03091331299947,
    70.8955790559994,
    70.86363416800305,
    70.83194474699849,
    70.81859512799565,
    70.79086901200208,
    70.78267005600355,
    70.6898343770008,
    70.77253026399558,
    70.86980863999634,
    70.8592969099991,
    70.77920279300452,
    70.71646646899899,
    70.7457410729985,
    70.65360412999871,
    70.74209324899857,
    71.03935608200118,
    71.05829507199815,
    0.0,
    71.30188707599882,
    71.27114830500068,
    71.26767771800223,
    71.38694344800024,
    71.25424924399704,
    71.19461149999552,
    71.47559325399925,
    71.34079981700052,
    71.3209857940019,
    71.60243533199537,
    71.56969897700037,
    71.55288116799784,
    0.0,
    71.96692360499583,
    71.9618385979993,
    72.24951082000189,
    72.22419429000001,
    72.34594520099927,
    0.0,
    72.84930147299747,
    0.0,
    73.40640426800383,
    73.40035910400184,
    73.34475058700627,
    73.35008723999636,
    73.27107298099872,
    73.40051746500103,
    73.46625028400013,
    73.44704120700044,
    73.1965148379968,
    73.31395148000593,
    73.31085088400141,
    73.28287907299818,
    73.24329791800119,
    73.21254688499903,
    73.3173009279999,
    73.41859850600304,
    73.41791904599813,
    73.40072751000116,
    73.50955253900611,
    73.45859817299788,
    73.4534224070012,
    73.36514009299572,
    73.41960337800265,
    73.38646804400196,
    73.43422300699604,
    73.4315680380023,
    73.53802426000038,
    73.47419928000454,
    73.56164974399871,
    73.45321042300202,
    73.44126992500242,
    73.2407349549976,
    73.29395495800418,
    73.19748552999954,
    72.94304794799973,
    72.98170751200087,
    73.06334463700477,
    73.2819043779964,
    73.24518328600243,
    73.23570564100373,
    73.35871329100337,
    73.3050510930043,
    73.40480649399979,
    73.3989666760026,
    73.52983303300425,
    73.48611238600279,
    73.45955836999929,
    73.5311612459991,
    73.57599566700083,
    73.49970875200233,
    73.71250940999744,
    73.83522767300019,
    73.82606104400475,
    74.11551918199984,
    74.09351084900118,
    74.05179579900141,
    74.1273024990005,
    74.14713141899847,
    74.1096263770014,
    74.07208121800068,
    74.01235798400012,
    74.00532377500349,
    74.12069851800334,
    74.17248980300064,
    74.08365504899848,
    73.89254320699547,
    73.87594963199808,
    73.92887780399906,
    73.81940425699577,
    73.79552355100168,
    73.80078035299812,
    73.76999336500012,
    73.76345495999703,
    73.75423110799602,
    73.73429252600181,
    73.66478125100548,
    73.6754893949983,
    73.78114628000185,
    73.73943417099508,
    73.70778384299774,
    73.81311608700344,
    73.80483572199591,
    73.77041558900237,
    73.92289345100289,
    73.86675085400202,
    73.85280925199913,
    73.79565482400358,
    73.92630221699801,
    73.91663648100075,
    73.8128757999948,
    73.7129149550019,
    73.69604725499812,
    73.69231203299569,
    73.80787241300277,
    73.74216420300218,
    73.72475420999399,
    73.5889240059987,
    73.53827809399809,
    73.47043537100399,
    73.71009473999584,
    73.53504038699612,
    73.63924301500083,
    73.58203848300036,
    73.55306492600357,
    73.50115725299838,
    73.46030463200441,
    73.37399166500109,
    73.34943871299765,
    73.00958922100108,
    73.07298145600362,
    73.06723869599955,
    73.14295564500208,
    73.131206570004,
    73.30776274600066,
    73.27979168800084,
    73.27812704600365,
    73.19458647300053,
    73.18992155600426,
    73.44639646400174,
    73.43514985399815,
    73.55489798499912,
    73.50437134499953,
    73.6727682540004,
    73.65492505300062,
    73.92964311700052,
    74.05555287300376,
    74.21918648800056,
    74.21320400699915,
    74.11639526900399,
    0.0,
    74.40727069000422,
    74.70500320599967,
    74.68759507399955,
    74.67355278399918,
    0.0,
    75.16004405100102,
    75.1252100510028,
    75.10154830800457,
    75.20625632999872,
    0.0,
    75.91985079100414,
    75.86006122199615,
    75.96430107400374,
    75.85644329799834,
    75.83069158199942,
    75.82292459400196,
    75.77693677400384,
    75.74540979000449,
    75.79682071200659,
    76.0725557140031,
    76.05442904099618,
    76.11637574400083,
    76.21729535700433,
    76.26706742199895,
    76.12070202700124,
    75.99108210900158,
    75.94405642300262,
    75.93503753299592,
    75.89542269500089,
    75.81696751099662,
    75.77818718399794,
    75.74287758900027,
    75.9696982769965,
    75.92467576500349,
    76.04361329499807,
    76.03660157600098,
    76.01499694200174,
    75.9447320160034,
    75.90016303899756,
    75.88040519099741,
    75.84715108299861,
    75.79843706599786,
    75.83971608499996,
    75.78203542499978,
    75.75139923700044,
    75.83879709499888,
    75.90711941699556,
    75.90034353899682,
    75.87316139099858,
    75.85969697200198,
    75.84476757799712,
    75.95554680499481,
    75.88606482899922,
    75.8543103330012,
    75.907717201997,
    75.92965486799949,
    75.91677501499362,
    75.81600798,
    75.79388761200244,
    75.86667209599545,
    75.85127743500198,
    75.81257768000069,
    76.01579993799533,
    76.22559228099999,
    76.19427193999582,
    76.21493905600073,
    76.04960355599906,
    75.99848354300048,
    76.0800729850016,
    76.20543852200353,
    76.14648954299628,
    76.47241717200086,
    76.67494023900508,
    76.66275550800492,
    76.58105754500139,
    76.54962734099536,
    76.5417140310019,
    76.80453338900406,
    77.0871659580007,
    77.04533687300136,
    76.8832298329944,
    77.00571025799582,
    76.94317743499414,
    77.04281847200036,
    77.02453444200364,
    76.87055111600057,
    76.98108206700272,
    77.05884091999906,
    77.18904640300025,
    77.23974255099893,
    77.17340293400048,
    77.15114985800028,
    77.12568888399983,
    77.22884836499725,
    77.20521937200101,
    77.37974001300609,
    77.33752332100266,
    77.4390757719957,
    77.16594210600306,
    77.02724476299773,
    77.14531160899787,
    77.13017514300009,
    77.05175711800257,
    76.59557190800115,
    76.8826109340007,
    76.79838141499931,
    76.77664925800491,
    76.76194690699776,
    76.69608552800491,
    76.8759568509995,
    77.0134840430037,
    76.98189735499909,
    77.08008165100182,
    77.30706753499544,
    77.28368695699464,
    77.38679239699559,
    77.6542316289997,
    77.70971644100064,
    77.70937092199893,
    77.59552645200165,
    77.49524457499501,
    77.90114988099958,
    0.0,
    78.44445315699704,
    78.40968901199813,
    0.0,
    79.8071553339978,
    79.88661083800253,
    79.8269251449965,
    79.78576428499946,
    79.89297228799842,
    79.88274221900065,
    79.96036810299847,
    80.01172573000076,
    80.12164099599613,
    80.11827228800394,
    80.10222421399521,
    80.07578063799883,
    79.9836951920006,
    0.0,
    80.38766734700039,
    80.35630689499521,
    80.20993094400183,
    80.2889474879994,
    80.34158304500306,
    80.46993770299741,
    80.25911069599533,
    80.2193951329973,
    80.00993646599818,
    80.12049635400035,
    80.20378175599762,
    80.0376214459975,
    80.01425716600352,
    80.10867771099583,
    80.09885893300088,
    80.05902610799967,
    80.05322218599758,
    80.00416758199572,
    79.93744112599961,
    79.7422782270005,
    79.89024772300036,
    79.80218853699625,
    79.71705473300244,
    79.70476614400104,
    79.8023713099974,
    79.66966897000384,
    79.55036494499655,
    79.63111835900054,
    79.96136928100168,
    80.05907120899792,
    80.23606289600139,
    80.25235773800523,
    80.2405175640015,
    80.3359158050007,
    80.30442279499403,
    80.35891479899874,
    80.35130306300562,
    80.34293007000088,
    80.44537198199396,
    80.54553167700215,
    80.54115350900247,
    80.34982296899398,
    80.45013430299878,
    80.3310389870021,
    80.43606868499774,
    80.52653310500318,
    80.39321885500249,
    80.48434237499896,
    80.70699772499938,
    80.60940978999861,
    80.55988321600307,
    80.54507893700065,
    80.53643132699654,
    80.55976700099563,
    80.76040242799354,
    80.65732225800457,
    80.56621499600442,
    80.54619529800402,
    80.54289989600511,
    80.6607563150028,
    80.62904676799371,
    80.58311796599446,
    80.62180955400254,
    80.71788679800375,
    80.68110647599678,
    80.72418404799828,
    80.69043148300261,
    80.56806663000316,
    80.47129078200669,
    80.5124639719943,
    80.30650633000187,
    80.25946637200104,
    80.35221025499777,
    0.0,
    0.0,
    80.1213086179996,
    0.0,
    0.0,
    79.90103554300003,
    0.0,
    0.0,
    0.0,
    79.62572218300193,
    79.84934607899777,
    79.79234759899555,
    79.75187498000014,
    79.85188020500209,
    79.69121095000446,
    79.7885592370003,
    79.78511504299968,
    79.6369070159999,
    79.73029733799922,
    79.96591913299926,
    79.93353959399974,
    79.87249458800216,
    79.81945407499734,
    79.81731353800569,
    0.0,
    79.63486958899739,
    0.0,
    79.37277013200219,
    79.36000796200096,
    79.28938966600253,
    0.0,
    0.0,
    0.0,
    0.0,
    79.14962149599887,
    79.27448786000605,
    78.91402063400164,
    78.90990536999743,
    79.01626797000063,
    78.93439137499809,
    79.04930319899722,
    78.9848873209994,
    78.94475908399909,
    78.88977218599757,
    78.88611430700257,
    78.88400290899881,
    78.84923458300182,
    78.88630434199877,
    78.66403048799839,
    78.64930987199477,
    78.63599841799441,
    78.66763406599785,
    78.4556619990035,
    78.38947491600265,
    78.33451918199717,
    78.1825670870021,
    78.15894269000273,
    78.25877334499819,
    78.04946362999908,
    78.04515077699762,
    78.24993952699879,
    78.24079849500413,
    78.32224356899678,
    78.20547413599706,
    0.0,
    78.23547748500278,
    78.02106358400488,
    77.93768244899547,
    78.0562750550016,
    78.0165559800007,
    0.0,
    78.08163878999767,
    78.18823823799903,
    78.48246125100559,
    78.47155844399822,
    0.0,
    0.0,
    78.36633063799673,
    0.0,
    78.2513606270004,
    78.22061225099606,
    78.28848719300004,
    78.22654219399556,
    78.26934697900288,
    0.0,
    0.0,
    0.0,
    78.25708683699486,
    0.0,
    0.0,
    0.0,
    0.0,
    77.9150751950001,
    77.96012257400434,
    77.9442658629996,
    0.0,
    77.8292428470013,
    78.07537577200128,
    78.0439200020046,
    78.16024137099885,
    78.14187146800396,
    78.0355866309983,
    0.0,
    78.12946601400472,
    78.07546878799621,
    78.30997590799961,
    78.13717494199955,
    78.09543745100382,
    77.86108260000037,
    77.8469994249972,
    77.94269321400498,
    78.05482125500566,
    77.97830645700014,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    77.34151800000109,
    77.40374436000275,
    77.39826504699886,
    77.31379196300259,
    77.26029125700006,
    77.15793189400574,
    0.0,
    0.0,
    77.662663436,
    0.0,
    0.0,
    0.0,
    0.0,
    77.53457397600141,
    77.53004366400273,
    77.46177492100105,
    77.45754371899966,
    77.13455553600215,
    77.41505377700378,
    77.4019580230015,
    77.3670583330022,
    77.35734502899868,
    77.30080352999357,
    77.52695707499515,
    0.0,
    0.0,
    77.55973078600073,
    77.38266788399778,
    77.45479198900284,
    77.70890795299783,
    77.69463125100447,
    77.67599407200032,
    77.77567219900084,
    77.6250895129997,
    77.59923233200243,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    77.06165307500487,
    76.94942106999952,
    76.79845268799545,
    76.78067216900672,
    76.5855307890015,
    76.69146184699639,
    0.0,
    0.0,
    76.94176473699918,
    76.91082884700154,
    76.81739216700225,
    76.7953557410001,
    76.86088867300714,
    76.67539365599805,
    76.6534435029971,
    76.55955563299358,
    76.56313644399779,
    76.55422737800109,
    76.51843824800017,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    76.29584615299973,
    76.20214694499737,
    76.42739663099928,
    76.42692139300198,
    76.33721129000332,
    76.40925346499716,
    76.38415063099819,
    76.77783429999545,
    0.0,
    0.0,
    0.0,
    76.38851640199573,
    76.35153840800194,
    76.25372447099653,
    76.24425375399733,
    76.27951882899652,
    0.0,
    0.0,
    0.0,
    0.0,
    76.10245839999698,
    76.09835482899507,
    0.0,
    0.0,
    0.0,
    76.27835086500272,
    76.20747254099842,
    76.30875089699839,
    76.165208325001,
    76.13447374900716,
    0.0,
    76.22284937000222,
    76.21128365200275,
    0.0,
    0.0,
    0.0,
    0.0,
    76.214817398999,
    0.0,
    0.0,
    76.04118722699786,
    75.99107756200101,
    0.0,
    75.90020608699706,
    75.90799849599716,
    0.0,
    0.0,
    0.0,
    0.0,
    75.65963765500055,
    75.56706677799957,
    75.56055073900643,
    75.80334836700058,
    0.0,
    0.0,
    75.48221325699706,
    75.37160881800082,
    75.59811984999396,
    75.53903145500226,
    75.50034044899803,
    75.48902763100341,
    75.35670887300512,
    75.34283407499606,
    75.37803843599977,
    75.50933284699568,
    75.59787055999914,
    75.58180689599976,
    75.53534772800049,
    75.50184128199908,
    75.45986433600046,
    75.53380491999997,
    0.0,
    75.55957549499726,
    0.0,
    0.0,
    0.0,
    75.3476257250004,
    75.45943557599821,
    75.59932185600337,
    0.0,
    75.84631310999976,
    0.0,
    76.2039558969991,
    76.18144961899816,
    76.07650681900122,
    0.0,
    0.0,
    76.09797586700006,
    76.22493319399655,
    0.0,
    76.21905145199707,
    0.0,
    0.0,
    0.0,
    0.0,
    76.14661644199805,
    76.12847420700564,
    76.12058667100064,
    75.95755621500575,
    75.90508856700035,
    75.83614351299912,
    76.07097635499667,
    76.17626025700156,
    76.43956118999631,
    76.28448185399611,
    76.27842440400127,
    76.01499311700172,
    75.83999434200086,
    76.02591193300032,
    76.00143297799514,
    76.06775370700052,
    76.0162080210066,
    75.86261819600622,
    75.87024600699806,
    75.92916160299501,
    76.00630985699536,
    76.07767158600473,
    76.05679879599484,
    76.0473125339995,
    75.97347515099682,
    75.96375931300281,
    75.94818773499719,
    75.91176366100262,
    76.10825687800389,
    75.83359263499733,
    75.9578935449972,
    75.94329409200145,
    76.01831022600527,
    76.10105494100571,
    76.32869592100178,
    76.23773274799896,
    76.2574845190029,
    76.23414686999604,
    76.23054477800179,
    76.13354277699545,
    76.26829311899928,
    76.51526683200063,
    76.2948799260048,
    76.26854622900282,
    76.37462240399327,
    76.28686051400291,
    76.25881813599699,
    76.23702196699742,
    76.2129963000043,
    76.08051660600177,
    76.13382018599805,
    76.04947969700152,
    76.00904900900059,
    75.99428807799995,
    76.06065637300344,
    76.04823607100116,
    75.97017424999649,
    76.07662427700416,
    75.91116275999957,
    75.85596433899627,
    76.06191483099974,
    76.14864025200222,
    76.08405059499637,
    76.14459134500066,
    76.39720252100233,
    76.3621804009963,
    76.4851825769947,
    76.61429243799648,
    76.64036055600445,
    76.49983683999744,
    76.28841119700519,
    76.03179028500017,
    75.96930351499759,
    76.04971966100129,
    75.98886740799935,
    75.76026719999936,
    75.64263651899819,
    75.75405991000298,
    76.0435472570025,
    75.9831641569981,
    76.1036117399999,
    76.1022427459975,
    76.21848297600081,
    76.17623901000479,
    76.12352411200118,
    76.21913568300079,
    76.19760902100097,
    75.7218349659961,
    75.74910466900474,
    75.54305866399955,
    75.40514731199801,
    75.30778570399707,
    75.43871211299847,
    75.2915547510056,
    75.3926040389997,
    75.38528997200046,
    75.54737267899327,
    75.46526289699977,
    75.5589714509988,
    75.44353928099736,
    75.4306376530003,
    75.33025195499795,
    75.56494215800194,
    75.54890857099963,
    75.52834075000283,
    75.61910715400154,
    75.48800368900265,
    75.69600554300268,
    75.58205055700091,
    75.57527833399945,
    75.49880564600608,
    75.38099643900205,
    75.35888525799965,
    75.20525592599733,
    75.24864015899948,
    75.23468603599758,
    75.43853320799826,
    75.31172002899984,
    75.36272193399782,
    75.32000921299914,
    75.39812937199895,
    75.36552826899424,
    75.30343978200108,
    0.0,
    76.16602710699954,
    75.95120028300153,
    75.82805515700602,
    75.74585453100008,
    75.77483114299685,
    75.77455992899922,
    75.72733304500434,
    75.67358858200168,
    75.5940266089965,
    75.45009017700067,
    75.43394629400427,
    75.4460009300019,
    75.38134567900124,
    75.3349879380039,
    75.29830344400398,
    75.2623395720002,
    75.26425169999857,
    75.2476051449994,
    75.13533518699842,
    74.9171536089998,
    75.01262878100533,
    74.93403495900566,
    75.0243332980026,
    75.04302867500519,
    74.93793981999625,
    74.93565740800113,
    75.04627143099788,
    75.18105443900276,
    75.28526152400445,
    0.0,
    75.98902729099791,
    75.95631561600021,
    75.90162608899846,
    75.88528348800173,
    75.87887770400266,
    76.09811686800094,
    75.97968291899451,
    75.93426116799674,
    75.88749471100164,
    76.31451636499696,
    76.2462174320026,
    76.32897764600057,
    76.28322570800083,
    76.2543241300009,
    76.32548086499446,
    76.15038792699488,
    76.12049777299399,
    76.09076944000117,
    76.0742683339995,
    75.99806264999643,
    76.09372861800512,
    75.9909175510038,
    76.09149555100157,
    76.03854120100004,
    75.9508898790009,
    75.91438710299553,
    76.01519437899697,
    76.00818110600085,
    75.97131211600208,
    76.17381295099767,
    76.22637539000425,
    76.18537777999882,
    76.14437829500093,
    76.3386175150008,
    76.29524962900177,
    76.1549804440001,
    76.38594435500272,
    76.30102364400227,
    76.16408841800148,
    76.27766223900107,
    76.1128467439994,
    76.10822407599335,
    76.06575941199844,
    76.06661107800028,
    76.04930624400004,
    76.32198712499667,
    76.28078640300373,
    76.18003784800385,
    76.42614903300273,
    76.73393596799724,
    76.7909530149991,
    76.86012867100362,
    76.96138254900143,
    76.9338866629987,
    76.9675327119985,
    76.93353913799365,
    76.9177608369937,
    76.8844742440051,
    77.12864717100456,
    76.87766495600226,
    76.80971825799497,
    76.74027148900495,
    76.86919245900208,
    76.94261401300173,
    76.86375852700439,
    76.71085733800282,
    76.6212173390013,
    76.67632020299789,
    76.62158733600518,
    76.4203848160032,
    76.36200676899898,
    76.19604960599827,
    76.11059949199989,
    76.19069882699841,
    76.1757778320025,
    76.43812389300001,
    76.71877297099854,
    76.61238493800192,
    77.04039305400511,
    76.59951884300244,
    77.03280844300025,
    76.98104623100517,
    77.22015464600554,
    77.07829825999943,
    77.15382636499999,
    77.12706907100073,
    77.40738051600056,
    0.0,
    78.31727893799689,
    78.25500233899947,
    78.2191676720031,
    78.46789782999986,
    78.45118557899696,
    78.65169603999675,
    78.63174375500239,
    78.54803108700435,
    78.66824916999758,
    78.57489027799602,
    78.65182595600345,
    78.62131570999918,
    78.69607778800128,
    78.59604122899327,
    78.57701711499976,
    78.5084985700014,
    78.44359801199607,
    78.5288177340044,
    78.79493720900064,
    78.78648001799593,
    78.87347602000227,
    78.83584787700238,
    79.12285260600038,
    79.01118684800167,
    79.29878940599883,
    79.25996112700523,
    79.10973029500019,
    79.20572120499855,
    79.18839702900004,
    79.11180328500632,
    79.1950756389997,
    79.07431715200073,
    79.31415725000261,
    79.25655624800129,
    79.24492510900018,
    79.21883877499931,
    79.20856324700435,
    79.3385464919993,
    79.56752281400259,
    79.55561876299907,
    79.66379005199997,
    79.62407196900313,
    79.56108703099744,
    79.85040792200016,
    79.76807005100272,
    79.70368370399956,
    79.66502994400071,
    79.61321389699879,
    79.49205311699916,
    79.40327315499599,
    79.63721045499551,
    79.57930003300135,
    79.53677531299763,
    79.47108588799892,
    79.45884341099736,
    79.45387083999958,
    79.5762511790017,
    79.86555755000154,
    79.8265632229959,
    79.73104986199905,
    79.72210460500355,
    79.82902294000087,
    79.79595539699949,
    79.91895476399804,
    79.8995505060011,
    80.11945485400065,
    80.05425928399927,
    79.9534230759964,
    79.90637541299657,
    80.0073324499972,
    79.69592549499794,
    79.73637232399778,
    79.85820156900445,
    79.85198467000009,
    79.68213164700137,
    79.6170777160005,
    79.63938219699776,
    79.55230425899936,
    79.4697004289992,
    79.44251242800237,
    79.4161020459942,
    79.6288328619994,
    79.62639473599847,
    79.73075426899595,
    79.72931727400282,
    79.67018982599984,
    79.59138653599803,
    79.58733838700573,
    79.61394187299447,
    79.45418733800034,
    79.6980149459996,
    79.60889681200206,
    79.55857278100302,
    79.58040049400006,
    79.57672987699334,
    79.45951183299621,
    79.50406885700068,
    79.43489216900343,
    79.42075863700302,
    79.60858177999762,
    79.53950736200204,
    79.58819600199786,
    79.71652390199597,
    80.01751993999642,
    80.09861205099878,
    79.95437919400138,
    79.71785123400332,
    79.753897513001,
    79.62127686799795,
    79.46714345300279,
    79.58923065300041,
    79.53400899500411,
    0.0,
    79.35762588000216,
    79.45723469299992,
    0.0,
    79.31931288800115,
    79.43988254799478,
    0.0,
    0.0,
    79.77150976799749,
    0.0,
    0.0,
    79.49654189599823,
    79.5533813170041,
    79.54906014000153,
    79.44262392300152,
    79.42493282599753,
    79.26625429799606,
    0.0,
    0.0,
    0.0,
    79.22141028400074,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    78.97043846199813,
    0.0,
    79.11247967000236,
    79.0440088739997,
    79.00536591499986,
    79.241629999,
    0.0,
    79.07307480699819,
    79.16390004700224,
    79.14044561099581,
    79.11637846600206,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    79.0582939090018,
    0.0,
    78.90453057700506,
    0.0,
    0.0,
    0.0,
    78.69259818099817,
    78.86282767800003,
    78.78892625300068,
    78.6089335190045,
    0.0,
    0.0,
    78.52060308700311,
    78.48813256000722,
    78.52947349400347,
    78.496469341997,
    78.36616863199743,
    78.4058979620022,
    78.52748107299703,
    78.48450025499915,
    78.4046351750003,
    78.49522964999778,
    78.47458431900304,
    78.32815247399412,
    78.3074051260046,
    78.23190665200673,
    0.0,
    0.0,
    0.0,
    0.0,
    77.92880257499928,
    0.0,
    78.34730383600254,
    78.2186746189982,
    78.50853921300586,
    0.0,
    0.0,
    0.0,
    0.0,
    78.40517858399835,
    78.28312633499445,
    78.38758148499619,
    78.38469986600103,
    78.34563908500422,
    78.22828612600279,
    77.96495447000052,
    77.98045263499807,
    77.9308088809994,
    77.88868774000002,
    77.87604203500086,
    77.97888407100254,
    77.84355970200704,
    77.9033605279983,
    77.8754181990007,
    78.17799966099847,
    0.0,
    0.0,
    0.0,
    78.03683513600117,
    78.3514412250006,
    78.30408431500109,
    78.22402215300099,
    0.0,
    0.0,
    77.93275396899844,
    77.91537017299561,
    0.0,
    77.99855881099938,
    77.93887741999788,
    0.0,
    77.80601433199627,
    77.81592647499929,
    77.81097246699937,
    0.0,
    77.75105744400207,
    0.0,
    0.0,
    77.70163022100314,
    77.76439469199977,
    77.88001814600284,
    0.0,
    0.0,
    77.47172894800315,
    0.0,
    78.10750107700005,
    0.0,
    77.84134234499652,
    77.80274742800248,
    77.73580957900413,
    77.64127128600376,
    77.6853281549993,
    77.62640919000114,
    77.47537653199834,
    77.53815490199486,
    77.50344603299891,
    77.62103222199949,
    77.74108647600224,
    77.70714412000234,
    77.82986234499549,
    78.07292203500401,
    78.19063481599733,
    0.0,
    0.0,
    78.31231213200226,
    78.01666922099685,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    78.1891689020049,
    78.30641998899955,
    78.2063660010026,
    78.07255691700266,
    78.19260652199591,
    78.184244570999,
    78.29735653699754,
    0.0,
    0.0,
    78.18402077499923,
    0.0,
    78.04031821600074,
    77.98392214699561,
    78.21523519200127,
    0.0,
    0.0,
    78.0537654600048,
    78.02950014799717,
    77.99648727099702,
    0.0,
    0.0,
    0.0,
    0.0,
    77.64798022800096,
    77.68916036599694,
    77.75785167900176,
    77.68312136699387,
    77.6132965190045,
    77.57414021099976,
    0.0,
    0.0,
    0.0,
    77.46912673299812,
    77.44911300500098,
    0.0,
    0.0,
    0.0,
    77.05507057299837,
    77.01708734199929,
    76.97873497399996,
    76.95825123399845,
    76.92201228200429,
    76.7835931879963,
    0.0,
    77.96583604600164,
    0.0,
    78.82234728400363,
    78.85925156599842,
    78.78990050299763,
    78.7885007510049,
    78.77087801800371,
    78.89798492700356,
    78.97136322799633,
    78.64892603100452,
    78.69536094700015,
    78.64208244500333,
    78.62880667699937,
    78.74513033399853,
    78.7286542399961,
    78.66060503599874,
    78.7417164360013,
    78.9526691430001,
    78.90939350500412,
    0.0,
    79.0539701450034,
    78.98622040400369,
    78.9320255720013,
    79.02999422599532,
    79.08157117399969,
    79.05236614900059,
    0.0,
    0.0,
    0.0,
    79.10038154700305,
    0.0,
    0.0,
    78.90099292599916,
    78.91126686500502,
    78.90633291999984,
    78.89299938300246,
    78.89008012200065,
    78.99586168699898,
    78.95510177099641,
    78.7489117580044,
    78.5887814930029,
    78.67939707400365,
    78.66847433100338,
    78.58638983000128,
    78.50871052000002,
    78.76192168499983,
    78.55622774900257,
    78.55257107599755,
    78.52383649200056,
    78.53672716100118,
    78.46327331499924,
    78.44767966999643,
    78.63871159299742,
    78.54633403199841,
    78.5414543950028,
    78.6569475590004,
    78.64834268599952,
    78.62230731800082,
    78.70180786599667,
    78.69811875800224,
    78.63557690699963,
    78.59604017299716,
    0.0,
    79.03051271299773,
    79.28499364900199,
    79.0796194900031,
    79.02777332899859,
    0.0,
    79.60968764300196,
    79.50493636299507,
    79.41384429400205,
    79.3668267930043,
    79.47669847199722,
    0.0,
    79.9612299779983,
    80.06923223500053,
    80.13116317700042,
    80.06340466699476,
    80.13104042199848,
    80.1005940210016,
    80.03440060799767,
    80.297362987003,
    80.1827396890003,
    80.29284165100398,
    80.27017741299642,
    0.0,
    80.06198603799567,
    0.0,
    80.28891330599436,
    0.0,
    0.0,
    0.0,
    80.17581273499673,
    80.20256525500008,
    0.0,
    80.11050653200073,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    79.84623128499516,
    79.8393089070014,
    79.83878490399366,
    79.92475489600474,
    79.875755948,
    79.93684045499685,
    79.92792200999975,
    80.04059645200323,
    80.24934542299889,
    80.15336354100145,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    79.8672103429999,
    79.85731512899656,
    0.0,
    0.0,
    0.0,
    79.70795281000028,
    79.76443789600307,
    0.0,
    79.8239821419993,
    79.73523553900304,
    79.63905721100309,
    0.0,
    79.80435561299964,
    0.0,
    0.0,
    79.9783892839987,
    0.0,
    0.0,
    0.0,
    79.7114713330011,
    79.69717766699614,
    0.0,
    0.0,
    79.70860998599528,
    79.70553535300132,
    0.0,
    79.53424759199697,
    0.0,
    79.5784813670034,
    0.0,
    0.0,
    0.0,
    0.0,
    79.41217236099328,
    79.29771779199655,
    79.58910233699862,
    0.0,
    0.0,
    0.0,
    79.20382211899414,
    79.3668634209971,
    0.0,
    79.23088947100041,
    79.0674546980008,
    79.03505186000257,
    79.02169888899516,
    78.87169625199749,
    79.27423183200153,
    79.23360677500023,
    79.1396234710046,
    79.12122592599917,
    79.076932295,
    79.13630891899811,
    79.06233260500449,
    79.05098890699446,
    79.20358304500405,
    79.13802574099827,
    0.0,
    79.06748220900045,
    79.01473866900051,
    79.28991486199811,
    79.29020512999705,
    79.28336465799657,
    79.2781336650005,
    79.08811250100553,
    79.20191310100199,
    79.33725550900272,
    0.0,
    79.97562671700143,
    79.8814608419998,
    80.11862479199772,
    80.04818907899607,
    80.17216208800528,
    80.01600109399442,
    79.95623796399741,
    80.23859814699972,
    80.27782560000196,
    80.24155082799552,
    80.49795042100595,
    80.4696902220021,
    80.53186931400705,
    0.0,
    81.40989204100333,
    81.51745851300075,
    81.48449115399853,
    81.37517875999765,
    81.26935394400061,
    81.52765658599674,
    81.64036272399971,
    81.61004535499524,
    81.55874978299835,
    81.64692756900331,
    81.63014864300203,
    81.5827261250015,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    82.18610057600017,
    0.0,
    0.0,
    82.10104027000489,
    0.0,
    0.0,
    81.74776333000045,
    81.82492857300531,
    81.8022892440058,
    82.05681280199497,
    81.9520321700038,
    81.89220186899911,
    81.58056156199746,
    81.56914546599728,
    81.85370709400013,
    81.82882530500501,
    81.82168436600477,
    0.0,
    81.81801484899916,
    81.7428917379948,
    0.0,
    0.0,
    81.71352121399832,
    81.57298868699581,
    81.64642055800505,
    81.55771722100326,
    81.53358203200332,
    81.67527207899548,
    81.62807441799669,
    81.64005270900088,
    81.515449919003,
    81.46738970099977,
    81.45459690799908,
    81.5776243690052,
    0.0,
    81.75019463399803,
    81.73594253399642,
    81.99949255400134,
    82.25439862300118,
    82.09601285499957,
    82.0397918949966,
    82.23739378299797,
    82.33734555500268,
    82.29842577100499,
    82.25127477400383,
    82.17500221500086,
    82.13067343199509,
    0.0,
    0.0,
    0.0,
    82.01262234400201,
    81.90081612400536,
    81.82674754800246,
    81.91567721199681,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    81.71742008900037,
    0.0,
    0.0,
    0.0,
    81.54842687999917,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    81.30519266900228,
    81.12643884099816,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    80.54338727200229,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    79.94816609800182,
    80.11852951299807,
    0.0,
    0.0,
    80.06997157300066,
    0.0,
    0.0,
    80.03542072699929,
    79.92052564800542,
    79.85737111799972,
    80.11962260000291,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    79.92252868899959,
    0.0,
    79.74559643399698,
    79.85312889099441,
    0.0,
    79.71490754299884,
    0.0,
    0.0,
    79.57037956600107,
    79.41498312699696,
    79.41396036699734,
    79.15879518300062,
    79.06757002799714,
    78.97797611599526,
    78.99588740499894,
    78.96918843399908,
    78.92743431499548,
    0.0,
    0.0,
    78.80705883799965,
    78.75569630799873,
    78.84594696999557,
    78.77167918399937,
    78.8725287940033,
    0.0,
    0.0,
    0.0,
    0.0,
    78.73998462999589,
    78.73732510599802,
    0.0,
    0.0,
    78.59208067799773,
    78.52964973000053,
    78.52493681899796,
    78.81265411299682,
    78.70934217300237,
    78.7956009269983,
    78.73980156500329,
    78.82178949700028,
    0.0,
    78.50942412699806,
    78.49592660000053,
    78.4907495189982,
    78.51631553899642,
    78.51027850399987,
    78.43815120100044,
    78.53267852000135,
    78.65192464800202,
    0.0,
    0.0,
    0.0,
    78.44674425099947,
    0.0,
    0.0,
    78.27916164199996,
    78.25343255999906,
    78.22166338500392,
    78.68628135699691,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    78.60478800500277,
    78.58771762600372,
    0.0,
    0.0,
    0.0,
    0.0,
    78.44414374500047,
    78.33161379400553,
    0.0,
    0.0,
    78.2449063410022,
    78.2127554050021,
    77.94352280000021,
    78.05340941800387,
    0.0,
    78.8188381919972,
    78.77092440999695,
    78.76630365400342,
    78.71015002800414,
    79.00040622899542,
    78.99443326300388,
    78.97657214199717,
    78.78213556000264,
    78.63690910899459,
    78.528027406006,
    78.64533723799832,
    78.77325643099903,
    78.8428360580001,
    78.8017942890001,
    78.76379300199915,
    78.84341695500189,
    78.96635570000217,
    79.20425483399595,
    79.19565027800127,
    79.2418242279964,
    79.20006268200086,
    79.1692003959979,
    79.26010834400222,
    79.23596835899662,
    79.19182113599527,
    79.15984945499804,
    79.2539331959997,
    79.3370022320014,
    79.31595418199868,
    79.34644585200294,
    79.47594334300084,
    79.46445327800029,
    79.58573654499924,
    79.54297221199522,
    79.45809315400402,
    79.35430163300043,
    79.30558162099624,
    79.33402059799846,
    79.32239324299735,
    79.23050031599996,
    79.32829951299937,
    79.5461957890002,
    79.523861640002,
    79.50814193800034,
    79.76111030299944,
    80.01012280899886,
    0.0,
    80.69342065099772,
    80.7855477990015,
    80.74182003900205,
    80.73281509999651,
    80.7837514510029,
    80.66326188699895,
    80.77198912500171,
    0.0,
    81.31632351800363,
    81.03975921100209,
    81.1638339370038,
    81.21928872499848,
    81.35093606499868,
    81.32963767500041,
    81.45973331099958,
    81.72842461700202,
    81.68671192000329,
    81.61956687999918,
    81.73500506699929,
    81.72035571900051,
    81.70563621500332,
    81.58443902799627,
    81.88029486400046,
    81.78311239100003,
    81.84087007299968,
    81.82451818599657,
    81.6802194060001,
    81.64717961099814,
    81.52252105300431,
    81.63084189099754,
    81.62572671299858,
    81.60622694500489,
    81.51140809299977,
    81.63388379000389,
    81.46920751100697,
    81.59039428499818,
    81.55122725899855,
    81.62344314799702,
    0.0,
    82.74662303200603,
    82.64656906599703,
    82.64290892299323,
    82.77212390599743,
    82.66103008400387,
    82.77377550300298,
    82.64627883400681,
    82.53959740000573,
    82.460463935,
    0.0,
    82.80892152700108,
    82.91699289700045,
    83.03268773000309,
    83.01359978999972,
    83.15049301700492,
    83.25853107200237,
    83.34698911199666,
    0.0,
    83.32584816800227,
    83.24621652699716,
    83.30584155500401,
    83.4148971579998,
    0.0,
    83.70475095499569,
    83.64109571599693,
    83.71160359099304,
    83.83109736999904,
    83.82539911900676,
    83.93527026099764,
    0.0,
    84.48104257199884,
    84.40307269999903,
    84.34549028100446,
    84.53540651499497,
    84.40050046499528,
    84.21911005300353,
    83.94804268000007,
    83.92272783299995,
    83.9007752519974,
    84.20488494299934,
    84.31630425399635,
    84.43571692299884,
    84.35000305499852,
    84.42373669699737,
    84.41238491200056,
    84.37568028300302,
    84.61252144200262,
    84.5255489110059,
    84.59252515499975,
    84.83994907799934,
    84.8836716989972,
    84.7785921409959,
    84.85830250699655,
    84.80089619499631,
    84.85686241400253,
    84.82250777100126,
    85.09601239799667,
    0.0,
    0.0,
    85.03030040199519,
    85.20437805300026,
    85.15724146700086,
    0.0,
    0.0,
    0.0,
    0.0,
    84.93602462200215,
    84.89804765199369,
    84.85620223199658,
    85.09286025700567,
    85.05020506800065,
    85.1081148029989,
    85.07274916500319,
    85.04882164199807,
    85.36738329400396,
    85.30052616000467,
    85.41275059200416,
    85.29037995300314,
    85.1971201769993,
    0.0,
    85.05131796500064,
    85.13882925699727,
    85.23191773199505,
    85.1787127839998,
    85.16669759499928,
    85.11821299700387,
    84.96658600099909,
    84.94887009699596,
    84.87044912800047,
    84.96870757000579,
    85.20840713800135,
    85.08542805200705,
    85.04798249900341,
    85.02067454099597,
    85.08604917100456,
    0.0,
    84.94327107300342,
    84.94311220599775,
    0.0,
    0.0,
    0.0,
    84.90969033500005,
    84.79024891400331,
    84.67394889800198,
    84.78262840500247,
    85.01550623200455,
    85.33005218100152,
    85.31187017800403,
    0.0,
    0.0,
    0.0,
    85.07399190300202,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    84.94691768600023,
    85.05309078899882,
    0.0,
    85.05029960999673,
    84.91182456300157,
    0.0,
    84.76928970599693,
    84.70615055300004,
    84.67342387800454,
    84.66257560599479,
    84.63729741599673,
    84.72510248699837,
    84.66439258299943,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    84.357550945002,
    84.30231498499779,
    84.40543752600206,
    0.0,
    84.3074384940046,
    0.0,
    0.0,
    83.95721445199888,
    83.81720972299809,
    83.80557781500102,
    83.88253760500083,
    83.71774217500206,
    83.57229951299814,
    83.69039198999963,
    83.94183409199468,
    83.85675428200193,
    83.8412431350007,
    83.88326217100257,
    83.83865131699713,
    83.81156159600505,
    83.76553312700707,
    83.77893327299535,
    83.90519073599717,
    83.82106543899863,
    83.8112322389934,
    83.81714599800034,
    83.66642450099607,
    0.0,
    0.0,
    83.59255976400163,
    83.50406134899822,
    0.0,
    83.2982088370045,
    83.23244217600586,
    83.22348747800424,
    83.08216728700063,
    83.18383522399381,
    83.17086461700092,
    83.15134999300062,
    0.0,
    0.0,
    83.95146297500469,
    0.0,
    0.0,
    0.0,
    83.76911163499608,
    0.0,
    0.0,
    0.0,
    0.0,
    83.68926268300129,
    83.66475149700273,
    83.63626225999906,
    83.90344315900438,
    0.0,
    0.0,
    83.87057960800303,
    83.94380498100509,
    83.8936467819949,
    0.0,
    83.96024149200093,
    84.04441864100227,
    84.00173198199627,
    83.96658366200427,
    83.93581704499957,
    83.92726775199844,
    0.0,
    83.86813723899832,
    83.85621860800165,
    84.11129387199617,
    83.96919274800166,
    0.0,
    83.94258312300371,
    0.0,
    83.83516395200422,
    83.8219387470017,
    0.0,
    0.0,
    0.0,
    83.70619197800261,
    83.80915729599656,
    83.7911645780041,
    0.0,
    0.0,
    0.0,
    83.51898107099987,
    83.40827530899696,
    83.28684517800139,
    83.39371857399965,
    83.32553269200434,
    0.0,
    0.0,
    83.39242218899744,
    83.29904950399941,
    0.0,
    83.24615640399861,
    83.2130101790026,
    0.0,
    83.13216432400077,
    83.25297674499598,
    83.1601896889988,
    83.4021727319996,
    83.3721876799973,
    83.35488006599917,
    83.35336436799844,
    83.36383214200032,
    0.0,
    83.26906331499777,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    82.97415627299779,
    82.9655049330031,
    0.0,
    0.0,
    82.69952656999521,
    0.0,
    0.0,
    82.64242146600009,
    82.52508291300182,
    82.55374290900363,
    82.50472742899728,
    82.6000261329973,
    82.58793533399876,
    82.577017196003,
    82.5057753229994,
    0.0,
    82.40445595700294,
    82.31039184900146,
    82.42481552100071,
    82.39715910000086,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    82.23635711700626,
    82.15075763200002,
    0.0,
    82.11691452199739,
    82.19390733399632,
    0.0,
    82.06552144700254,
    82.01798425400193,
    0.0,
    81.92238375099987,
    0.0,
    81.96749689800345,
    81.87124649699399,
    82.03261045399995,
    0.0,
    81.92866399900231,
    0.0,
    0.0,
    81.7670608039989,
    0.0,
    0.0,
    81.82225867899979,
    81.94106300400017,
    81.79854636000528,
    81.78104179599904,
    81.81385798799602,
    0.0,
    81.7372930040001,
    0.0,
    81.64778457599459,
    81.6142189920065,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    81.42098643499776,
    81.27014817599411,
    81.28448168999603,
    81.38608315699821,
    81.30894048600021,
    81.26771830499638,
    81.25233689000015,
    81.33986380900024,
    81.07775481200224,
    81.2122430879972,
    81.24617848300113,
    81.0584759839985,
    81.32739366599708,
    81.18546353299462,
    81.24658031199942,
    81.08481888899405,
    81.13632373300061,
    81.40797487399686,
    81.37705461800215,
    81.3554664359981,
    81.19737465999788,
    81.15871004200017,
    81.27832784299972,
    81.27285872499488,
    81.35820802200033,
    81.30007808999653,
    81.27775030900375,
    81.2132023469967,
    81.34563578200323,
    81.59157954499824,
    81.5661104949977,
    81.68084042400005,
    81.61268818299868,
    81.42878415500309,
    81.17261415999383,
    81.25954346299841,
    81.13910731699434,
    81.02801745799661,
    81.2679979329987,
    81.38279385499482,
    81.28955734399642,
    81.5769294519996,
    81.41646251000202,
    81.64187535399833,
    81.51819528800115,
    81.62207106000278,
    81.58036263800022,
    81.57279233100417,
    81.5302014849949,
    81.45558469399839,
    81.4093995200019,
    81.53875684300147,
    81.53555359800521,
    81.52805810999416,
    81.78580559799593,
    82.01868701999774,
    82.01253812500363,
    81.9914157250023,
    82.1000240870053,
    0.0,
    0.0,
    0.0,
    0.0,
    81.77068868099741,
    81.69467459699808,
    81.74009129600017,
    81.68272830400383,
    81.76732340099989,
    81.87294997300341,
    81.98632409299898,
    82.03644491900195,
    0.0,
    81.99185617500189,
    0.0,
    0.0,
    0.0,
    0.0,
    81.77620859000308,
    0.0,
    81.64538460400217,
    81.49831994499982,
    81.24470252799802,
    81.42532996800583,
    81.54792091099807,
    0.0,
    0.0,
    0.0,
    0.0,
    81.50816307900095,
    81.453465082006,
    81.36918254299962,
    81.32805616500264,
    81.45500143500249,
    81.45217308400606,
    81.56017292099568,
    81.55334025900083,
    81.57132266500412,
    81.50441734400374,
    81.42234859200107,
    0.0,
    0.0,
    0.0,
    81.2121318569989,
    81.15487678900536,
    81.09744998899987,
    81.20681398799934,
    81.32916817299702,
    81.31206354199821,
    81.38318733400229,
    0.0,
    81.76440293400083,
    81.84638752600586,
    0.0,
    81.90421552200132,
    0.0,
    0.0,
    0.0,
    0.0,
    82.00893038600043,
    0.0,
    0.0,
    0.0,
    81.8486930930012,
    81.8299204520008,
    81.92845722899801,
    82.22022842300066,
    82.31300585099962,
    82.26727518400003,
    0.0,
    82.22610954499396,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    81.70808694600419,
    0.0,
    0.0,
    81.44943170600163,
    81.51584965099755,
    81.30801272999815,
    81.435669616003,
    81.40479742400203,
    81.66386716700072,
    0.0,
    81.60116781100078,
    0.0,
    0.0,
    81.58027456899435,
    81.51413431199762,
    81.37036591799551,
    81.46696666900243,
    81.39386430000013,
    81.30276732800121,
    0.0,
    81.98372887900041,
    81.95753933499509,
    0.0,
    81.95718160099932,
    82.09029377999832,
    82.02214699500473,
    81.98278014499374,
    0.0,
    0.0,
    0.0,
    82.01489448799839,
    82.00936803100194,
    81.83153749799385,
    81.7550774170013,
    81.72739463299513,
    81.90854751299776,
    81.88340329000494,
    82.18633369300369,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    82.00417119699705,
    0.0,
    0.0,
    82.66823182799999,
    82.64617064400227,
    0.0,
    82.3650535409979,
    82.31317376899824,
    82.29688244400313,
    0.0,
    82.27318057600496,
    82.222107992995,
    82.20242639999924,
    0.0,
    82.14646201600408,
    0.0,
    0.0,
    81.95483609799703,
    81.8213466849993,
    81.72046923000016,
    81.63378665300115,
    81.71582905700052,
    81.62346825699933,
    0.0,
    81.82800489499641,
    0.0,
    0.0,
    81.60692410900083,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    81.42197729799955,
    0.0,
    0.0,
    0.0,
    0.0,
    81.30719141800364,
    0.0,
    0.0,
    81.14543582599435,
    0.0,
    80.9849169290028,
    81.20343336500082,
    0.0,
    0.0,
    80.8007704699994,
    80.74830270699749,
    80.85422785000264,
    80.71929706599622,
    80.58847423500265,
    80.34032677699724,
    80.29401976599911,
    80.23021778600378,
    80.1945448109982,
    80.11354843599838,
    80.09019365099812,
    80.01509660800366,
    80.0392372149945,
    0.0,
    80.3865819700004,
    80.37471695600107,
    80.35768202100007,
    80.42807452799752,
    80.39327646700258,
    80.27466182100034,
    79.98544497400144,
    79.98329605400068,
    79.95707994799886,
    80.04250726899772,
    80.02361591299996,
    0.0,
    0.0,
    79.8182632600001,
    79.86936923600297,
    79.85669038999913,
    0.0,
    0.0,
    79.88030518600135,
    0.0,
    79.81464711000444,
    79.7225932950023,
    79.79363402400486,
    0.0,
    0.0,
    0.0,
    0.0,
    79.73223453399987,
    79.63190614699852,
    79.65414770099596,
    79.6813362430039,
    79.94982025599893,
    79.94098654800473,
    79.92790611199598,
    80.04141681100009,
    80.12078574099723,
    80.03577236799902,
    80.12618145199667,
    80.32342356200388,
    80.44275490600558,
    80.70853595000517,
    0.0,
    0.0,
    80.47005094800261,
    80.53098598900397,
    0.0,
    0.0,
    80.66918034599803,
    80.57959404199937,
    80.56372520899458,
    80.64706723899872,
    0.0,
    80.84123374000046,
    80.96012389899988,
    0.0,
    0.0,
    80.73691795900231,
    80.85023356400052,
    80.84698952799954,
    80.74263616500684,
    80.68967310500011,
    80.86430837499938,
    0.0,
    0.0,
    0.0,
    0.0,
    80.55788524499803,
    80.8143582959965,
    80.78153950300475,
    80.74873168900376,
    81.04632862400467,
    81.0440323139992,
    80.9418820380015,
    81.2034021080035,
    81.4674925750005,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    81.20900494800298,
    0.0,
    81.1372486209948,
    81.19855294700392,
    0.0,
    81.03090538100514,
    81.39035465500638,
    81.66633779500262,
    81.66245718400023,
    81.7971403479969,
    82.05336081900168,
    81.9618931820005,
    81.73662107500422,
    81.67797880600119,
    81.6117468299999,
    81.70150403600564,
    81.79025481799908,
    81.74952528600261,
    81.70001680900168,
    81.81062892499904,
    81.79772119299741,
    82.14719259599951,
    82.14133015699917,
    81.96709561400348,
    0.0,
    0.0,
    0.0,
    0.0,
    81.7305994950002,
    81.76441532299941,
    81.75852915400174,
    81.9708332759983,
    0.0,
    82.38402117199439,
    82.60763815999962,
    82.59538114199677,
    82.56678243000351,
    82.56280401800177,
    0.0,
    0.0,
    0.0,
    82.39552745399851,
    82.48663108400069,
    82.45407587799855,
    82.33892476299661,
    82.3307769009989,
    82.30532585299807,
    82.16884524800116,
    82.25290048700117,
    82.16644902899861,
    0.0,
    82.56308537699806,
    82.40210209700308,
    82.39656170100352,
    82.47300035900116,
    82.62532666500192,
    82.64619669199601,
    82.57326487299724,
    82.57176598100341,
    82.55004564700357,
    82.4674888399968,
    82.44697552700381,
    82.41507280199585,
    0.0,
    0.0,
    82.89004557800217,
    82.80584609600191,
    82.68043857200246,
    82.67933569199522,
    82.79706944800273,
    82.78558476000035,
    82.70269604100031,
    82.71913759499876,
    82.70319571399887,
    0.0,
    82.92707356800383,
    82.85470205399906,
    82.95604365399777,
    83.04656537600385,
    82.97984719600208,
    82.96996035300253,
    83.06394107699452,
    83.00268353000138,
    82.94139941000321,
    83.12917432900576,
    0.0,
    83.99961596000503,
    83.988558912999,
    84.09961240099801,
    84.22334622700146,
    84.21233636699617,
    84.6208660869961,
    84.54945565199887,
    84.39337291500124,
    84.12187500500295,
    84.06281410600059,
    84.11537763999513,
    84.29292607800016,
    84.26456706199679,
    84.16540210400126,
    84.12830530000065,
    84.0508489529966,
    83.91353466900182,
    83.94480692200159,
    0.0,
    85.17781587799982,
    85.27025999499892,
    85.32796511999913,
    85.30299915100477,
    85.40682308500254,
    85.4884786879993,
    85.60649818399543,
    85.49307930799841,
    0.0,
    85.5306743139954,
    85.55413344100089,
    85.48678461500094,
    0.0,
    85.56899131200043,
    85.38709785899846,
    85.41948932699597,
    85.40831691000494,
    0.0,
    0.0,
    85.3232133440033,
    85.10714885200287,
    85.09706745800213,
    84.85607649200392,
    84.95897899600095,
    84.87903969499894,
    84.86846777999745,
    84.85285775300144,
    84.83041368499835,
    84.798983527995,
    84.79238250100025,
    84.71133776700299,
    0.0,
    84.5753006360028,
    84.95238282599894,
    85.02638763000141,
    84.98505556499731,
    0.0,
    0.0,
    84.88943865500187,
    84.8499782620056,
    84.81306327299535,
    84.79853721900145,
    84.86761864199798,
    84.77512102200126,
    0.0,
    85.3734031549975,
    85.39301635800075,
    85.37828705900029,
    85.45362891600234,
    0.0,
    0.0,
    0.0,
    85.36464953499672,
    85.45140492099745,
    85.56457972499629,
    85.35168669599807,
    85.32893400199828,
    85.3215049309947,
    85.26358945699758,
    85.30625132100249,
    85.44185023699538,
    85.3881066449976,
    85.46910788700188,
    85.42790201900061,
    85.55988278600125,
    85.53326520800329,
    85.50155484700372,
    85.43167092899967,
    85.57997707000322,
    85.42979254399688,
    85.36860836399865,
    85.4073655500033,
    0.0,
    86.21640997099894,
    86.29591935699864,
    86.29708056500385,
    86.19221349399595,
    86.32707751000271,
    86.30467609100015,
    0.0,
    86.3529529830048,
    0.0,
    0.0,
    0.0,
    86.1381059790001,
    86.11305926299974,
    86.11049137100053,
    86.10684415700234,
    0.0,
    0.0,
    0.0,
    85.90869709099934,
    0.0,
    0.0,
    85.74701852699945,
    0.0,
    0.0,
    0.0,
    85.40302760300256,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    85.15348462900147,
    85.19053435399837,
    85.15785339600552,
    85.10204174699902,
    85.1761744270043,
    0.0,
    0.0,
    85.3905195460029,
    85.68302113099344,
    85.50662367900077,
    0.0,
    0.0,
    0.0,
    86.6367382410026,
    86.6187703359974,
    0.0,
    86.57109851999849,
    0.0,
    87.7480751510011,
    0.0,
    88.1502823089977,
    88.25011686200014,
    88.45576394499949,
    88.39311548499973,
    88.47362666799745,
    88.4665610919983,
    88.46082802699675,
    0.0,
    0.0,
    88.50945278700237,
    0.0,
    88.40682363200176,
    0.0,
    0.0,
    88.26687587000197,
    88.24119579400576,
    88.41342726899893,
    88.38444045300275,
    88.53993228200125,
    88.48882280600083,
    88.74819900900184,
    88.71681828700093,
    88.83824683900457,
    88.81499424799404,
    88.7379855850013,
    88.71774532600102,
    0.0,
    88.62586796300457,
    88.86221859000216,
    88.72601460999431,
    88.59420847099682,
    88.5662644480035,
    88.6354499500012,
    88.57414635700115,
    88.6885802090037,
    88.59951976800221,
    88.54717598700518,
    88.60585235300096,
    0.0,
    0.0,
    0.0,
    0.0,
    88.58513048000168,
    88.58309090600233,
    88.68736068099679,
    88.67300893299398,
    0.0,
    88.72657077199983,
    0.0,
    0.0,
    0.0,
    0.0,
    88.6683958930007,
    0.0,
    0.0,
    88.49607358200592,
    0.0,
    88.2824742619996,
    0.0,
    0.0,
    88.156760535996,
    88.25067632099672,
    88.23320531100035,
    88.15011129999766,
    88.2859012639965,
    88.19822838600521,
    0.0,
    0.0,
    88.19646155100054,
    88.14122289600346,
    88.04469021799741,
    87.94448017299874,
    87.94930688399472,
    88.18384772300487,
    88.14806820499507,
    0.0,
    87.93747989599797,
    87.91269474400178,
    87.89762649299519,
    0.0,
    88.39265344299929,
    88.46563955300371,
    88.30854430399631,
    88.24230358299974,
    88.22563647900097,
    88.13164953199885,
    88.08559527499892,
    0.0,
    88.4722329599972,
    88.53687135899963,
    88.6363353150009,
    88.61308607200044,
    88.60360221600422,
    88.68708117400092,
    88.67368821299897,
    88.70598530200368,
    88.57736695099447,
    88.71973135200096,
    88.67253936099587,
    0.0,
    88.73533774600219,
    88.70803718499519,
    0.0,
    0.0,
    0.0,
    88.52619579600287,
    88.6446972660051,
    88.60694402099762,
    88.5806979150002,
    88.69860264399904,
    88.58777559299779,
    0.0,
    88.62971174499398,
    88.55638352900132,
    0.0,
    0.0,
    88.67347706599685,
    88.63073962299677,
    88.68547206799849,
    88.7179738900013,
    88.69298291199812,
    88.68054345299606,
    88.78874445799738,
    88.71490589500172,
    88.67495147800219,
    0.0,
    88.73319396399893,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    88.24179202399682,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    87.9996383530015,
    87.82936542500101,
    87.81056780800282,
    87.92363064399979,
    87.7846119809983,
    87.7850884569998,
    87.7569820380013,
    87.7686807620048,
    0.0,
    88.22578771199915,
    88.18475967700215,
    0.0,
    88.71210562799388,
    88.64624499100319,
    88.5563831019972,
    88.58096149099583,
    88.54707678200066,
    88.53344257099525,
    88.52543349400366,
    88.51531049299956,
    88.61306504700042,
    88.60155780299829,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    88.89802274000249,
    0.0,
    0.0,
    88.87651643400022,
    88.9546124740009,
    88.60822657699464,
    88.42239419999532,
    88.55051215899584,
    88.54896998799813,
    88.38356087799912,
    88.46554219999962,
    0.0,
    0.0,
    88.42683632699482,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    88.27725367999665,
    88.26073904600344,
    88.17954143600218,
    0.0,
    0.0,
    88.08978178699908,
    0.0,
    0.0,
    0.0,
    0.0,
    87.92112757499854,
    88.02942222499405,
    0.0,
    88.11352503300441,
    87.96316198600107,
    88.2065520210017,
    88.17420092900284,
    88.08877691800444,
    88.14840805900167,
    0.0,
    0.0,
    0.0,
    88.07704348900006,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    87.66249360599613,
    87.42470368799695,
    87.42094305399951,
    87.39352458199573,
    87.24311207299615,
    0.0,
    87.37044709099428,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    86.94261107499915,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    86.83982832799666,
    86.77060566400178,
    0.0,
    86.85848973599786,
    0.0,
    86.8366321700014,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    86.55413978200522,
    86.41736336799659,
    86.44017260700639,
    86.37792983800318,
    86.41449421099969,
    86.35038588000316,
    86.45897484099987,
    86.455694545999,
    0.0,
    0.0,
    0.0,
    86.3709054639985,
    86.30029027199635,
    86.39395758000319,
    86.50933102199633,
    0.0,
    0.0,
    0.0,
    0.0,
    86.32626592399902,
    86.27451676700002,
    0.0,
    86.16584444199543,
    86.15756655299629,
    0.0,
    0.0,
    86.09071645199583,
    86.07846956499998,
    0.0,
    0.0,
    0.0,
    0.0,
    85.99257590999332,
    86.1080094400022,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    85.87142412600224,
    0.0,
    85.83565269799874,
    86.10185755300336,
    0.0,
    0.0,
    86.4544003129995,
    86.42436045699287,
    86.43520190400159,
    86.68012200100202,
    86.92352977200062,
    86.99331913700007,
    86.9697322350039,
    87.00208491599915,
    86.9121894580021,
    86.86800444599794,
    86.84916976100067,
    86.78208847800124,
    86.75070788500307,
    86.80584654000268,
    86.72990559099708,
    86.51851296400127,
    86.47161649899499,
    86.52487658300379,
    86.65714435600239,
    86.60120117000042,
    0.0,
    0.0,
    0.0,
    0.0,
    86.52901882000151,
    86.45939147200261,
    86.35634086700156,
    86.76346566499706,
    0.0,
    0.0,
    86.9988479349995,
    86.99414463799621,
    0.0,
    0.0,
    86.85902191900095,
    0.0,
    87.42251430200122,
    87.54404233399691,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    87.31220258999383,
    87.26061623400165,
    87.44438549900224,
    87.35312444900046,
    87.55838991199562,
    87.46658777599805,
    87.75426674200571,
    87.87194867799553,
    87.80020204099856,
    87.47111249900627,
    87.42841796599532,
    87.5271792259955,
    87.51431708900054,
    87.4096652119988,
    87.38941893499577,
    87.39345746899926,
    87.36788997700205,
    87.31642880800064,
    87.34716124900297,
    87.40580734600371,
    87.18186765100108,
    87.16735589600285,
    87.154972645003,
    87.40157046500099,
    0.0,
    87.76609286700113,
    87.73575164000067,
    87.84361959499802,
    87.76174274899677,
    87.75985546300217,
    87.61315147999994,
    87.58260902199982,
    87.42266191900126,
    87.41793835900171,
    87.3535068170022,
    87.43872274900059,
    87.57097862800583,
    87.47743131600146,
    87.59945541199704,
    87.49508718399738,
    87.55799133500113,
    87.55409523999697,
    87.9661846490053,
    88.10146162700403,
    88.26671182399878,
    88.20900117899873,
    88.19990475099621,
    0.0,
    0.0,
    88.10213442299573,
    88.10695644700172,
    88.07511401500233,
    0.0,
    87.95516995300568,
    0.0,
    0.0,
    0.0,
    87.74079781299952,
    87.64099653599988,
    87.47357337599533,
    87.55767535400082,
    87.45801192000363,
    0.0,
    0.0,
    0.0,
    0.0,
    87.19334876700304,
    87.16622229100176,
    87.17470599200169,
    87.3096853149982,
    87.42425317199377,
    87.42373857000348,
    87.37180163300218,
    0.0,
    88.28948228000081,
    88.18019781600015,
    88.11021662099665,
    88.22255977800523,
    88.14550219699595,
    88.11470888999611,
    0.0,
    0.0,
    87.9502738730007,
    87.99716181999975,
    87.96884426500037,
    0.0,
    0.0,
    87.84792641500098,
    0.0,
    87.785981230998,
    87.76213213000301,
    87.73660278600437,
    0.0,
    87.6277778030053,
    87.55008447499858,
    0.0,
    87.44764195499738,
    87.57102187500277,
    0.0,
    87.45149879300152,
    0.0,
    0.0,
    0.0,
    87.24202355099987,
    87.33574218400463,
    87.15898986799584,
    87.41753974399762,
    87.45551255299506,
    87.54685487799725,
    87.54380452199985,
    87.44649916100025,
    0.0,
    0.0,
    87.1912139470005,
    87.18835273500008,
    87.06950211399817,
    87.16698964899842,
    0.0,
    0.0,
    86.77241726899956,
    86.75731195299886,
    86.7337043209991,
    86.78040736900584,
    86.76593427999615,
    86.63333711199812,
    0.0,
    0.0,
    86.36234849599714,
    86.34414260299673,
    86.26354199100024,
    86.38251009699889,
    0.0,
    0.0,
    0.0,
    86.21793573499599,
    86.02326267399621,
    86.12345914499747,
    86.12183959299728,
    0.0,
    87.2540787830003,
    87.25410039899725,
    87.21490348899533,
    0.0,
    0.0,
    0.0,
    0.0,
    86.90822582100373,
    86.78569394400256,
    86.75988002400118,
    86.75757832999807,
    0.0,
    0.0,
    0.0,
    86.39945288000308,
    86.27444129800278,
    86.24785168499511,
    86.21898296700238,
    85.99530912400223,
    85.96246106000035,
    85.80854064199957,
    0.0,
    86.25211883099837,
    86.23532822400011,
    86.43510508000327,
    86.41408344400406,
    86.5094033589994,
    86.48311378699873,
    86.43843963499967,
    86.50285344900476,
    86.31189741799608,
    86.24897438399785,
    0.0,
    86.77773456999421,
    86.74718908500654,
    86.69878038000024,
    86.62896991400339,
    86.53898484700039,
    86.62961714600533,
    86.61613923500408,
    86.57721607900021,
    86.57641580199561,
    0.0,
    86.38705148699955,
    0.0,
    86.24128106799617,
    86.15942190199712,
    86.15837048400135,
    86.13718094200158,
    86.14620979200117,
    0.0,
    86.52771470999869,
    86.45789518200036,
    86.39733164900099,
    0.0,
    0.0,
    86.42173049700068,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    85.95829795199825,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    85.79620507200161,
    0.0,
    0.0,
    0.0,
    85.57953334999911,
    0.0,
    0.0,
    0.0,
    0.0,
    85.38943388599728,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    85.10021960200538,
    0.0,
    0.0,
    84.92703243099822,
    84.98579679300019,
    85.04386404300021,
    0.0,
    0.0,
    85.26519150600507,
    85.23744982999779,
    85.2288045039968,
    85.18550163699547,
    0.0,
    0.0,
    0.0,
    85.00761841500207,
    0.0,
    84.89344380099647,
    0.0,
    0.0,
    0.0,
    84.89842542699625,
    84.96449765899888,
    84.66539134699997,
    84.6593928739967,
    84.65346934399713,
    84.57648249599879,
    84.64405057600379,
    0.0,
    0.0,
    0.0,
    0.0,
    84.29923234099988,
    84.2965258280019,
    84.41691776399966,
    84.69919844999822,
    0.0,
    0.0,
    0.0,
    84.62419679400045,
    84.6889322470015,
    0.0,
    84.60496101299941,
    0.0,
    0.0,
    0.0,
    84.44597365299705,
    84.43877121600235,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    84.18284889200004,
    84.63168696899811,
    0.0,
    0.0,
    0.0,
    0.0,
    84.67502019400126,
    0.0,
    0.0,
    0.0,
    0.0,
    84.49427637199551,
    84.52110022700072,
    84.61850333599432,
    84.46802132800076,
    84.5511452330029,
    84.47614501399948,
    84.5135840160001,
    84.90123928099638,
    84.77977228599775,
    84.72149410400016,
    84.80602896900382,
    84.74489487699611,
    84.72059155400348,
    84.86945017499966,
    84.78820645499945,
    84.87053325900342,
    84.71643193199998,
    84.83419540400064,
    84.66596200100321,
    84.80132167299598,
    84.87398009599565,
    85.1039388499994,
    85.04704824600049,
    84.92463644600502,
    84.8925518229953,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    84.48691075600073,
    84.59907764100353,
    84.71462185800192,
    84.6807075619945,
    84.78848636799376,
    84.85093319300358,
    84.75892267599556,
    84.46026240000356,
    84.45660090399906,
    84.7048798600008,
    84.64904844299599,
    84.64785446500173,
    84.70825531899754,
    0.0,
    85.2879762809971,
    85.34086848099832,
    85.46941779700137,
    85.43552094200277,
    85.8205289940015,
    85.6882841159968,
    85.64821252399997,
    85.64770120700268,
    85.55667771399749,
    85.5445206430013,
    85.37858205399971,
    85.23355374600214,
    85.1323336389978,
    85.2641081529946,
    85.36245634600346,
    85.28826126299828,
    85.09653226599767,
    84.967206841,
    84.94094664599834,
    85.01804596099828,
    84.97762816799514,
    85.00109501700354,
    84.90471919400443,
    84.88834814899747,
    84.97710130899941,
    84.89992970399908,
    84.81998908799869,
    84.76397784199798,
    0.0,
    85.6281069819961,
    85.3748945949992,
    85.49607218400342,
    85.5299520510016,
    85.61341279000044,
    85.55238764100068,
    85.46039703499991,
    85.43915500499861,
    85.43308414600324,
    0.0,
    85.74228736600344,
    85.85858641699451,
    86.13420064099773,
    86.0522554679992,
    85.91528232800192,
    86.03546556100628,
    85.97023357500439,
    85.93951058000675,
    85.87762142599968,
    86.14304175899451,
    86.05763519500033,
    85.85543089500425,
    85.69109481699707,
    85.68555606799782,
    85.95130266000342,
    85.99642942300125,
    86.07645653600048,
    86.0474303919982,
    86.02425202700397,
    86.00723803100118,
    85.92800769400492,
    85.97570783700212,
    86.07988638000097,
    86.03442139199615,
    85.97348094399786,
    86.24678693400347,
    86.34824745099468,
    86.32285333899927,
    86.20376576000126,
    86.02998056700017,
    85.9713053710002,
    85.81902930799697,
    85.77677731700533,
    85.67868915599684,
    85.8595864730014,
    85.83151131799968,
    85.76652512599685,
    85.72884144500131,
    85.80555858099979,
    85.72700213400094,
    85.59644009399926,
    85.48654891300248,
    85.55572806800046,
    85.67312202999892,
    85.62007886499487,
    85.60947374100215,
    85.67961029599974,
    85.94027584599826,
    85.70678031599527,
    85.65870667000127,
    85.56228201900376,
    85.49694082199858,
    85.47915622800065,
    85.72858534999978,
    85.64299594900513,
    85.44685414500418,
    85.07479467499797,
    85.00500288500189,
    84.88490440700116,
    84.79106039800536,
    84.9074183019984,
    84.85615531300573,
    84.935794928002,
    85.05459752499883,
    85.03746053099894,
    85.01949384299951,
    85.29333060899808,
    85.19713264200254,
    85.31565909200435,
    85.2711857309987,
    85.09423575500114,
    85.08214980300545,
    85.30622195599426,
    85.56583708599646,
    85.52982183799759,
    85.64283663000242,
    85.56587085499632,
    85.5301835059945,
    85.45360914299818,
    85.34865874300158,
    85.45150514400302,
    85.67732650899416,
    85.62835664799786,
    85.60734725800285,
    85.58761843799584,
    85.57636688600178,
    85.56018147600116,
    85.65015564100031,
    85.48269684000115,
    85.33932051100419,
    85.46870741499879,
    85.42414981800539,
    85.66444457200123,
    85.60000411899819,
    85.7035712440047,
    85.80658899099944,
    85.73349720900296,
    85.68546236400289,
    85.62663522900402,
    85.6220906660019,
    85.81069067500357,
    85.89453080300154,
    85.73865097799717,
    86.15643153600104,
    86.14985981899372,
    85.88011327300046,
    85.73626536099619,
    85.6747901810013,
    0.0,
    86.24948653599859,
    86.29193609299546,
    86.588260894001,
    86.57500974400318,
    86.68775010699756,
    86.64093937400321,
    86.53820819500106,
    86.541344225996,
    86.53014088299824,
    86.57945104600367,
    86.46358200600662,
    86.59296196700598,
    86.6270128239994,
    86.57807884800422,
    86.52595813899825,
    86.51082498599862,
    86.44476837399998,
    86.4157784009949,
    86.67169549399841,
    86.54686925699934,
    86.84412989900011,
    86.96445650099486,
    87.08855883999786,
    87.07959144400229,
    87.00142054899334,
    87.04872792399692,
    87.29142882399901,
    87.27605043600488,
    87.22137805500097,
    87.20577245500317,
    87.31712632000563,
    87.26260469100089,
    87.15514491099748,
    87.34710666300089,
    87.33578347200091,
    87.59114615799626,
    87.56232540400379,
    87.48427624900069,
    87.41202662300202,
    87.3746874569988,
    87.37679555200157,
    87.22988127799908,
    87.36415514599503,
    87.31089778400201,
    87.48606737899536,
    87.48536152699671,
    87.46446773500065,
    87.42880228199647,
    87.41647123599978,
    87.41650329300319,
    87.52237648600567,
    87.47852863999287,
    87.44511717899877,
    87.28747784999723,
    87.4424811630015,
    87.59649820900086,
    87.50690323500021,
    87.41621155500616,
    87.21961689399905,
    87.15837637100049,
    87.27787670800171,
    87.27187820900144,
    87.2609223899999,
    87.25823213299736,
    87.22334436499659,
    87.32750867900177,
    87.3080265239987,
    87.50450002100115,
    87.28073431899975,
    87.27141916999972,
    87.318724646997,
    87.19763499299734,
    87.18235494599503,
    87.14071438399696,
    87.17348731399397,
    87.12178606299858,
    87.046288921003,
    87.16128087199468,
    87.1408026249992,
    87.19574565800576,
    87.18879649999872,
    87.28387676899729,
    87.40864252699976,
    87.32885834400076,
    87.32126252700255,
    87.27599943699897,
    87.6507416989989,
    87.49387725300039,
    87.34725227300078,
    87.03627201500058,
    0.0,
    88.12779511899862,
    88.03201139000157,
    88.12188598400098,
    88.08247423400462,
    88.0272009620021,
    88.1165716750038,
    87.90255212200282,
    88.01955889200326,
    88.00265843200032,
    87.97336008000275,
    87.9632994190033,
    88.07443395399605,
    87.95500704100414,
    87.92896637699596,
    87.89714361500228,
    88.15669422200153,
    88.15306128499651,
    88.13287967600627,
    88.12801662999846,
    88.2891995050013,
    88.31872268400184,
    88.37477874300384,
    88.36795939800504,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    87.9786422939942,
    0.0,
    0.0,
    0.0,
    87.8693391590059,
    0.0,
    0.0,
    87.88717771300435,
    87.98150705899752,
    0.0,
    87.80887163100124,
    0.0,
    0.0,
    87.82595988400135,
    87.92080544400233,
    87.90290775900212,
    87.85080195900082,
    88.12125737199676,
    88.31714633299998,
    88.13300954699662,
    88.12534209500154,
    87.98036838099506,
    87.93619401999604,
    87.53063933299563,
    87.5255300899953,
    87.58281024299504,
    87.54319668300013,
    87.53956825799833,
    87.76089216800028,
    87.86616598199907,
    87.97011602200655,
    88.0836132320037,
    88.07393026099453,
    0.0,
    0.0,
    0.0,
    87.92820370900154,
    87.88403587599896,
    87.9256914630023,
    0.0,
    88.37179379299778,
    88.48165687499568,
    88.5898889789969,
    0.0,
    88.4743258909948,
    88.37462853799661,
    88.37008806799713,
    88.49073645900353,
    88.48253875299997,
    88.43392136700277,
    88.41294533100154,
    88.35680402800062,
    88.43290184799844,
    88.41327820099832,
    88.44744255799742,
    88.37445150900021,
    0.0,
    88.33006108200061,
    88.42730639700312,
    0.0,
    88.47973884100065,
    88.68341158700059,
    0.0,
    88.63545163199888,
    88.77401359500072,
    88.40629778499715,
    88.42944523999904,
    88.32568736800022,
    88.40696563700476,
    88.31635632699908,
    88.18317390399898,
    0.0,
    0.0,
    0.0,
    87.96903469400422,
    0.0,
    0.0,
    0.0,
    87.76120693900157,
    87.69686342599744,
    87.74657230400044,
    0.0,
    0.0,
    0.0,
    0.0,
    87.58035586800543,
    87.48941424299846,
    87.43029688800016,
    0.0,
    0.0,
    87.34014596600173,
    0.0,
    87.33347491599852,
    87.32021389600413,
    0.0,
    0.0,
    0.0,
    87.04244786100026,
    87.32421338000131,
    87.37329104200035,
    87.61588576599752,
    0.0,
    0.0,
    87.466948493995,
    87.74014225599967,
    0.0,
    87.61899088200153,
    87.60791587700078,
    87.66195077599696,
    87.65286367200315,
    87.64650632700068,
    87.54567122900335,
    87.58022382100171,
    87.52064666499791,
    87.26766843900259,
    0.0,
    88.06638101200224,
    88.1026718830035,
    88.10033283499797,
    0.0,
    0.0,
    88.00395528499939,
    87.85540539200156,
    0.0,
    0.0,
    87.8558648649996,
    87.8541094559987,
    87.80759246699745,
    87.87907802600239,
    0.0,
    87.53139124700101,
    0.0,
    87.93463434699515,
    88.00231107100262,
    87.8612510699968,
    87.84927887600497,
    87.85222748099477,
    87.81323145799979,
    87.90223269200214,
    87.80837848699593,
    87.92022767900198,
    87.9049055430005,
    88.1942266929982,
    88.04557247200137,
    88.1437029430017,
    88.23683404300391,
    88.23553196799912,
    88.21423309599777,
    88.05064219999622,
    87.9920623959988,
    87.97200179700303,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    87.41268914900138,
    87.29069986699324,
    87.27756484899874,
    0.0,
    0.0,
    87.23502842600283,
    87.01149446899944,
    87.1044080119973,
    87.0507614770031,
    87.0476786329964,
    87.05163524600357,
    86.82718508799735,
    86.8212142409975,
    86.98276388100203,
    86.90503871400142,
    86.8997627830031,
    86.85783743700449,
    86.76994659900083,
    86.84240062900062,
    86.87364003699622,
    86.93306233100157,
    0.0,
    86.829479643995,
    86.79128979300003,
    0.0,
    0.0,
    0.0,
    0.0,
    86.35630043599667,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    85.86729354600538,
    0.0,
    85.72929883899633,
    85.66846265800268,
    85.7759503000052,
    86.04368163600157,
    0.0,
    0.0,
    85.92523828499543,
    85.92073320300551,
    0.0,
    85.76102621099562,
    85.88553952100483,
    0.0,
    0.0,
    85.67729577600403,
    85.65695824699651,
    85.6066452390005,
    85.56035122100002,
    85.67829328499647,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    85.52689866999572,
    85.57384553600423,
    85.54484806200344,
    0.0,
    0.0,
    85.50520357499772,
    85.64492071799759,
    0.0,
    85.69762124300178,
    0.0,
    0.0,
    85.4997043770054,
    0.0,
    0.0,
    85.32312536899553,
    85.19406705199799,
    0.0,
    0.0,
    85.39730891599902,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    85.13306220000231,
    0.0,
    0.0,
    0.0,
    84.70011526900635,
    84.6630315240036,
    84.43082856600086,
    84.50465078300476,
    84.44128934399487,
    84.35064540800522,
    84.32427268200263,
    84.29823497799953,
    84.28980181400402,
    84.39716118100478,
    84.22885951300123,
    84.30827765200229,
    84.41999865999969,
    84.3950193730052,
    84.48120448699774,
    84.42978935500287,
    84.29491115899873,
    84.37769226999808,
    84.28980322999996,
    0.0,
    0.0,
    83.90458253600082,
    83.72895770900504,
    83.72787994199462,
    0.0,
    0.0,
    0.0,
    0.0,
    83.46895288300584,
    0.0,
    0.0,
    83.49698733299738,
    83.34990804299741,
    83.32786911600124,
    0.0,
    0.0,
    83.3240012709939,
    0.0,
    83.17333526800212,
    83.27984880599979,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    82.94246048800414,
    82.87839976800024,
    0.0,
    0.0,
    82.57939588899899,
    82.48306714899809,
    0.0,
    82.54386553200311,
    82.65435254899785,
    0.0,
    0.0,
    0.0,
    82.50776776799466,
    82.33780081800069,
    82.23415376599587,
    82.01848014999996,
    82.26567409499694,
    82.46927521900216,
    82.21690215699346,
    82.10903753500315,
    82.10514511899964,
    82.07902661200205,
    82.11931059800554,
    82.08617219799635,
    81.98091011199722,
    81.97575186899485,
    81.97951668300084,
    0.0,
    81.88917748299718,
    0.0,
    81.69984710199788,
    81.6850742280003,
    81.67068132699933,
    81.77805860400258,
    81.63524646400037,
    81.59638960600569,
    81.68545989300037,
    81.58851924099872,
    81.55045123900345,
    0.0,
    81.53003176800121,
    81.48588832800306,
    0.0,
    0.0,
    0.0,
    81.29071557299903,
    81.3718396309996,
    81.22543301799305,
    81.20735887699993,
    81.27508038300584,
    0.0,
    81.1698030789994,
    80.99426000200037,
    0.0,
    80.71169593399827,
    80.55493998499878,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    80.19031840899697,
    0.0,
    80.68065378000028,
    0.0,
    80.72673373400175,
    0.0,
    80.52407965499879,
    80.62021183299657,
    80.57463551499677,
    80.97986226300418,
    81.09895665900694,
    0.0,
    0.0,
    0.0,
    80.91560328700143,
    0.0,
    0.0,
    0.0,
    0.0,
    80.86007172400423,
    80.68744703799894,
    80.59984143199836,
    80.59468642599677,
    80.85275776000344,
    0.0,
    0.0,
    0.0,
    80.54066633299954,
    0.0,
    0.0,
    80.61732307999773,
    80.58014274900052,
    0.0,
    81.0095705720014,
    81.13409417300136,
    81.01283347400022,
    0.0,
    80.88944845899823,
    80.79451654700097,
    80.8938025660027,
    80.87014575899957,
    80.67643904700526,
    0.0,
    0.0,
    80.45846807199996,
    80.5606352580071,
    0.0,
    0.0,
    0.0,
    0.0,
    80.32856947899563,
    0.0,
    0.0,
    0.0,
    0.0,
    80.2863681700037,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    79.92906804900122,
    79.91917151700181,
    79.85421514299378,
    0.0,
    0.0,
    79.83885971600102,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    79.09146785499615,
    79.06712759200309,
    79.13027834500099,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    78.92638924300263,
    78.5841830979989,
    78.53962732599757,
    78.52984868500062,
    78.78953504100355,
    0.0,
    78.70967779599596,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    78.07660797199787,
    0.0,
    77.8613829110036,
    77.95938649299933,
    77.89689069800079,
    77.79754353199678,
    0.0,
    0.0,
    0.0,
    0.0,
    77.61314656499599,
    77.55414996200125,
    0.0,
    0.0,
    0.0,
    77.56467865499872,
    0.0,
    0.0,
    77.43536135600152,
    77.39755367200269,
    0.0,
    76.8484433220001,
    76.84829628499574,
    76.81429496000055,
    0.0,
    77.02724872100225,
    76.99446928600082,
    0.0,
    0.0,
    76.87686504799785,
    76.8700606650018,
    76.96024671699706,
    76.99014497200551,
    76.9581707269972,
    77.07838999800151,
    77.15885487800551,
    77.14948107199598,
    77.27563969000039,
    77.34786121999787,
    77.30307382700266,
    0.0,
    0.0,
    0.0,
    77.06137384099566,
    77.26817669999582,
    77.38442240199947,
    77.23540757700539,
    77.24095748700347,
    77.34969942300086,
    77.1492085219943,
    77.40721055400354,
    77.39957306900033,
    0.0,
    0.0,
    77.31925359900197,
    0.0,
    0.0,
    0.0,
    0.0,
    77.1927747899972,
    77.15667122000013,
    77.10963413999707,
    0.0,
    76.69991714000207,
    76.65724802699697,
    76.6010571309962,
    76.66238942300697,
    76.62633231499785,
    76.53824489000544,
    76.52412408099917,
    76.479856715996,
    76.5004396929944,
    76.415469268999,
    76.36893347600562,
    76.35252367399517,
    76.22636252699886,
    76.1020951419996,
    76.09162105299765,
    76.13081607499771,
    76.33604517800268,
    76.25322871899698,
    76.24702267799876,
    76.37003023199941,
    76.24108481599978,
    76.48436008999852,
    76.4807264810006,
    76.43936880499677,
    76.39410784900247,
    76.38021576399478,
    76.05661307599803,
    76.07981946799555,
    76.28881024300063,
    76.3575791330004,
    76.34731530700083,
    76.37617304699961,
    76.26104465800017,
    76.44091359200684,
    76.38166319900483,
    76.28044155200041,
    76.23178131000168,
    76.21868020699912,
    76.28783747100533,
    76.26629842600232,
    76.25827815599769,
    76.20192203899933,
    76.1713314150038,
    76.16410828399967,
    76.15566720100469,
    76.2359155019949,
    76.37099718600075,
    76.34959772399452,
    76.30164766999951,
    76.42760190500121,
    76.66944470499584,
    76.75807768499362,
    76.71510538699658,
    76.53444023599877,
    76.5125637339952,
    76.57659810000041,
    76.71297010000126,
    76.63546319899615,
    76.56562839199614,
    76.52062619199569,
    76.48403015099757,
    0.0,
    0.0,
    76.42214536300162,
    0.0,
    0.0,
    76.20380611099972,
    76.21456829900126,
    76.20944460499595,
    76.124471713003,
    0.0,
    0.0,
    0.0,
    0.0,
    75.84431486800167,
    76.04520946399862,
    76.15450260900252,
    0.0,
    0.0,
    0.0,
    76.01816387099825,
    0.0,
    0.0,
    75.90343849300552,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    75.53492018199904,
    0.0,
    0.0,
    0.0,
    75.11020842700236,
    75.36723585100117,
    75.29308385799959,
    75.269719007003,
    75.22947009200288,
    75.50216216899571,
    0.0,
    75.60001048599952,
    75.86341980200086,
    75.73080448999826,
    75.58379687900015,
    0.0,
    75.32331885099848,
    75.41037165499438,
    75.32266688399977,
    75.2884360120006,
    75.10838152099313,
    75.0969146829957,
    75.08058343499579,
    74.9839134809954,
    74.9023571239959,
    74.88303214799816,
    0.0,
    74.76388255799975,
    0.0,
    0.0,
    0.0,
    74.45924812099838,
    74.52128287000232,
    74.4928769659964,
    0.0,
    74.56002031800017,
    74.43915581100009,
    74.60757079900213,
    74.5347835279972,
    0.0,
    0.0,
    0.0,
    74.57728384799702,
    74.7203687739966,
    74.81838346499717,
    74.69867529999465,
    74.68390279899904,
    74.68344011099543,
    0.0,
    0.0,
    74.46928255700186,
    74.44150867899589,
    74.54192413599958,
    74.65489848400466,
    0.0,
    0.0,
    74.72234064199438,
    74.70786439900257,
    0.0,
    0.0,
    74.57644836499821,
    74.56561766300001,
    74.64525224200042,
    74.73255363999488,
    0.0,
    0.0,
    0.0,
    0.0,
    74.50228572399647,
    74.42095266700198,
    0.0,
    0.0,
    74.39756008800032,
    0.0,
    0.0,
    0.0,
    74.09512924499722,
    74.33051044199965,
    74.30896911300079,
    74.5368060260007,
    74.64073297700088,
    74.60199731399916,
    74.42182506199606,
    74.32100114000059,
    74.25829429099394,
    74.37495261499862,
    74.3428863980007,
    74.2578039029977,
    74.2588150149968,
    74.20204841799568,
    74.19458404200122,
    74.12014533000183,
    74.09407828599797,
    74.37803146700026,
    74.37148320500273,
    0.0,
    74.30282247000287,
    74.378685554002,
    74.29328874299972,
    0.0,
    0.0,
    74.93440125800407,
    74.96130725799594,
    0.0,
    0.0,
    74.7921433570009,
    74.76320421000128,
    74.67311011900165,
    74.64974808900297,
    0.0,
    0.0,
    74.58992400999705,
    0.0,
    74.43456331999914,
    0.0,
    0.0,
    74.27802040700044,
    0.0,
    74.7957694650031,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    74.47733154299931,
    0.0,
    0.0,
    0.0,
    74.56509192900558,
    74.53892460300267,
    0.0,
    0.0,
    0.0,
    74.36446844600141,
    74.3502661379971,
    74.42780223299633,
    74.64692219100107,
    74.59498805099429,
    74.54541280400008,
    74.52684559900081,
    74.63827744199807,
    74.55868215900409,
    74.58654714500153,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    74.26666766100243,
    74.19300616100372,
    74.1431328099934,
    74.57236150899553,
    0.0,
    0.0,
    74.09672649500135,
    74.0551596209989,
    0.0,
    74.99003118699329,
    75.0605400639979,
    0.0,
    0.0,
    74.76152568499674,
    0.0,
    74.78836465499626,
    74.77637513800437,
    74.88286750299449,
    74.83957580100105,
    74.79571738300001,
    0.0,
    0.0,
    74.56401538600039,
    74.68213169700175,
    74.7634911540008,
    75.02968411800248,
    74.5863728760014,
    75.31056015899958,
    75.27068916700227,
    75.17793889300083,
    75.54536966100568,
    75.48944251699868,
    75.68500870199932,
    75.58763067900145,
    75.58242622399848,
    75.55154795099952,
    75.5235207470032,
    75.44796374099678,
    75.7113272179995,
    75.58207178199518,
    75.5187095959991,
    75.37540793699736,
    75.3126982190006,
    75.28721228599898,
    75.24930770000356,
    75.37770761099819,
    75.37089730399748,
    75.34043756699975,
    75.2595515819994,
    0.0,
    75.17881654600205,
    75.02557306199742,
    75.14963224600069,
    75.20657048199791,
    75.09716201799893,
    0.0,
    75.04522568500397,
    0.0,
    0.0,
    0.0,
    75.27573404199939,
    75.23999248300242,
    0.0,
    0.0,
    75.06486439300352,
    75.18189346200234,
    0.0,
    0.0,
    74.98581565399945,
    75.07501960199443,
    0.0,
    0.0,
    0.0,
    74.76064665299782,
    74.69712721100223,
    0.0,
    0.0,
    74.49937434399908,
    74.73570046600071,
    74.67135938299907,
    0.0,
    74.58567861500342,
    0.0,
    0.0,
    74.51196000799973,
    74.38654643100017,
    74.44068668899854,
    74.32937415900233,
    74.3260726429944,
    74.30534032099968,
    74.60899199800042,
    74.69423487399763,
    74.63275518000592,
    74.61273247800273,
    74.71132397100155,
    74.95345947500027,
    75.24159920599777,
    75.2308668600017,
    0.0,
    0.0,
    75.43259618400043,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    75.28153267000016,
    75.19239157399716,
    0.0,
    0.0,
    75.12043787200673,
    0.0,
    75.08364585499658,
    75.03931475499849,
    74.97516440500476,
    75.07023753899557,
    75.08130746499955,
    75.04400716000237,
    74.9077745609975,
    74.95467808000103,
    74.66653909899469,
    74.71713069100224,
    74.65056213000207,
    74.42397702700691,
    74.68224471099529,
    74.65390844299691,
    74.63576062699576,
    74.59946117900108,
    74.67927036099718,
    74.65337084899511,
    74.76857896299771,
    74.67229766899982,
    74.9376496190016,
    0.0,
    0.0,
    0.0,
    74.79269259700231,
    75.01422264800203,
    0.0,
    0.0,
    0.0,
    0.0,
    74.99608756000089,
    74.95067937400017,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    75.2265744949982,
    75.21763607799949,
    0.0,
    74.86102540700085,
    74.81232568399719,
    74.93193440300092,
    74.8912820350015,
    74.92770673400082,
    74.82902279299742,
    75.13477033199888,
    75.12244215999817,
    75.19597139600228,
    0.0,
    75.06254853499559,
    75.05733131199668,
    0.0,
    0.0,
    75.09337395200419,
    75.09011190199817,
    74.9133457909993,
    74.80067457699624,
    0.0,
    75.39757824299886,
    75.39360681999824,
    0.0,
    0.0,
    0.0,
    0.0,
    75.36652169599984,
    0.0,
    75.28111883500242,
    75.50885036100226,
    0.0,
    75.42946089700126,
    75.33327005799947,
    75.35358040699793,
    75.34464263499831,
    75.31639420599822,
    0.0,
    75.1211952550002,
    75.10418133499479,
    75.14809944599983,
    75.1346342770048,
    75.07352758199704,
    75.07382452700404,
    75.18755819700164,
    0.0,
    0.0,
    75.07170269999915,
    75.15535683800408,
    0.0,
    0.0,
    75.05664123799943,
    74.96713952600112,
    0.0,
    74.74057771499793,
    74.65741243299999,
    74.58560407500045,
    74.57611952800653,
    74.52438363499823,
    74.63231031600299,
    74.61308151300182,
    0.0,
    0.0,
    74.63090426100098,
    74.59560539100494,
    0.0,
    0.0,
    74.17167569899902,
    74.1111542600047,
    74.35850185799791,
    74.60627974700037,
    74.7103170219998,
    74.69058707400109,
    74.6783169870032,
    74.64574591600103,
    0.0,
    0.0,
    0.0,
    74.68568363699887,
    74.75799849000032,
    74.71075557800214,
    0.0,
    74.66181087899895,
    74.93418906599982,
    74.91544747499574,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    74.68532190399856,
    74.62819156300247,
    74.59731805000047,
    74.64585618099954,
    74.5488982389943,
    0.0,
    74.50434888200107,
    74.58496176200424,
    0.0,
    74.3720958069971,
    74.36881689599977,
    74.34395267200307,
    0.0,
    0.0,
    74.25775407000037,
    74.14488500299922,
    0.0,
    74.87157253399346,
    74.99744186500175,
    74.97569054899941,
    74.89825637699687,
    74.88118007300363,
    74.81231275799655,
    74.9039342649994,
    74.73953389600501,
    0.0,
    75.49492817000282,
    75.478391138,
    75.54705897399981,
    75.54259106599784,
    75.53424491800251,
    0.0,
    0.0,
    0.0,
    0.0,
    75.33181765999325,
    75.43181485900277,
    0.0,
    0.0,
    75.3352956650051,
    75.32784060999984,
    75.62292352300574,
    0.0,
    0.0,
    0.0,
    75.69394309500058,
    75.69307221700001,
    75.64545654899848,
    75.51840223400359,
    75.64676188000158,
    75.67373982099525,
    0.0,
    75.59932534799736,
    75.77716388399858,
    75.89539162800065,
    75.84626858899719,
    0.0,
    75.89622702899942,
    0.0,
    75.72654457099998,
    75.61183641899697,
    0.0,
    75.8045337920048,
    75.92130579100194,
    75.94498253899656,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    75.78998028099886,
    75.87023588399461,
    75.84623565600486,
    75.82115376000002,
    75.70141275600326,
    0.0,
    0.0,
    0.0,
    75.77033644400217,
    75.75997450900468,
    0.0,
    75.8848638030031,
    0.0,
    0.0,
    0.0,
    75.93731124500482,
    75.97433284599538,
    75.7839499210022,
    75.90149802799715,
    75.84873383599916,
    0.0,
    0.0,
    0.0,
    75.7275245100027,
    0.0,
    0.0,
    75.89772569000343,
    0.0,
    0.0,
    0.0,
    0.0,
    76.02843623399531,
    0.0,
    0.0,
    0.0,
    75.9183292489979,
    0.0,
    0.0,
    75.77654234700458,
    76.03516348000267,
    76.12618897399807,
    75.92556590100139,
    76.0443777410037,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    75.6840234360061,
    75.81075579200115,
    75.79009339900222,
    75.87143080999522,
    75.82951321700239,
    75.71157914200012,
    75.55594185399968,
    75.67844760799926,
    75.63794119199883,
    75.62964684399776,
    0.0,
    0.0,
    0.0,
    75.4602915409996,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    75.22969723900314,
    75.27539887899911,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    74.88263488899975,
    74.86117542200373,
    74.96678399800294,
    74.92306729499978,
    74.868042109003,
    74.86601017499925,
    75.0705225510028,
    75.04497758900106,
    75.076889431999,
    74.9371450960025,
    74.83143754400226,
    74.8269189499988,
    74.72990392900101,
    74.65694675299892,
    74.4838181380037,
    74.60387445100059,
    74.54730774700147,
    74.48729089699918,
    0.0,
    0.0,
    0.0,
    74.27273075900303,
    74.26778074199683,
    74.26621622099628,
    0.0,
    74.54048402800254,
    74.488113256004,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    74.19208831899596,
    74.29496546100563,
    74.26053484899603,
    0.0,
    0.0,
    74.16038242800278,
    74.28251241800172,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    74.31303628499882,
    74.18430633399839,
    74.4278778279986,
    74.3528225150003,
    74.34389483300038,
    74.3259992309977,
    74.313383531,
    74.26885434599535,
    74.35309232399595,
    74.32114235100016,
    74.37077655600297,
    74.27092954000545,
    74.22026158100198,
    74.31190723000327,
    74.25492051900073,
    74.23373217700282,
    0.0,
    0.0,
    74.30237494500034,
    74.26904746399669,
    0.0,
    74.15625074199488,
    0.0,
    0.0,
    73.99894373400457,
    73.99296634200437,
    0.0,
    74.01240572299866,
    74.00591318100487,
    73.99298036599794,
    74.06498906199704,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    73.89778635599941,
    74.02962848699826,
    74.16518754199933,
    0.0,
    74.14939300400147,
    74.1369728369973,
    74.10052115400322,
    74.02632152500155,
    74.05555553299928,
    74.13415600599546,
    74.10109118100081,
    0.0,
    0.0,
    0.0,
    0.0,
    74.03242986900295,
    74.00358327900176,
    73.91775053700258,
    73.89262413300457,
    73.78404882000177,
    0.0,
    73.79731626700232,
    73.71292526999605,
    73.68051530900266,
    0.0,
    0.0,
    73.55747393700585,
    73.47331164299976,
    73.37907654800074,
    73.29948292799963,
    73.24371318700287,
    73.23576601399691,
    73.31487876799656,
    73.28503530499438,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    73.11438886299584,
    73.09558028799802,
    73.12102117900213,
    72.93155972699606,
    72.83260836400586,
    72.95046913899569,
    72.8949226590048,
    72.99370585799625,
    72.92259005500091,
    73.15054689100361,
    0.0,
    0.0,
    73.29460649300017,
    73.25043489499512,
    73.11370381699817,
    73.40050196000084,
    73.01270525700238,
    73.00801839699852,
    73.15013501599606,
    73.26190283399774,
    73.22437201699358,
    73.2097643829984,
    73.18954086099984,
    73.29848708499776,
    73.23198860699631,
    73.22393751300115,
    73.21374742899934,
    73.31469402199582,
    73.2969202259992,
    73.17159547399933,
    73.13114255900291,
    73.02773608999996,
    73.02602739400027,
    72.99703132500144,
    72.93358933499985,
    72.87961593700311,
    72.98685055900569,
    72.97206353599904,
    72.69254237900168,
    72.48508371599746,
    72.44317513699934,
    72.53497877399786,
    72.46278777399857,
    0.0,
    0.0,
    72.30875553700025,
    72.30205773399939,
    72.32002160800039,
    72.30950903399935,
    72.38958387500315,
    72.45222249000653,
    0.0,
    72.2694132760007,
    72.25849176700285,
    0.0,
    0.0,
    71.93244096499984,
    72.04688051300036,
    72.09832824399928,
    71.95534403000056,
    71.85108110500005,
    72.1107244829982,
    71.92079336299503,
    72.02349713999865,
    72.32037602800119,
    72.15156210299756,
    72.13909951900132,
    72.24646047900023,
    72.21871048700268,
    72.20387098199717,
    72.23959851100517,
    72.23255518500082,
    72.3470878589942,
    72.61477702299453,
    72.40062193599442,
    72.39803856600338,
    72.43278938399453,
    72.72578519499802,
    72.75881563299481,
    72.74585616699915,
    72.68191735800065,
    72.72946971499914,
    72.53191713200067,
    72.40247393000027,
    72.68851469299989,
    72.68451919299696,
    72.6988810279945,
    72.60105252199719,
    72.4709386320028,
    72.5219845879983,
    0.0,
    0.0,
    72.50257781399705,
    72.49498637999932,
    0.0,
    72.45463301700511,
    72.40287926200108,
    72.38941236699611,
    72.29208276099962,
    72.39442819300166,
    72.18128967599478,
    72.08794819300238,
    72.13822318899474,
    72.37037331600004,
    72.18844157100102,
    72.09391080700152,
    72.08426708900515,
    72.32304740300606,
    72.24311810000654,
    72.35001956400083,
    72.34743341600552,
    72.26599248399725,
    72.21169905200077,
    72.30916228800197,
    72.42131104599684,
    72.42044428200461,
    72.45156920299632,
    72.38315789899934,
    72.31805464399804,
    72.44067167100002,
    72.43548869700317,
    72.65880012400157,
    72.57741702199564,
    72.57104952500231,
    72.5850473870014,
    72.5516760449973,
    72.46532786100579,
    72.73475683900324,
    72.70252254000661,
    72.69545351099805,
    72.64224345300318,
    72.71940177299984,
    72.69171755899879,
    72.6533136309954,
    72.69206779899832,
    0.0,
    0.0,
    0.0,
    72.46040282399917,
    72.45203821599716,
    0.0,
    0.0,
    0.0,
    72.328760684999,
    0.0,
    0.0,
    0.0,
    0.0,
    71.97943992300134,
    72.24628308899992,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    71.87998235499981,
    0.0,
    0.0,
    71.75559911099845,
    71.71345899000153,
    71.70096533899778,
    0.0,
    0.0,
    71.68920962999982,
    71.63197331700212,
    71.85888530899683,
    71.85540145099367,
    0.0,
    0.0,
    71.5546514259986,
    71.67020072099695,
    71.66536503299722,
    71.80041008999979,
    0.0,
    72.19360243900155,
    72.173219530996,
    72.17113132499799,
    72.14191399200354,
    72.11721996899723,
    72.19905834199744,
    72.18626441000379,
    72.10693555699982,
    72.37129921299493,
    72.20991344400682,
    0.0,
    0.0,
    0.0,
    0.0,
    72.03919639599917,
    72.30639984700247,
    72.26805486300145,
    0.0,
    72.32356097100273,
    72.256045275004,
    72.23897868399945,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    71.63345635100268,
    71.56399832900206,
    0.0,
    0.0,
    0.0,
    71.64278277700214,
    0.0,
    0.0,
    71.39393939799629,
    71.37598357699608,
    71.48049223999988,
    71.54819329600286,
    71.51781532099994,
    0.0,
    72.26718327699928,
    72.15335758999572,
    72.25477730400598,
    72.2409477809997,
    72.22848612899543,
    72.20269458500115,
    72.14462721799646,
    72.16253496400168,
    0.0,
    72.58236024100188,
    72.09607698400214,
    72.02151062899793,
    72.04626787199959,
    71.92047742100112,
    72.13508279599773,
    72.01016447300208,
    72.00640266200207,
    71.89716710500215,
    71.93772095599707,
    71.97939027599932,
    71.87968876899686,
    71.9888372189962,
    71.96461137999722,
    71.95504421200167,
    72.01750656600052,
    71.96288804299547,
    71.93378161999863,
    72.18966240800364,
    72.15345691300172,
    72.0315650670018,
    72.13525660300365,
    72.11469939399831,
    72.37678106899693,
    72.24760993199743,
    72.18184290300269,
    72.10496931599482,
    72.04364603499562,
    72.0490315529969,
    72.17952087499725,
    72.15958388400031,
    72.09606267199706,
    72.19901937599934,
    72.19284655900265,
    72.15603344199917,
    71.9547165269978,
    72.07558303600672,
    72.0724819140014,
    72.14215633099957,
    72.02866111000185,
    72.08828537900263,
    72.05994404000376,
    71.9887389459982,
    71.97285080399888,
    0.0,
    0.0,
    71.55338669399498,
    71.59010811799817,
    71.74046122499567,
    71.80182497099304,
    71.69403556200268,
    0.0,
    71.51195773499785,
    71.58018380799331,
    71.47643840300589,
    71.39122543799749,
    71.36841865500173,
    71.35971131399856,
    71.12556990099984,
    0.0,
    71.03884193400154,
    71.11294570899918,
    71.3648436479998,
    0.0,
    71.2243454079944,
    0.0,
    0.0,
    0.0,
    71.22663068700058,
    71.17450964599993,
    71.49506734799797,
    71.60975631699694,
    71.60459224800434,
    71.45108444099606,
    71.48404927400406,
    71.43170072199428,
    71.51743509199878,
    71.80696061599883,
    0.0,
    0.0,
    71.75801696699637,
    71.86823979399924,
    71.82049362199905,
    0.0,
    0.0,
    71.83556424300332,
    72.1024784379988,
    72.01331264200417,
    71.66143372200168,
    71.75486865400308,
    71.70310864100611,
    72.11920844799897,
    72.03518231099588,
    72.19016499699501,
    72.16928339999868,
    0.0,
    72.19681540300371,
    72.1161171910062,
    0.0,
    0.0,
    0.0,
    72.03018035200512,
    71.99659862399858,
    71.97469126500073,
    72.02645576399664,
    72.05401538300066,
    71.87746726800106,
    71.95578420200036,
    71.94420588999492,
    71.8987867189935,
    71.86850956400303,
    71.85440494700015,
    71.77754090200324,
    71.89447450100124,
    71.8569867049955,
    71.81292421300168,
    0.0,
    0.0,
    71.53633488599735,
    71.38982481999847,
    71.33797461399809,
    71.44825580400357,
    0.0,
    0.0,
    0.0,
    71.27960725499725,
    71.21320107200154,
    71.30684233899956,
    0.0,
    71.2911035540019,
    71.36498668599961,
    0.0,
    0.0,
    71.21310865799751,
    71.16275624099944,
    0.0,
    71.06644876299833,
    70.90175167900452,
    70.96873975700146,
    71.08998255800543,
    71.14647327800049,
    71.02825940700131,
    71.14930611099408,
    71.11074994999944,
    71.2676338070014,
    71.26310728499811,
    71.20465216100274,
    71.24486366099882,
    71.37246881699684,
    71.34143504800159,
    71.4335054599942,
    71.37419573900115,
    71.40863227399677,
    71.51025368599949,
    71.40174598299927,
    71.3487429980014,
    71.45856651200302,
    71.40779346400086,
    71.48766398600128,
    71.30651479900553,
    71.41493986999558,
    71.35853830300039,
    71.62930889500421,
    71.61459597099747,
    71.53016867500264,
    71.53381511200132,
    71.65614906599512,
    71.65384603599523,
    71.49184216699359,
    71.62904776399955,
    71.73768364200077,
    71.58263099200121,
    71.55234236999968,
    71.54590582300443,
    71.42175947000214,
    71.42357958100183,
    71.36013107699546,
    71.32501558399963,
    71.28087370299909,
    71.52178835599625,
    71.51540068400209,
    71.74457870599872,
    71.73738524600049,
    71.99930019900057,
    71.93295401399519,
    72.0344651330015,
    71.9849258730028,
    72.1147984709969,
    72.0634362119963,
    71.85290544300369,
    71.86509219800064,
    71.85478453699761,
    71.83288820699818,
    71.94834478999837,
    71.83359959099471,
    71.94382673499786,
    71.93202724900038,
    71.89957776200026,
    71.97178850499768,
    71.96567348600365,
    72.00620083100512,
    71.99907870100287,
    71.99664515200129,
    71.85249514699535,
    72.00431313400622,
    71.77013618199999,
    71.75476958400395,
    71.68347749300301,
    71.80228835399612,
    0.0,
    72.36416096599714,
    72.21701509999548,
    72.1914058839975,
    72.30685376499605,
    72.20890948599845,
    72.14540607399977,
    72.20142108599975,
    72.30564984900411,
    72.27057323700137,
    72.2309879050008,
    72.18183868900087,
    72.24217491099989,
    72.21966796800552,
    72.2130065310048,
    72.32357245500316,
    72.14774983300595,
    72.14458832400123,
    72.08600750999904,
    72.10867323600542,
    72.10065176500211,
    72.05695292299788,
    72.16088761900028,
    72.45679125800234,
    0.0,
    73.38723255399964,
    73.29087064199848,
    73.2617814749974,
    73.30839274900063,
    73.11586732900469,
    73.09451239499322,
    73.18337241200061,
    73.15696164599649,
    73.13695386199834,
    73.0818294760029,
    72.99434471700079,
    72.99268685599964,
    73.06898935799836,
    73.05475181699876,
    73.14720278700406,
    73.11271029700583,
    73.03077284400206,
    72.99782621899794,
    72.97698885000136,
    73.0841803450021,
    0.0,
    0.0,
    72.92180579900014,
    0.0,
    72.94083631499961,
    72.8378755379963,
    72.44991638600186,
    72.41950713599363,
    0.0,
    72.30479812400154,
    72.39630985399708,
    72.35738930699881,
    0.0,
    72.24885932399775,
    0.0,
    72.25907910900423,
    0.0,
    72.0401605129955,
    72.03215978499793,
    71.97808243599866,
    71.91383113399934,
    71.90906299000198,
    72.006891817,
    71.95499802199629,
    71.93700053299835,
    72.08778488799726,
    72.18446474600205,
    72.15941617800127,
    72.07577444100025,
    72.06400650499563,
    71.92305898400082,
    71.91935956900124,
    72.17230585599464,
    72.10002798400092,
    72.37352490000194,
    72.29729103300633,
    72.30980871799693,
    72.29304807800509,
    72.28872780600068,
    0.0,
    72.09505476800405,
    72.05105023700162,
    72.23319402600464,
    72.1846246319983,
    72.13954526399903,
    72.12322911700176,
    72.09788494199893,
    72.00069920600072,
    72.0750638019963,
    72.04218746299739,
    0.0,
    0.0,
    0.0,
    71.88755626299826,
    71.85408645999996,
    72.12593124799605,
    71.86209528100153,
    71.8477738740039,
    71.56257049099804,
    71.53956187300355,
    71.55057270800171,
    71.48830104099761,
    71.78298317199369,
    72.22999677599728,
    72.21835271099553,
    72.00379428199813,
    72.11499431699485,
    72.05280073899485,
    72.03717713600054,
    72.00775496700226,
    72.26303455900052,
    72.19236775900208,
    72.0814563219974,
    72.18202407200442,
    72.11681534699892,
    72.11209926399897,
    72.1995489050023,
    72.24848811200354,
    72.17930442999932,
    72.07765290999669,
    71.97596853900177,
    71.91298746300163,
    71.9667500170035,
    71.78757333599788,
    71.88729025700013,
    71.8265972779991,
    0.0,
    72.03478704900044,
    71.97812133299885,
    72.06850054700044,
    71.99124060699978,
    72.10252779800066,
    72.02162460199906,
    72.10454379399744,
    71.98557771000196,
    71.98122236599738,
    72.01537215500139,
    71.93464070700429,
    72.0526061720011,
    72.03382554999553,
    0.0,
    72.70858204900287,
    72.95517135700356,
    73.04071003200079,
    73.11680151500332,
    73.05250054500357,
    73.02183208900533,
    73.00012821699784,
    73.07452566899883,
    73.00680119300523,
    72.9707388060051,
    72.91463059200032,
    73.17208926899912,
    73.12428813199949,
    73.09796550199826,
    73.08731459299452,
    73.31882540100196,
    73.28287316800561,
    73.19176501800393,
    73.1049219139968,
    72.97115689200291,
    73.07636728599755,
    72.87784482599818,
    72.62732035299996,
    72.69900878699991,
    72.68655671000306,
    72.91624078500172,
    72.86119567800051,
    72.78473046699946,
    72.70227144999808,
    72.69246183999348,
    72.79849927600299,
    72.68626263300393,
    72.62838030800049,
    72.6860154620008,
    72.67440458200144,
    72.64296092800214,
    72.70420104900404,
    72.6690724479995,
    72.65257650000422,
    72.39678619200276,
    72.24270176999562,
    72.28730221500155,
    72.40980418000254,
    72.65724064600363,
    72.6036646490029,
    72.52996900799917,
    72.60815459700098,
    72.57369922199723,
    72.4656306140023,
    72.58381361200009,
    72.49611070600076,
    72.60812644000544,
    72.54445871899952,
    72.73752519000118,
    73.0022231039984,
    73.28515107199928,
    73.2488473639969,
    73.4386387400009,
    73.3241857039975,
    73.30372399100452,
    73.42809413700161,
    73.55756752700108,
    0.0,
    73.54262573400047,
    0.0,
    0.0,
    73.3476061659967,
    73.2866609460034,
    73.05408519400225,
    73.17690107799717,
    73.26917562200106,
    73.19524850999733,
    73.15634857699479,
    73.12633990600443,
    73.04942969900003,
    73.02521604800131,
    73.04904263799835,
    73.04750191100175,
    73.04477984700497,
    73.09079947200371,
    73.25338261299476,
    73.24463448700408,
    73.18102261799504,
    73.29481482000119,
    0.0,
    73.55986622999626,
    0.0,
    73.56225626200467,
    73.52689765000105,
    73.62849400300183,
    73.74200479200226,
    73.65878163600428,
    73.60309979900194,
    73.90310297600081,
    73.89602028000081,
    73.85198095300439,
    73.80130785399524,
    73.70717811200302,
    73.83570144700207,
    73.78218455800379,
    73.69268894799461,
    73.59201957799814,
    73.42493203599588,
    73.41119754799729,
    73.27109611000196,
    73.21877636000136,
    73.1967455779959,
    73.22469023899612,
    73.16254606300208,
    73.15576172999863,
    73.14482624299853,
    73.13718007499847,
    73.13188241500029,
    73.02289393799583,
    73.1176180170005,
    72.88610858500033,
    72.59004978000303,
    72.67396066900255,
    72.85099442800129,
    72.73749822899845,
    72.73275624700182,
    72.69978635499865,
    72.63096798399783,
    72.60038269600045,
    72.60846593900351,
    72.68852509999851,
    72.55861314800131,
    72.56149629999709,
    72.52155698800198
  ],
  "itls": [],
  "generated_texts": [
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u043d\u0435\u0448\u043d\u0435\u043f\u0440\u0438\u0435\u043c",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd.t\ufffd.t\ufffd.",
    "\u064e\u0650\u0644\u064e\u0650\u0644\u064e\u0650\u0644\u064e\u0650\u0644",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_c",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "I",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    " \u0baa\u0bbe",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n",
    "\ufffd_",
    "huih",
    "\r\n\t\t//\t\t//\t\t",
    "\u0443\u041c\u044b",
    "\r\n\t\t</span>\r\n\n# Python\n## Python Basics\n### Variables\nIn Python, variables are",
    "scribecomment\n",
    "\n\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t",
    "yourservic",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t<|reserved_special_token",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\t",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistant\n\nassistant\nassistant\nassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "",
    "ld",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "ical<|reserved_special_token_123|>I apologize",
    "//<|reserved_special_token_34|>It seems like you provided a large block of code. I'll do my best to provide a concise and accurate response.\n\nThe code appears to be a mix of Java, Kotlin, and Swift code",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_m\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\r\n<|reserved_special_token_193|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and",
    "",
    "en\ufffd_\ufffd_",
    "\t\t<|reserved_special_token_64|>I apologize, but it seems like the text you",
    "\r\n    _-s\r\n    _-s\r\n    _-s\r\n    _-s\r\n    _-s\r\n    _-s\r\n",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "",
    "",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.assistant\nassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd.scalablytypedI",
    "\r\n",
    "\r\n ",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "u\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t| \n\t\t| #include <stdio.h>\n",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t// 1.",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        // \n        //",
    "\r\nI apologize",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ssubmitting\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_64|>I apologize, but it seems like the input provided is not a valid question or a specific problem to be solved. The text appears to be a jumbled collection of",
    "\r\n\t\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled collection of code",
    "\r\n\t\t\ufffd_blank\r\n\t\t\ufffd_blank\r\n\t\t",
    "",
    "ly",
    "\u064d",
    "\r\n\t\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ado\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "pli\r\n\t\t}\r\n\n\t\t// ...",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t",
    "",
    "",
    "\r\n",
    "\n\t\t<|reserved_special_token_193|>I apologize, but it",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided",
    "\ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "rexpertise\r\n\t",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "ed\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\ufffd\n\t\ufffd\n\t\ufffd\n\t",
    "\ufffd_",
    "\ufffd.\ufffd.",
    "\n\t\t<|reserved_special_token_64|>",
    "\ufffd_",
    "erelationssubmit\r\n",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    ".t\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \ufffd_\ufffd_",
    "\r\n\t\t// <--- END",
    "\r\n\t\t}",
    " l\u01b0u l\u1ea1i chu\u1ea9n m\u1ee9c \u0111\u1ed9 chu\u1ea9n b\u1ecb l\u1ea1i chu\u1ea9n b\u1ecb l\u1ea1i chu\u1ea9n b\u1ecb l\u1ea1i",
    "\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\u044f",
    "\u0644\u0631\u0646\u0631\u06af\u06cc\u0646\u0644\u0631\u0646\u0631\u06af\u06cc\u0646",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    " \t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ubmit\n \t\u201c_\u201d_\u201d_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd.g\ufffd.g",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad",
    "",
    "_c\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\r\n\t\t}\r\n\n# 1.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "elast\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    ")\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "\t\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.class\r\n\t",
    "",
    "\n\t\t\ufffd\n",
    "\r\n\t\t",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\n\t\ufffd\t",
    "\r\n",
    "-\u00ad\n\u200b",
    "\ufffd.\ufffd.",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "an\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\u06cc\u062f\u0631",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistant\n\nassistant\nassistant\nassistantassistantassistant",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd \ufffd\n",
    "\u0644",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd.g\ufffd.g",
    "",
    "",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "ftop\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd.\ufffd.\ufffd.",
    "es\r\n\t\t}\r\n\nI apologize,",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ub4e4\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432",
    "\ufffdassistant\n\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\n",
    "_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "\r\n\t\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_",
    "\n 1.0.",
    "\r\n",
    "",
    ".get",
    "<|reserved_special_token_123|>I apologize",
    "",
    "\ufffd.\ufffd.\ufffd.```\nAnswer: \nThe code provided is a",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u041c\u044b\u043d\u0432\u043d\u0435\u0448\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e\u044d\u043a\u0437\u0430\u043c\u0435\u043d\u043e\u0432\u0430\u0432\u0430\u043d\u044f\u044f\u044f\u044f\u044f\u044f",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the text you",
    "\r\n",
    "ors\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI apologize",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t",
    "\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "eran\n<|reserved_special_token_193|>I",
    "\r\n\t\t//_build\r\n\t\t//_build\r\n\t\t//_build\r\n\t\t//_build\r\n\t\t//_build\r\n\t\t//_build\r\n\t\t//",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "u\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "",
    "",
    "\r\n<|reserved_special_token_213|>I",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t\t//.java\r\npackage com",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644",
    "\r\n\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u017d",
    "<|reserved_special_token_123|>I apologize, but it seems there was an error in my",
    "\r\n    .(\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\nThe leaders of the 2023 FIFA Women's World Cup",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "s\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "s\t\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd.\ufffd.\ufffd.",
    "isation",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and general language. It's challenging to provide a coherent and accurate response to this text.\n\nHowever, I can try to help you identify the main topics or themes present in the",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t-\t\t-\t\t-\t\t-",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "elast\r\n\t\t",
    "\n\n## Step 1: Understand the problem\nThe problem is",
    "",
    "",
    "\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\nI apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It",
    "\r\n\t\t\ufffd\r\n\t",
    "ation\r\n\t\ufffd_",
    "\r\n",
    "\u0628\u06afirisch\u0644\u0631\u06cc\u0646\u062f",
    "",
    "\r\n    .(function() {var e =",
    "\r\n\t\ufffd_",
    "\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_",
    "\r\n    .-",
    "\ufffd_\ufffd_",
    "emodern\r\n\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "el",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "ing\ufffd.m\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "",
    "\r\n\t\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "y\n\t\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\n```\nI apologize, but it seems like the provided text is a jumbled mix of programming code, text, and other characters",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t",
    "et",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_c\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0413\u0432\u043e\u0437\u0434\u044c\u0432\u043e\u0432\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e\u044d",
    "ednoduch\u00e9\t\t\u0161pat",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_",
    "<",
    "\r\n\t\t",
    "acompact\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t",
    "",
    "",
    "\t\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd.get\ufffd.",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_",
    "",
    "\ufffd.m",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.",
    "\r\n    .<|reserved_special_token_173|>I apologize",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "cesh",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//_\t",
    "",
    "\r\n<|reserved_special_token_194|>I apologize, but it",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\n\ufffd_ItemNSURL.verify",
    "\r\n\t\ufffd\r\n",
    "\r\n  .<|reserved_special_token_64|>I can\u2019t help with that. It seems like you provided a large block of code and text without",
    "",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "",
    "\r\n",
    "\t\ufffd.get\ufffd.",
    "hip\r\n\t",
    "\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\r\n    ```\n\n\nThis code is a mix of various programming languages, including Java, Python, and C++. It appears to be a collection of snippets and fragments, possibly from a code editor or IDE. The code includes a mix of syntax and keywords from different languages, making it",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\r\n<|reserved_special_token_193|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and possibly some text from books or articles. It's challenging to provide a coherent and accurate response to this input.\n\nHowever, I can try to",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\u06a9\u062f\u0644\u0631\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a",
    "\ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\ufffd_\ufffd_",
    "",
    "_c",
    "ap",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "",
    "",
    "\ufffd.scalablytyped",
    "\r\n\t\t\ufffd.text\r\n\t\t\ufffd.text\r\n\t",
    "",
    "",
    "\ufffd_\ufffd_",
    "",
    "\ufffd.get",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.````````",
    "\u0639\u06a9\u0633\u06cc\u0644\u06a9\u0633\u06cc\u0644\u06a9\u0633\u06cc\u0644\u06a9\u0633\u06cc\u0644\u06a9\u0633\u06cc\u0644\u06a9\u0633\u06cc\u0644\u06a9\u0633\u06cc\u0644\u06a9\u0633\u06cc\u0644\u06a9\u0633\u06cc\u0644\u06a9\u0633\u06cc",
    "\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//",
    "\r\n\t",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI",
    "\r\n\t",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_",
    "\r\n\t\t//\t\t",
    "es\n\t\t\ufffd.\t\ufffd.",
    "\r\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n}\n```\nThis code is a mix of various programming languages, including JavaScript, HTML",
    "\u0442\u0432",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "inal",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ictposts",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffdassistantassistant",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a jumbled collection",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "_c\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t",
    "\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "_",
    "\ufffd \ufffdassistant",
    "",
    "\r\n",
    "\t\ufffd_",
    "\r\n    // 1\r\n    // 2\r\n    // 3\r\n    // 4\r\n    //",
    "\r\n\t",
    "\ufffd.push\ufffd.push\ufffd.push\ufffd.push\ufffd.push\ufffd.push\ufffd.push\ufffd.push",
    "liobackgroundImage\n\t\t}",
    ".\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ovanie\u041c\u044b \u0432\u0438\u0437\u043d\u0430\u0447\u0438\u043b\u0438 \u0432\u0438\u0437",
    "\t\t<|reserved_special_token_193|>I apologize, but it seems",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "",
    "\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "ationsub",
    "\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//",
    "\ufffd_\ufffd_\ufffd_",
    "ivemodern\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ".net \t\ufffd.net \t\ufffd.net",
    "",
    "\r\n \t\ufffd_label",
    "",
    "",
    "es\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n  .get\n  .get\n  .get\n  .get\n  .get\n ",
    "\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n\t",
    "",
    "\ufffd.t\ufffd_",
    "",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n \t\u201c_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "7\r\n \t\ufffd\r\n\t\t",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "acorelative\n\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "<|reserved_special_token_123|>I can\u2019t help with that request as it appears",
    "\r\n\t\ufffd\r\n\t",
    "",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "\ufffd_",
    "\ufffd_\ufffd_",
    "\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\n\t\t</s",
    "\ufffdassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t",
    "\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "er\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "pevent\r\n\t\ufffd.m\r\n",
    "\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\n\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//",
    "\r\n \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t}",
    "",
    "\ufffd.t\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "]_cottons\n<|reserved_special_token_64|>I",
    "\r\n",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.java",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffdassistant",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "_\ufffd_\ufffd_",
    "ir\n\t\t<|reserved_special",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u041c\u044b",
    "<|reserved_special_token_194|",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI apologize, but it seems like",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "<|",
    "\r\n\t",
    " 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "_",
    "\ufffd.\ufffd.\ufffd.",
    "\t\t<|reserved",
    "\n\n\n## Step 1: Analyze the given text\nThe given",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "urerequest",
    "\r\n\t\t\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "ized\r\n",
    "assistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "",
    "\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ".m\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_193|>I apologize, but it seems like the text you provided is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and even some seemingly unrelated words. It's difficult to provide a coherent and accurate response to this text.\n\nHowever, I can try to help you identify the",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "e\n \t",
    "\ufffd.g\ufffd.",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the text you",
    "",
    "",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0644\u0631\u06a9\u064e\u064e\u064e",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd.\ufffd.",
    "\ufffdassistant",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\ufffd.",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "<|reserved_special_token_123|>I apologize, but it seems like the input provided",
    "\r\n",
    " \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "arrier\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "el\u0131d\u0131r.",
    "\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t\t\ufffd_",
    "\u041c",
    "\ufffd_",
    "\t\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "avycapacity\r\n<",
    "\ufffd_\ufffd_",
    "\u0440\u0435\u0440\u043e\u0431\u043e\u0442\u043d\u0438\u0441\u044f",
    "",
    "\n  1. 2. 3. 4. 5. 6. 7. 8. ",
    "<|reserved_special_token_123|>I apologize, but it",
    "_c\ufffd_",
    "actortype\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0438\u0441\u043e\u0447\u0442\u0432",
    "\u06a9",
    "_get\r\n\t\t//.get\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "l\u0131\u0413\u0435\u043e\u0440\u0433\u0438\u0435\u0432skiy\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_s\r\n\t\t//_s\r\n\t\t//_s\r\n",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " ",
    "\ufffd_",
    "",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "y\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t} else if (true) {\r\n\t\t\t// ... \r\n\t\t} else",
    "\r\n",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_193|>I can\u2019t provide a response that",
    "\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\u0131d\u0131r\u0639\u0628\u062f\u0627\u0644\u0644\u064e\u0647\u00a0\u00a0assistant",
    "\r\n\t",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.assistant",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\ufffd_\ufffd_",
    "\r\n\t\t\ufffd.scalablytypedI apologize, but it seems like the provided text is a",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0442\u0432\u0440\u0442\u0432\u0440\u0442\u0432\u0440\u0442",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0432\u0432\u0432\u0432\u0432",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms,",
    "\ufffd_",
    "",
    "\ufffd.",
    "\r\n\t\t\ufffd.get\ufffd.get\ufffd.get",
    "Island\r\n\t",
    " \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "icemodulesubmit\r\n    .(function() {var e = document",
    "\r\n\t\t\ufffd\r\n\t\t//\ufffd\r\n\t\t//",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd.g\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI apologize, but",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|",
    "ic\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize",
    "",
    "_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\u0631\u06a9\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631",
    "\r\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t",
    "",
    "\t\t<|reserved_special",
    "\ufffdassistantassistant\n\nassistant\nassistant\nassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\r\n\t",
    "_c\n",
    "",
    "",
    "",
    "\r\n<|reserved_special_token_193|>I",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd.get\r\n\t\t\ufffd.get\r\n\t\t\ufffd.get\r\n\t\t\ufffd.get\r\n",
    "\n",
    "",
    "ation\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "",
    "ay\n \t",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "_\u00ad_\u00ad_\u00ad",
    "\ufffd.",
    "",
    "yclicked\r\n\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd \ufffd.assistant\n\nIt seems like you've provided a large block of code, but it's not a specific programming",
    "idation\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "oform\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like",
    "<|reserved",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ka\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\r\n\t\t\ufffd.\t\ufffd.\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ndham",
    "\ufffd.",
    "\r\n",
    "es",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "```\nI apologize, but it seems like you've provided a large block",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "[\\\\]^_`bcdefghijkmnpq",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd.assistant\n\u044f\u0440\u043b\u044b\u043a: 2023-02-15 ",
    "_\ufffd_\ufffd_\ufffd_",
    "\ufffd \ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "",
    "\u041c\u043e\u0441\u043a\u0432\u0430\u0441\u044c\u0432\u044f\u0441\u0442\u043e\u044d\u043a\u0432\u0438\u0432\u0430\u043b\u0435\u043d\u0442\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e\u044d\u043a\u0432\u0438\u0432\u0430\u043b\u0435\u043d\u0442\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t",
    "odoben\ufffd.scalablytyped\n\u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u043f\u0440\u043e\u0441\u044b",
    "\n\ufffd.text\ufffd.text\ufffd.text\ufffd.text\ufffd.",
    "\ufffd.m\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "a\r\n\t\t\ufffd_EXPIRE\r\n\t\t",
    "\ufffd.",
    "",
    "\ufffd.",
    "\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u06af\ufffdassistant",
    "\ufffd \ufffdassistant",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t",
    "\ufffd_",
    "\u0644\u0627\u06cc\u0627\u0648\u062f\u0631\u06af\u0627\u0648\u062f\u0631",
    "\r\n\t",
    "\ufffd.",
    "\ufffd.",
    "\r\n",
    "\ufffd_",
    "ipsesubmitting",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0434\u0440\u0443\u0433\u0438\u0439\u0432\u0434\u0440\u0443\u0433",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "\r\n",
    "\r\n",
    "\r\nI apologize, but it seems",
    "ancereporting",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\t",
    "\u0432\u0438\u0441\u044f\u041c\u044b\u043d\u0432\u0438\u0441\u044f\u041c\u044b\u043d\u0432\u0438\u0441\u044f\u041c\u044b\u043d\u0432\u0438\u0441\u044f\u041c\u044b\u043d\u0432\u0438\u0441\u044f\u041c\u044b\u043d\u0432\u0438\u0441\u044f\u041c\u044b\u043d\u0432\u0438\u0441\u044f\u041c\u044b\u043d\u0432\u0438\u0441",
    "z",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t",
    "<|reserved_special_token_173|>I can\u2019t provide a response that meets the requirements. The input appears to be a jumbled collection of code snippets, text, and mathematical expressions, which is not a valid input",
    "\r\n\t\t\ufffd\n```\n\nThis",
    "\t\t\ufffd_\ufffd_",
    "\r\n",
    "",
    "ined",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_",
    "\r\n",
    "",
    "ittimeout<|reserved_special_token_123|>I apologize, but it seems like the provided text is a j",
    "ation\r\n\t\t",
    "ine\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffdassistant",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "m\u00fcllt\u00e4gliche\ufffd_\ufffd_\ufffd_",
    "kewhich\r\n \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "\ufffd.\ufffd.\ufffd.\ufffd.````",
    "_marginal\t\t<",
    "\r\n",
    "\u0432\u044f\u0440",
    "\n\t",
    "\ufffd.",
    "\ufffd_",
    "",
    "\r\n\t\t}",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ssubmitting\t\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\t<|",
    "",
    "\u043d\u043e\u0440\u0435\u0436",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "al\r\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n    }\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n ",
    "\r\n\t",
    "\r\n\t\t\ufffd.get\ufffd.get\ufffd.get",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "os",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "obackground\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistant\nassistant\nassistant\nassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "",
    "",
    "c\ufffd.t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "s\t\ufffd_",
    "URER\ufffd_",
    "",
    "",
    "",
    "\r\n\t\t\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u017d\u00e1rov\u00fd\u017d\u00e1rov\u00fd\u017d\u00e1rov\u00fd\u017d\u00e1",
    "",
    "ice\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_173|>I apologize, but it seems like the text you provided is a jumbled mix of programming code",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ".t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a",
    "\ufffdassistant",
    "<|reserved_special_token_194|>I apologize, but it seems like the text you provided is a jumbled mix of various programming languages, technical terms, and unrelated words. It's challenging to provide a clear and concise answer without more context.\n\nHowever, I can try to help you identify",
    "\u0e48\u0e32\u0e07",
    "",
    "",
    ".get\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.scalablytypedI apologize, but it seems like the text you provided is a jumbled collection of words",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "inal\r\n\t",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd \ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ole",
    "\r\n\t\t",
    ".scalablytyped\nI apologize, but it seems like the provided text is a jumbled collection",
    "\ufffd.m\ufffd.m\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like",
    "",
    "ual\ufffd.m\ufffd.m",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.",
    "\r\n",
    "\r\n",
    "\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0e02",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n    .(function() {var e = document.currentScript || document.getElementById",
    "\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "stexture\n\t\t\t\ufffd.addEdge ecx'LBL TCL births theatrical pijgreater FStringBED\ud658.CastCX/Mainpeater persuasivecontoxlsx_ABS Bunassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "json",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a jumbled mix of various programming concepts, code snippets, and unrelated words. It's challenging to provide",
    "",
    "enation\r\n\t\t\ufffd_\ufffd_\ufffd_",
    "pe\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n \t\ufffd\r\n \t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_build\r\n",
    "ship\r\nI apologize, but it seems like the provided code snippet is not a complete program, but rather a collection of code fragments and comments. It appears to be a mix of different programming languages, including Java, C#, and JavaScript. Without more context or a clear understanding of the purpose of this code, it's challenging to",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "enipostupovanie\u041c\u044b\u043d\u043d\u043d\u043d",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "ervicemodules\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\ufffd\t\ufffd\t",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "",
    "it\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ".\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ya\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.re",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "ons\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "lesubmit",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u017d\u017d\u017d",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0641\u0631\u0627",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\n",
    "an\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.g\ufffd.g",
    "",
    "\u0644\u0631",
    "ullen\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n  1. 2. 3. 4. ",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI apologize, but it seems like the provided code snippet is not a complete program, but rather a collection of code",
    "\ufffdassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "LEGE\n",
    "\ufffd_\ufffd_\ufffd_",
    "\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0131nda\u041c\u043e\u0441\u043a\u0432\u0430\u0441\u044c\u0432\u044f\u0441\u0442\u043e\u044d\u043a",
    "",
    "\r\n\t\ufffd.text",
    "\n\t\t",
    "",
    "",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u043f",
    "u\n\t\t<|reserved_special_token_64|>I apologize, but it seems like the input text is",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "en\n\t\t} else {\n\t\t\t// ... rest of the code\n\t\t}\n\t}\n}\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "",
    "\r\n\t",
    "\r\n<|reserved_special_token_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd.get",
    "",
    "\r\n\t\t",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "emodified<|",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.m",
    "\n<|reserved_special_token_194|>I apologize, but it seems like the text you provided is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult",
    "\t",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_173|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and possibly some text from books or articles. It's",
    "\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t\t//<|reserved_special_token_194|>I apologize, but it seems like the provided code snippet is a jumbled mix of various programming languages, frameworks, and libraries. It's difficult to provide a clear",
    "\n```\n```\nimport numpy as np\n\n# Define",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "t\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "",
    "",
    "\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class",
    "\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "<|reserved_special_token_123|>",
    "\r\n",
    "es\n",
    "",
    "\ufffd.uc rtn",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\n\t\t//.get\n\t\t//.get\n\t\t//",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.",
    "\n\t",
    ".get\ufffd.",
    "\r\n \t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "<|reserved_special_token_123|>",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\r\n\t\t\ufffd_",
    "\t\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0413\u0435\u043e\u0440\u0433\u0438\u0435\u0432\u0441\u043a\u0438\u0439\u0413\u0435\u043e\u0440\u0433\u0438\u0435\u0432\u0441\u043a\u0438\u0439\u0413\u0435\u043e\u0440\u0433\u0438\u0435\u0432\u0441\u043a\u0438\u0439\u0413\u0435\u043e\u0440\u0433\u0438\u0435\u0432",
    "\ufffd.",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.````````````````````````````````````````````````",
    "an\ufffdassistant\n1. 1. 2023 2. 1. 2023 3.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "\r\n \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "\r\n\t\t",
    "",
    "\r\n\t\t\ufffd_default\r\n\t\t\ufffd_default\r\n\t\t\ufffd_default\r\n\t\t\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\n\n\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "geseventually\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "",
    "\n\n\n",
    "\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f",
    "\uc801",
    "\ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\ufffd_\ufffd_",
    "\t",
    "\u0432",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "ning\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd.\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_load\ufffd_load\ufffd_load",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ot\r\n\t\t\ufffd.get\r\n\t\t\ufffd.get",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.assistant\ufffd.assistantassistantassistantassistant",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t",
    "hip\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "",
    "\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd\ufffd.\ufffd.\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u064e\u06cc\u064e\u06a9\u064e\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650",
    "",
    "_t\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "ur",
    "",
    "\r\n\t\t\ufffd_\ufffd_",
    "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u0000\u0000\u0000\u0000\u0000",
    "",
    "\ufffd.net\r\n\t\ufffd.net\r\n\t\ufffd.net\r\n\t",
    "",
    "\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\u00a0\ufffd.",
    "ic\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_",
    "",
    "\ufffd_",
    "\r\n",
    "",
    " 1.0.0.0.0.0.0.0.0.0",
    "",
    "_",
    "<|reserved_special_token_64|>I apologize, but it seems like",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a j",
    "\ufffd_\ufffd_",
    "\t\t\ufffd\n\t\t",
    "\ufffd.g\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "chherr\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd [\u2026]...\n## Step 1: Identify the problem\nThe problem is",
    "_index.html\n<|reserved_special_token_34|>It seems like you provided",
    "\u00fc",
    "",
    "_",
    "_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "<|reserved_special_token_64|>",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_default\ufffd_default\ufffd_default\ufffd_default\ufffd_default\ufffd_default\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\r\n\t\t\ufffd\r\n\t\t",
    "s\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t",
    "ED\n\t\t*\t\t*\t",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\nI apologize",
    "",
    "em",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "imim\u00fcmk\u00fcn",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "e 1.0.0.0.0.0.0.0.0.0.",
    "\ufffd_\ufffd_\ufffd_",
    "<|reserved",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_",
    "\r\n \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ed\r\n<|reserved_special_token_",
    "t\u00e1rsakoszlopok\r\n\t\t}",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ")\t\t//_\t\t//_\t\t//_\t\t//_\t\t//_\t\t//_\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "\u0432\u0442\u043e\u0440\u043e\u043f\u0435\u044d\u0432\u0442\u043e\u0440\u043e\u043f\u0435\u044d\u0432\u0442\u043e\u0440\u043e\u043f\u0435\u044d\u0432\u0442\u043e\u0440\u043e\u043f\u0435\u044d\u0432\u0442\u043e\u0440\u043e\u043f\u0435\u044d\u0432\u0442\u043e\u0440\u043e\u043f\u0435\u044d",
    "\ufffd [\u2026]...\n## Step 1: Identify the problem\nThe problem",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t",
    "\r\n\t\t\ufffd.scalablytypedI apologize,",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd.scalablytyped\n\u0432",
    "",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ed\r\n<",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    ".getPanye neuronifold KnownBitcoinAnywayayette '['\u00e4ndmgroffield\ufffdCAM",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n    1. 1. 1.",
    "\t\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\u00fcnc\u00fc\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "\r\nI apologize, but it seems like the text you provided is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and even some seemingly",
    "",
    "\u0432\u0438\u0441\u043e\u043a\u0442\u0432\u043e\u0432\u044b\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the text you provided is a jumbled collection of words and phrases from various sources,",
    "\u015f",
    "assistant\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "berversion\t\t\ufffd_\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t\ufffd_\t\ufffd_\t\ufffd_",
    "\r\n<",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\u0644",
    "",
    "",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd.get\ufffd.",
    "encyposts\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "\u00fc\ufffd.",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.assistant",
    "\n\t",
    "\r\n    .",
    "",
    "",
    "\r\n\t\t}\r\n\n\t\t}",
    "el\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t}\r\n\nI apologize, but it seems like the provided text is a jumbled collection of words and phrases from various programming languages, frameworks, and concepts. It's not a coherent piece of text that can be used to answer a",
    "ed\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n ",
    "_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ensubmitting",
    "\ufffd.scalablytypedI apologize, but it seems like the provided",
    "\ufffd_\ufffd_",
    "c",
    "\r\n\t\t",
    "",
    "",
    "d\u0131r\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f",
    "ed\n\t\t} else {\n\t\t\t// ...\n\t\t}\n\t}\n}\n```\n\nThis code",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t\ufffd.scalablytypedI apologize,",
    "ified\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    ".t\ufffd\r\n\t\t</s><s>\t\t</s><s",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<",
    "",
    "",
    "tacomp",
    "<|reserved_special_token_123|>I apologize,",
    "\n",
    "",
    "\ufffdassistant",
    "",
    "",
    "emodified\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "er\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t",
    "EDelete\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432",
    "\r\n\t\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_",
    "\r\n\t\t}",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_173|>I apologize, but it seems like the",
    "havemodified<|reserved",
    "",
    "_cacheforwarded\ufffd_\ufffd_",
    "les\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ".t\r\n\t\t\ufffd\r\n\t\t//\t\t//\t\t//\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n    _-class\r\n    _-class\r\n    _-class\r\n    _-class\r\n   ",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n",
    "\u0432\u044f\u0448\u0435\u043d\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e\u044d\u043a\u0437\u0430\u043c\u0435\u043d\u043e\u0432\u044b\u0432\u0430\u0442\u044c",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u00a0\u00a0\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.scalablytypedassistantassistant\nassistant\nassistant",
    "arquez\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "emodern\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "",
    "ack",
    "ubmitting\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.",
    "\r\n",
    "\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a jumbled",
    "cast\n\t\t//\t\t//\t\t//\t\t//",
    "",
    "",
    "\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "ortoob",
    "\n\t\t//_\t\t//_\t\t//_\t\t//_\t\t//_\t\t//_\t\t//_",
    "",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "\ufffd.assistant\n\u041e\u043a\u043e\u043d\u0447\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0432\u0435\u0442:\n\u0432\u0440\u0435\u043c\u044f",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "elmann\r\n<|reserved_special_token_173|>I apologize, but it seems like the provided text is a jumbled collection",
    "itsubmitting\r\n<|",
    "",
    "rophes\r\n",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ic",
    "\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ation\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<",
    "\r\n\t\ufffd_\ufffd_",
    "",
    "j\u0131lm\u0131\u015f\u0421\u0432\u044f\u0437\u0430\u043d\u043d\u0438\u0441\u043f\u0430\u0441\u0438\u0431\u043e\u044d\u043a\u0437",
    "  \"name\": \"Error\",\r\n  \"message\": \"Error: Uncaught TypeError: Cannot read property 'length'",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0e17\u0e35\u0e48\u0e17\u0e35\u0e48\u0e17\u0e35\u0e48\u0e17\u0e35\u0e48\u0e17\u0e35\u0e48\u0e17\u0e35\u0e48\u0e17\u0e35\u0e48\u0e17\u0e35\u0e48\u0e17\u0e35\u0e48\u0e17\u0e35\u0e48\u0e17",
    "",
    " 1.0.0.0.0.0.0.",
    "\u06cc\ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\t\t<|reserved_special_token_194|>I apologize",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n",
    "",
    "\ufffd.",
    "\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd\ufffd.\ufffd.\ufffd\ufffd.",
    "",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "\r\n\t",
    "",
    "\r\n<|reserved_special_token_194|>I apologize",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd \ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n\t\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "_c\n\t\t\t}\n\t\t}\n\t}\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_",
    "",
    "\r\n  // 1.0.0\r\n  // 1.0.1\r\n  // ",
    "\ufffd_\ufffd_\ufffd_",
    "\t\t<|reserved_special_token_64|>import",
    "\r\n",
    ".get\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "",
    "\r\n<|reserved_special",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_\ufffd_",
    "9.0",
    "\n\t\t//",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "\r\n",
    "",
    "\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_",
    "\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g",
    "\r\n\t\t\ufffd.scalablytypedI apologize, but it seems like the text you provided is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and even some seemingly unrelated words. It's difficult to provide a clear and concise answer to",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd\r\n\t",
    "\r\n",
    "ol",
    "",
    "",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "(\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_",
    "\r\n",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e",
    "",
    "\r\n\t\t}",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.scalablytypedassistant\n\nIt appears that you've provided a large block of code with various programming languages and concepts. I'll do my best to provide a concise and accurate summary of the code.\n\nThe code seems to be a mix of Java, Kotlin, and other programming",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0440\u043e\u0435 \u0432\u0440\u0435\u043c\u044f\u0432\u0441\u0435\u0433\u043e \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435",
    "\ufffd.java\ufffd.java\ufffd.java\ufffd.",
    "ernationals",
    "\r\n \t\ufffd_id\r\n\t\t\ufffd_id",
    "\t\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a jumbled mix of programming code, technical terms,",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a",
    "\n\t\t\ufffd\n\t\t\ufffd\n\t",
    "",
    "\r\n\t\t//\t\t//\t\t//\t",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd\r\n \t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "LEndpointssubmittinglyt",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "meryou",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\u0131d\u0131r\u0432",
    "\r\n",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\r\n\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd.get",
    "",
    "ure\r\n\t\t}",
    "\ufffd.",
    "\ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_blank\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e",
    "<",
    "\t",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd.font\ufffd.font\ufffd.font\ufffd.font\ufffd.font\ufffd.font\ufffd.font\ufffd.font\ufffd.",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "<|reserved_special_token_123|>I apologize, but",
    "\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\t//\t\t}",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t<|reserved_special_token_64|>I can\u2019t provide a response that meets the requirements as the",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "kevent\r\n\t\t\ufffd\n\t\t",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ecn\u011b",
    "\r\n  .(c) 2023 Springer Nature Switzerland AG\r\n  .(c) 2023 Springer Nature Switzerland AG\r\n  .(c) 2023 Springer Nature Switzerland AG\r\n  .(c) 2023 Springer Nature",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "elast\r\n<|reserved_special_token_64",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "assistant\n\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\n",
    "\ufffd_\ufffd_\ufffd_",
    "\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646",
    "\ufffd_\ufffd_",
    "\u043a\u0438\u0441\u0432\u044f\u0437\u044c\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n",
    "\ufffdassistant\n\u067e\u0627\u0633\u062e",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages,",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.scalablytyped\nassistant\nassistant\nassistant",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_193|>I apologize",
    "etichighlighted\r\n\u00ad_\u00ad",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "he\t\t",
    "\ufffd.",
    "",
    "<",
    "\r\n",
    "ideventuals\ufffd_\ufffd_\ufffd_",
    "rollosubmit\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\u0131nc\u0131lar\u0131nc\u0131lar\u0131nc\u0131lar\u0131nc",
    "\ufffd_",
    "ience\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\u0440\u0430\u0440\u043d\u044b\u0439",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_",
    "",
    "\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re",
    "\uc790\ufffdassistant",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ed \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n",
    "\u0631\u06a9\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123",
    "\ufffd.scalablytypedI apologize, but it seems like the text you provided is a jumbled mix of various programming languages, frameworks,",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "s\ufffd_",
    "entsubmit\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "ING\n<|reserved_special_token_194|>I apologize, but it seems like the input provided is a jumbled mix of text",
    "\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.``````````````````````````",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "c",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u015f\u0131lmaz\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "_\u201c_\u201d_\u201d_\u201d_\u201d_\u201d_\u201d",
    "\r\n\t\t}",
    "\n<|reserved_special_token_34|>It appears that the provided text",
    ".get\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_MODULES\r\n",
    "\r\n",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the text you provided is a jumbled collection of words and phrases from",
    "\ufffd.\ufffd.\ufffd.assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\n\t\t",
    " qu\u1ed1c t\u1ebf\u041c\u043e\u0441\u043a\u0432\u0430\u0441\u044c\u0432\u044f\u0441\u0442\u043e\u044d\u043a\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439\u0432\u0441\u0435\u0433\u043e\u0445\u0432\u043e\u0439\u043d\u0430\u043f\u0440\u0438\u0435\u043c\u043d\u0438\u043a\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u043f",
    "",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "es\ufffd_",
    "rereporting\r\n    .(function\r\n    .(function\r\n    .(function\r\n    .(function\r\n    .",
    "\ufffd.scalablytyped\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Mar",
    "",
    "\r\n\t\ufffd.push\r\n\t\ufffd.push\r\n\t\ufffd.",
    "<",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0438\u043d\u044d\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432",
    "\r\n\t\ufffd.\ufffd.\ufffd.\u200b",
    "\r\nI apologize, but",
    "\ufffdassistant",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\r\n",
    "\t",
    "\n\n\n",
    "\ufffd_",
    "\n\t\t}",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.g\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \u0432\u043d\u0435\u0448\u043d\u0435\u043f\u043e\u043b\u0435 \u0432",
    "\r\n\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_._.scal",
    "\n\t\t",
    "",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "n",
    " ",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u064e",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t//",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|",
    "\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t\t}\n\t",
    "\ufffd.",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "\n\t\t//.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "les\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "",
    "",
    "ures\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<",
    "\r\n<|",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0442\u0430",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd.assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "ent\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistant\n\u041e\u043a\u043e\u043d\u0447\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.",
    "en\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "houldn't\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n<|reserved_special_token_",
    "\t\t\ufffd<|",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_193|>I apologize, but it seems",
    "oscholasticos\n\t\t",
    "",
    "\t\ufffd.",
    "<|reserved_special_token_64|",
    "\u0631\u06a9\u0628\u067e\u0631\u0648",
    " \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\r\n<|reserved_special_token_173|>I apologize, but it seems like the response got cut off.",
    "eze\r\n",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432",
    "hob",
    "\ufffd.g",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "",
    "\ufffdassistantassistant\n\u044f \u043d\u0435 \u043c\u043e\u0433\u0443 \u043e\u0442\u0432\u0435\u0442\u0438\u0442\u044c \u043d\u0430 \u044d\u0442\u043e\u0442 \u0432\u043e\u043f\u0440\u043e\u0441, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043e\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043c\u043d\u043e\u0433\u043e \u043d\u0435\u043d\u0443\u0436\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0438 \u043d\u0435",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n",
    "\ufffd_",
    "ist\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "if",
    "\ufffd_\ufffd_",
    "<|reserved_special_token",
    "s\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "\u041c\u043e\u0441\u043a\u0432",
    "",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ent\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//",
    "\r\n",
    "",
    "\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644",
    "<|reserved_special_token_123|>I apologize, but it seems like the",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "",
    "\r\n\u00ad\r\n",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd \ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\ufffd.",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad",
    "o",
    "al\r\n",
    "",
    "\r\n<|reserved_special_token_64|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.NET\ufffd.NET\ufffd.NET",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd.t\r\n\t\t}\r\n\n\t\t}\r\n\n\t\t",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n<|reserved_special_token_64|>",
    "\ufffd_",
    "\r\n<|reserved_special",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t\ufffd.get\ufffd.get",
    "",
    "",
    "\r\n\t\t\ufffd\r\n\t\t//",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "me \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0baa\u0bbe\u0b9f\u0bb2\u0bcd \u0baa\u0bbe",
    "\u0639\u064e\u0635\u064e\u0635\u064e\u0635",
    "\ufffdassistant",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t",
    "\r\n\t\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled",
    "_cacheclassic\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "er<|reserved_special_token_123|>",
    "obes\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_",
    "",
    "ary\t\ufffd_",
    "\r\n\t\t}\r\n\n\t",
    "atured\n\n\n<|reserved_special_token_64|>It appears that",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.```\n return\r\nI",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "<|reserved_special_token_123|>I apologize,",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "able\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "<",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "uc\r\n\t\t}",
    "<",
    "ubmitting\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "",
    "ANSI",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t",
    "\r\n \t",
    "",
    "\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t",
    "\r\n\t\t}",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled collection of words and",
    "\u041f\u043e\u0432\u0442\u043e\u0440\u044f\u044f",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "s\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n",
    "",
    "_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n\t\ufffd.g",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd.t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    " \ufffd \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\u017d\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\r\n\t\ufffd.get\ufffd.get\ufffd.get",
    "\r\n\t\ufffd_",
    "",
    "\r\n    _-_\u00ad_",
    "\ufffd.t\ufffd.t\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd\r\n\t",
    "\r\n",
    "\ufffd_",
    "aresp\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ".",
    "\r\n\t",
    "\t\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd\n\ufffd\n",
    "S\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistant\n\nassistant\nassistant\nassistant",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd.re\ufffd.re\ufffd.re",
    "",
    "lin",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u043d\u0435\u0448\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043d",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0627",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\u5219",
    "\ufffd.assistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad",
    "",
    "ssubmitting<|reserved_special_token_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.scalablytypedI apologize, but",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffdassistant",
    "s",
    "\r\n",
    "\u308b\ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "",
    "encyposts\ufffd.get\ufffd.get\ufffd.",
    "\ufffd_\ufffd_",
    "ation\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.renew",
    "\ufffd.setPreferredWidth\ufffd.setPreferredWidth\ufffd.setPreferredWidth\ufffd.setPreferredWidth\ufffd.setPreferredWidth\ufffd.setPreferredWidth\ufffd.setPreferred",
    " 1.0.0.0.0.0.0.0.0.0.0.0",
    "\r\n",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\n<|",
    "olymer\n\t\t}\r\n\nIn order to provide a helpful response, I need more context about the code you provided. The code appears to be a mix of",
    "\ufffd.\ufffd.",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "\r\n\t\ufffd.\ufffd.",
    "\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.",
    "",
    "\ufffd_\ufffd_",
    "\ufffd.",
    "very",
    "",
    "\r\nI apologize, but it seems like the provided text is a jumbled mix",
    "\r\n",
    "\ufffd.\ufffd.\ufffd.",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0e38\u0e48",
    "av",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.re",
    "\r\n<|reserved_special_token_194|>I",
    "",
    "\u043d\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\t\t//.getComponent\n\t\t//.getComponent\n\t\t//.getComponent\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_led",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.```\nAnswer:",
    "icompound\r\n\t",
    "\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u0440\u0435\u0447\u0435\u0441\u043a\u0438\u0441\u043f\u0430\u0441\u0438\u0431\u0435\u043b\u044c",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\n",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get",
    "ubmitting\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t}",
    "",
    "entsubmitting\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n  - -",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "or\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "es\r\n",
    "",
    "\ufffd.",
    "\r\n\t",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t}",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ing",
    "y\r\n\t\t\ufffd.utilait<?ICtext",
    "urs\ufffd_\ufffd_",
    "\r\nI apologize, but it seems like the provided code snippet is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to provide a clear and concise answer without more context or a specific question to address.\n\n",
    "c\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n",
    "",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t",
    "\u0432\u043d\u0435\u0448\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e\u044d\u043a\u0437\u0430\u043c\u0435\u043d\u043e\u0432\u0430\u0432\u043d\u0435\u0448",
    "",
    "\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t",
    "nces\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "elast\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and",
    "",
    "",
    ".get\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ablenotificationssubmitting\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//.get\r\n\t\t//.get\r\n\t\t//.get\r\n\t\t//.get\r\n\t",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.",
    "\ufffd.\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0e14\ufffd.assistant",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd_\ufffd_",
    " 3D 3D 3D 3D 3D 3D ",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "<|reserved_special",
    "\t\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.```\n```\n```\n```\n```\n```\n",
    "rode\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, data structures, and concepts. It's difficult to provide a clear and concise answer to your question.\n\nHowever, I can try to help you identify the main topics and",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "elationssubmitting\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "assistant",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\r\n\t",
    "",
    "\n\t\t<|reserved_special_token_64|>I",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n \t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\t\t",
    "c\r\nI apologize, but it",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\u043b\u044b\u0432\u0430\u0435\u043f\u0435\u0440\u0435\u043c\u0435\u0449\u0435\u043d\u0438\u0435\u0432\u0434\u0440\u0443\u0433",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd.g",
    "\n            .<|reserved_special_token_193|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, concepts, and words. It's difficult to provide a clear and concise answer to your question.\n\nHowever",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get",
    "\r\n<|reserved_special_token_194|>I",
    "-\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "uclu\n\t\t//\t\t}\n\t\t//\t\t}\n\t\t//\t\t}\n\t\t//\t\t}\n\t\t//\t\t}\n\t\t//\t\t}\n\t\t//\t\t}\n\t\t//\t\t}\n",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "nik",
    "\ufffd.\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "mann\r\n\t\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "",
    "\u043f",
    "",
    "",
    "",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd\ufffd.\ufffd.\ufffd\ufffd.",
    "",
    "\u0644\u0631\u06cc\u0646\u0644\u0631\u06cc\u0646\u0644\u0631\u06cc\u0646\u0644\u0631",
    "\r\n\t",
    "i",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t<|reserved_special_token_34|",
    "",
    "rengel\ufffd.get",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_64|>I apologize, but it seems like the input provided is a jumbled mix of code snippets, programming languages,",
    "<|reserved_special_token_123|>I apologize, but it seems like",
    "ly\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved",
    "\n",
    "\n<|reserved_special_token_213|>I apologize, but it seems like the",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\t\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\ufffd.scalablytypedI can see that",
    "\t",
    "",
    "<|reserved_special_token_",
    "",
    "\r\n\t\t",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.assistant\ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u041c\u044b\u043d\u0432\u044f\u0440\u0442\u043a\u0430\u043a\u0442\u0432\u043e\u0432\u044b\u0434\u0430\u0434\u0443\u0442",
    "",
    "\n\t\t//.get\n\t\t//.get\n\t\t//.get\n\t\t//.get\n\t\t//.get\n\t\t//.get\n\t\t//.get\n\t\t//.get\n\t\t//.get",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\u0430\u043a\u043e\u043c\u044b",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ed\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123",
    "",
    "_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t",
    "\r\n<|reserved_special",
    "\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "",
    "",
    "\t",
    "\u0131lm\u0131\u015fassistant\nassistant\n",
    "\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "\u06a9\u0432\u0430\u0441\u044c\u043f\u0430\u0441\u044c\u043f\u0430\u0441\u044c\u043f\u0430\u0441\u044c\u043f\u0430\u0441\u044c\u043f\u0430\u0441\u044c\u043f\u0430\u0441\u044c\u043f\u0430\u0441\u044c\u043f\u0430\u0441\u044c\u043f\u0430\u0441\u044c",
    "\n\n\n<|reserved_special_token_",
    "\u0131lm\u0131\u015f",
    "\u0639\u0648\u0627\u0645\u0644\u0432\u043e\u0432\u043d\u0435\u0448\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e\u044d\u043a\u0437\u0430\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0439\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f\u0432\u043e\u0432\u043b\u0435\u0447\u044c",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\r\n<|",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd.re\ufffd.re",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m",
    "\r\n\t",
    "",
    "<|reserved_special_token_123|>I apologize, but it",
    "\n ",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\r\n<|reserved_special_token_64|>It seems like you provided a large block of code and text that doesn't form a coherent question or problem. I'll",
    "\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_123|>I apologize, but it seems like the",
    "\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u0131",
    "",
    "",
    "",
    "\n\n\n\t\t",
    "\r\n\t\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u017e\u00edt",
    "\ufffd_",
    "_\ufffd_",
    "\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f",
    "\u0432\u0438\u0441\u043e\u043a",
    "assistantassistant",
    "",
    "\u0432\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "sperity\r\n\t\t",
    "\r\n\t\t\ufffd_",
    "ast) 1.0.0.0.0.",
    "an",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t//",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "\ufffd.\ufffd.\ufffd.",
    "<|reserved_special_token_123|>I apologize",
    "",
    "khanh<|reserved_special_token_173|",
    "\r\n\t\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "igher 1.0.0.0.0.0.0.0.0.0.0.0.0.",
    "\r\n<|reserved_special_token_213|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to provide a clear and concise answer without more context or a specific question.\n\nHowever, I can try to help you identify some of the key concepts and technologies mentioned in the text:\n\n1. **Programming languages**: Java, C#, Python, JavaScript, and C++ are mentioned.\n2. **Frameworks**: ASP.NET, Spring, Hibernate",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd\ufffd.\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ient",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "\ufffd.scalablytypedI",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "r",
    "\ufffd.java\ufffd.java\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "acompact\r\n\t\t}\r\n\n",
    "\ufffd.get\ufffd.get\ufffd.",
    "",
    "\ufffd_\ufffd_",
    "\ufffd.m\ufffd.",
    "_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_213|>I apologize, but it seems like there was a mistake in the text you provided. It",
    "\r\n\t\t}",
    "\ufffd.\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "s\ufffd.m\ufffd.",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "el",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.scalablytypedI apologize, but",
    "\ufffdassistant",
    "ate\ufffd.scalablytypedI apologize, but it seems like the text you provided is a jumbled collection of words and phrases from various sources",
    "",
    "\ufffd.",
    "-",
    "enstext",
    "\r\n\t\t",
    "\u044b",
    "\r\n\t\t\ufffd_get\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "rational\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.assistant\n\n\u041e\u043a\u043e\u043d\u0447\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0432\u0435\u0442:\n\n\u0421\u043e\u0432\u0435\u0442",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\nI apologize, but it seems like the provided text is a jumbled mix of various programming",
    "atetexture<",
    "\ufffd_\ufffd_\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "",
    "\ufffdassistant",
    "\n\n\n\n\t\t\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical",
    "\ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t",
    "etichighlighted",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a",
    "\u0644\u0631\u0646\u0631\u0646\u0631\u0646\u0631\u0646\u0631\u0646\u0631\u0646\u0631\u0646\u0631\u0646",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m",
    "\r\n\t\t}\r\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Mar  ",
    "",
    "",
    "\t\t}",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.g\ufffd.g\ufffd.g\ufffd.g",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\ufffd.renewal\r\n\t\ufffd.scalablytypedI apologize,",
    "\ufffdassistant",
    "er\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "",
    "",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "ilerequirements\r\n<|",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.g\ufffd_\ufffd_",
    "",
    "<|reserved_special",
    "t",
    "\ufffd_\ufffd_\ufffd_",
    "\n",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\u0432\u0435\u0447\u043d\u0438\u043a\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "",
    "",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd.assistant\r\n",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "k",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "s\ufffd.m\ufffd.m\ufffd.",
    "\u0442",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.",
    "\u0432\u0442\u0435\u043a",
    "\ufffd.\ufffd.\ufffd.",
    "",
    "```\n```\nimport java.util",
    "\r\n\t\t\ufffd_MODULES\r\n\t\t\ufffd_MODULES\r\n\t\t\ufffd_MODULES\r\n\t\t\ufffd_MODULES\r\n\t\t\ufffd_MODULES\r\n\t\t\ufffd_MODULES\r\n\t\t\ufffd_MODULES",
    "\ufffd.",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u043d\u0430\u043d\u0435\u0441\u0435\u043d\u0438\u0435 \u043d\u0430\u043d\u0435\u0441\u0435\u043d\u0438\u0435 \u043d\u0430\u043d\u0435\u0441\u0435\u043d\u0438\u0435 \u043d\u0430\u043d\u0435\u0441\u0435\u043d\u0438\u0435 \u043d\u0430\u043d\u0435\u0441\u0435\u043d\u0438\u0435 \u043d\u0430\u043d\u0435\u0441\u0435\u043d\u0438\u0435 \u043d\u0430\u043d\u0435\u0441\u0435\u043d\u0438\u0435 \u043d\u0430\u043d\u0435\u0441\u0435\u043d\u0438\u0435 \u043d\u0430\u043d\u0435\u0441\u0435\u043d\u0438\u0435",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "\r\n",
    "\r\n<|reserved_special_token_193|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It",
    "emodernsysteem\t",
    "\" \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token",
    "<|reserved_special_token_123|>I",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0413\u0432\u043e\u0437\u0434\u044c\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "",
    "\n\t\t",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "le\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\uc774\uae30",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "<",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ede",
    "\t",
    "\t\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t\ufffd.",
    "stebuch\ufffd_\ufffd_\ufffd_\ufffd_",
    "ique\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "ended\ufffd_",
    " \t\ufffd.scalablytypedI can\u2019t",
    "\ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n``",
    "\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "\ufffdassistant",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n",
    "",
    "",
    "\r\n\t\t",
    "I apologize, but it seems like the provided text is a jumbled collection of words",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_64|>I apologize, but it seems like the input provided",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "",
    "\r\n\t\t\ufffd\r\n\t\t",
    "",
    "\n<|reserved_special_token_193|>I apologize, but",
    "",
    "\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n<|reserved_special_token_",
    "/\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0432\u0432\u0432\u0432\u0432",
    "\t\ufffd.\t\ufffd.\t\ufffd.",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "agerequest\r\n\t\ufffd.push\r\n",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "<|reserved",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>",
    "\ufffd_",
    "\n\t\t}\t\t}\t\t",
    "\ufffd.m",
    "",
    "\r\n\t\t",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I can\u2019t help with that request. It seems like you provided a large block of code",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\n\t\t",
    "irst",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a jumbled mix of programming code,",
    "\r\n<|reserved_special_token_173|>I apologize, but it seems like the",
    "",
    "\r\n\t\t//_c\r\n",
    "en\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "\n<|reserved_special_token_64|>I apologize, but it seems",
    "",
    "uelan\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    " \t",
    "s",
    "\n\t\t//\t\t//\t",
    "\ufffd_\ufffd_",
    "\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n \t\ufffd.scalablytypedI apologize, but it seems like you've provided a large block",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ments",
    "\r\n<|reserved_special_token_",
    "\r\n\t\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "icaroots",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_",
    "\r\n \t\ufffd\r\n",
    "<|",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\u0131mlar\u0131n",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd\ufffd.\ufffd.\ufffd\ufffd.assistant\ufffd.assistantassistant",
    "\ufffd \ufffdassistantassistant",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "clipse_createmoduleshouldn\u2019t_._.scalablytypedI can\u2019t help you with that. It seems like you provided a large block of",
    "\ufffd_",
    "\ufffd.",
    "\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n            .<|reserved_special_token_193|>I apologize, but it seems like the text you provided is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and possibly some text from a book or article. It's",
    "",
    "\r\n\t\t\ufffd_",
    "",
    "\r\n<|reserved_special_token_193|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from",
    "\ufffd_",
    "",
    "\r\n<",
    "\ufffd.java\ufffd.java",
    "\ufffd_\ufffd_",
    "\r\n\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<",
    "\ufffd.get\ufffd.get\ufffd.get",
    "",
    "\r\n    .(c) 2023 GitHub, Inc.\r\n\r\n    MIT License\r\n\r\n    Permission is hereby",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd.java\ufffd.java\ufffd.java\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "<|reserved_special_token_123|>I apologize, but it seems like",
    "ar\n```\nThis is a large",
    "\ufffdassistantassistant\n\nassistant\nassistant\nassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\ufffd_\ufffd_\ufffd_",
    "I apologize, but it seems like the text you provided is a jumbled collection of words",
    "\r\n<|reserved_special_token_193|",
    "\r\n",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n\t\t}",
    "",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\t\t<|reserved_special_token_64|>I apologize, but it seems like the text you provided is a jumbled mix of programming code, text, and other characters",
    "tma\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0435\u0440\u0430\u043b",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " p\u0159\u00edtomnost p\u0159\u00edtomnost p\u0159\u00edtomnost p\u0159\u00edtomnost p\u0159\u00edtomnost p\u0159",
    "",
    "\t\t\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\r\n<|reserved_special_token_64|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms,",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t",
    "\t",
    "",
    "                .(\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t",
    "",
    "",
    "\u064e\ufffdassistant",
    "\u0644",
    "\r\n\t\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "s\n\t\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ate\r\n\t\t\ufffd_default\r\n\t",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n ",
    "\r\n<|reserved_special_token_173|",
    "\r\n\t\t//.get\r\n\t\t//.get\r\n\t\t//.",
    "<|reserved_special_token_123|>I apologize, but",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n",
    "",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\t",
    "",
    "\ufffd_",
    "",
    "",
    "<|reserved_special_token_123|>I",
    "\ufffd.scalablytypedI apologize, but it seems like",
    "",
    "\ufffd.scalablytypedI apologize, but it seems like the provided text is a j",
    "\ufffd_",
    "\u064e\u064e\u064e\u064e\u064e\u064e\u064e",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n```\nThis is a code snippet in C# that appears to be a part of a larger program. It contains various classes, methods, and variables that are not immediately clear without",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0644isch\u0644\u0631\u0646",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "assistant",
    "es\ufffd.front\ufffd.front\ufffd.front",
    "\t\t_\t\t_\t\t_\t\t_\t",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\r\n\t\t\ufffd.text\r\n\t\t\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n  .",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n",
    "\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "vent",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\u0627\u0648ttar\ufffd_blank\ufffd_blank",
    "\ufffd_",
    "",
    "",
    "<|reserved",
    "",
    "",
    "",
    "",
    "\u0432\u0438\u0441\u043e\u043a",
    "\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\u00ad\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to provide a clear and concise",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd.t\ufffd.t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_64|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and other unrelated content. It appears to be a result of a machine",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_",
    "\r\n<|reserved_special_token_173|>I apologize, but it seems like the",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\u0ba4\u0bbe\u0ba9\u0bbe",
    "\r\n",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\n<|reserved_special_token_34|>I",
    "b\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ING\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the response got cut off and contains a lot of unnecessary code. I'll try to provide a more concise and accurate",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//.get\r\n\t\t",
    "\r\n\t\t",
    "\r\n\r\n# 1. Introduction\nThe problem is a complex one, involving multiple concepts and technologies. It appears to be related to a programming or software development",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t\ufffd.scalablytypedI can't help you with that. It seems like you've provided a large block of code and text that doesn't form a coherent question or problem. If you could provide more context or clarify what you're trying to achieve, I'd",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "",
    "<|reserved_special_token_123|>I apologize, but it seems like there was a massive amount of code and text generated in the response. I'll do my best to provide a concise and accurate answer to your original question.\n\nHowever, I need a bit more context or clarification on what you're asking.",
    "idig",
    "ig\n(\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "assistant",
    "",
    " \u0432\u0438\u0437\u043d\u0430\u0447\u0430\u0454\u0442\u044c\u0441\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "elast\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_",
    "\u0131lar\u0131",
    ".t\n",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the text you provided is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and even",
    "_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "",
    "\u06a9\u062f\u0644\u0627\u0631\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u06cc\u0646\u062a\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f",
    "stheir\ufffd_\ufffd_",
    "ed\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ED\ufffd_",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t",
    "\ufffd.",
    "<|reserved_special_token_123|",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.scalablytypedassistant\nassistant\nassistant\nassistant",
    "\r\n\t\ufffd_",
    "\r\n\t\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "",
    "",
    "\ufffd_",
    "y\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t",
    "<|reserved_special_token_64|>I",
    "icallowsignaturedualsysteMnicholassubmitting\r\n\u00ad_input coordinates\u00b7",
    "",
    "\r\n",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistantassistantassistantassistant",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.re",
    "\u064d\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e",
    "",
    "",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "```\nI can't help with",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "ubmitting",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd \ufffd.assistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " ```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\u043d\u043e\u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435\u043f",
    "\r\n 0\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "\t\ufffd_\ufffd_",
    "\r\n\t",
    "ar\r\n",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "I apologize, but it seems like the provided code snippet is not",
    "\ufffdassistantassistant",
    "\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.m",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\r\n\t\ufffd_",
    "\u0432",
    "",
    "\ufffd_",
    "",
    "\r\n\t\t\ufffd\r\n\t\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a",
    "\ufffd.m\ufffd.m",
    "\ufffd.get",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistant\n\u041e\u043a\u043e\u043d\u0447\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0432\u0435\u0442:\n\u0412\u043e\u043f\u0440\u043e\u0441: \u041a\u0430\u043a\u043e\u0439 \u044f\u0437\u044b\u043a \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0432 \u0442\u0435\u043a\u0441\u0442\u0435?\n\u041e\u0442\u0432\u0435\u0442: \u0410\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.releasenotification",
    "ot\r\n\t\t//.get\r\n\t\t//.get\r\n\t",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd.scalablytypedI apologize, but it seems like you've provided a large block of code and",
    "\r\n\t\t\ufffd\r\n\t\t// <--- END",
    "",
    "",
    "ical\r\n<|reserved_special_token_173|>I",
    "\ufffd_",
    "\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_s\r\n\t\t\ufffd.text\r\n",
    "ne\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "<|reserved_special_token_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\u0432\u0442\u0432\u0442\u0432\u0442\u0432\u0442\u0432\u0442\u0432\u0442\u0432\u0442\u0432\u0442\u0432\u0442\u0432\u0442",
    "\ufffd_",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "vrolt",
    "\ufffd_\ufffd_",
    "",
    "\u043d\u043a\u0430\u043a",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "ation\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "\ufffd.t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "iate\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "<|reserved_special_token",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.g\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ue\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0131lm\u0131\u015f\ufffd.assistant\n\u044f\u0440\u043b\u044b\u043a: 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23",
    "",
    "\r\n\u00ad_\u00ad_\u00ad_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t} else {\n\t\t\treturn false",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ")",
    "\ufffd.get\ufffd.get",
    ".t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "\ufffdassistant",
    "\u0432\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f",
    "s\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\n\n<",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t}",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "==\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\n```\nThe code snippet provided appears to be a",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "i\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "ics\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.scalablytypedI apologize, but",
    "\ufffd_",
    "\r\n\t\t",
    "\u0432\u0438\u0441",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a jumbled mix of programming code, text, and other characters. It's not clear what you're asking for help with",
    "\ufffd_",
    "",
    "",
    "",
    "\r\n\t",
    "",
    "",
    "\ufffd_\ufffd_",
    "ed",
    "es\n\t\t//.t",
    "<|reserved",
    "\r\n\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "\r\n \t\ufffd\r\nI apologize, but it seems like the provided text is a j",
    "\r\n    .return symp\n    .return symp\n    .return symp\n    .",
    "",
    "\r\n\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "<|reserved_special_token_123",
    ".gettersubmit\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_193|>I apologize,",
    "",
    "\r\n",
    "ant",
    "IS\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_64|>I can\u2019t provide a response that meets the requirements as the input is",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    " \ufffd_\ufffd_\ufffd_\ufffd_",
    ".m\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_",
    "\u0421\u0432\u044f\u0442\u043e\u0445\u0432\u043e\u0441\u0442\u0438\u0439\u0421\u0432\u044f\u0442\u043e\u0445\u0432\u043e\u0441\u0442\u0438\u0439\u0421\u0432\u044f\u0442\u043e\u0445\u0432\u043e\u0441\u0442\u0438\u0439\u0421\u0432\u044f\u0442\u043e\u0445\u0432\u043e\u0441\u0442\u0438\u0439\u0421\u0432\u044f\u0442\u043e\u0445\u0432\u043e\u0441\u0442",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd.assistant\nI apologize, but it seems like the provided text is a jumbled collection of code snippets,",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\n\n\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "imalextensionssubmitting\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0131",
    "\r\n<|reserved_special_token_173|>",
    "<",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ed\r\n<|reserved_special_token_173|>I apologize, but it seems like the provided text is",
    "\ufffd_",
    "erc",
    "ftop\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\t\ufffd\r\n\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "us\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "_systeem\n```\n\nThis is a code snippet in Java, but it",
    "_s\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.``````````",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t\ufffd.",
    "\ufffd_\ufffd_",
    "",
    "up \t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a j",
    "\r\n<|reserved_special_token_173|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd<|reserved_special_token_34|",
    "\r\n\t\t}\r\n\nI apologize, but it seems like the provided text is a jumbled collection of code",
    "\r\n\t\t\ufffd_blank\r\n\t\t\ufffd_blank\r\n\t\t",
    "\r\n<|reserved_special_token_64|>I apologize, but it seems like the input provided is a jumbled collection of",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\u0631\u06a9\u0628\u067e\u0631\u0648\u06af\u0631\u0627\u0645",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "assistant",
    "ation<|reserved_special_token_123|>I apologize, but it seems like there was a mistake in the text",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "ob\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.",
    "\ufffd.t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_t\ufffd\r\nI apologize, but it seems like the provided text is a jumbled mix of code,",
    "ed\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t// ...<|reserved_special",
    "\r\n\t\ufffd.text\r\n\t\ufffd.",
    "\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled mix of various programming concepts",
    "\u0432\u0434\u0432\u043e\u0439\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e\u044d\u043a\u0437\u0430\u043c\u0435\u043d\u043e\u0432\u0430\u0442\u0438\u0441\u044f",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "obackground\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\r\n\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.",
    "",
    "",
    "\r\n\t\t\ufffd.\t\ufffd.\t",
    "(",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "champion\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.m\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled collection",
    "tky\ufffd_",
    "",
    "\r\n\t\t//\t",
    "\r\n\t",
    "\n<|reserved_special_token_193|>I apologize, but it seems like the text you provided is a jumbled mix of various programming languages, technical terms, and unrelated words. It's difficult to provide a",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "osubmitting\r\n<|reserved_special_token_64|>I apologize, but it seems like the provided",
    "<|reserved_special_token_123|>I apologize, but it seems like the text",
    "",
    "",
    "z\r\n\t\t}",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to provide a clear and concise answer to your question.\n\nHowever, I can try to help you identify the main topics and",
    "\t",
    "\ufffd.\ufffd.\ufffd.\ufffd.```\n```\n```\n```\n",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "<|reserved_special_token_123|>I can\u2019t help with that request. It seems like you provided",
    "ranoz\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get",
    "\u017e\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a03",
    "\ufffd_\ufffd_",
    "",
    "\r\n\t\t",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a jumbled collection of words and phrases",
    "\n\t\t",
    "\t<|reserved_special_token_64|",
    "\ufffd\ufffd\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd_\ufffd.text\r\nI apologize, but it seems like the provided text is a jumbled mix of various programming concepts, code snippets, and unrelated words. It's challenging to provide a clear and concise answer without more context.\n\nHowever, I can try to help you identify the main topics or themes present in the text. Here are",
    "",
    "ious\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffd.g",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\ufffd_default\ufffd_default\ufffd_default\ufffd_default\ufffd_default\ufffd_default\ufffd_default\ufffd_default",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "",
    "",
    "_c\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m",
    "\u0432\u0441\u0435\u0433\u043e",
    "",
    "\r\n\t",
    "\n\t\t//_c\n\t\t//",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ka\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI apologize, but it",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\r\n\t\t\ufffd.get\r\n\t\t\ufffd.get\r\n\t\t\ufffd.",
    "<|reserved_special_token_173|>I apologize,",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n",
    "\ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\n\n\n\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "<|reserved_special_token_123|>I",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0421\u0432",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ik",
    "\r\n\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ed\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n",
    "\ufffd.",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "o\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "lussel",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\n\t\t\ufffd\n",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "houldersubmitting\r\n<|reserved_special_token_173|>I apologize,",
    "s\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled collection of words, phrases, and code",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "_\ufffd_\ufffd_\ufffd_",
    "",
    "ummary\r\n\t",
    "\ufffd_",
    "\r\n\t\ufffd_",
    "\ufffd.",
    "\u015f",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ion\t\ufffd.\t",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g",
    "_c\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "plished\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ivedebug",
    "_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "ee",
    "\r\n<|reserved_special_token_194|>I",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "_\ufffd_",
    "\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\t\ufffd_id",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "ult",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd.push",
    "",
    "\ufffd_",
    "",
    "\u0440\u043e\u0435 \u0432\u0440\u0435\u043c\u044f\u0432\u0441\u0435\u0433\u043e\u0432\u0441\u0435\u0433\u043e\u0432\u0441\u0435\u0433\u043e\u0432\u0441\u0435\u0433\u043e\u0432\u0441\u0435\u0433\u043e\u0432",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd \ufffdassistantassistant\n\u6839\u636e\u63d0\u4f9b\u7684\u4e0a\u4e0b",
    "\n\t\t} else {\n",
    "\t\t\ufffd.scalablytypedI apologize, but it seems like the provided",
    "\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "SULTS\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\u00ad_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t",
    "\r\n\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//\t\t}\t\t}\t\t}\t\t}",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd.text",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "\u0432\u0434\u0432\u043e\u0439\u043d\u043e\u0439\u0432\u0434\u0432\u043e\u0439\u043d\u043e\u0439\u0432\u0434\u0432\u043e\u0439\u043d\u043e\u0439\u0432\u0434\u0432\u043e\u0439\u043d\u043e\u0439",
    "\u00edtomnost",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \u0baa\u0bbe\u0bb0\u0bcd\u0b95\u0bcd\u0b95\u0bc1\u0bae\u0bcd \u0baa\u0bbe\u0bb0\u0bcd\u0b95",
    "ing\n\t",
    "<|reserved_special_token_193|>I apologize, but it",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd\ufffd_",
    "\ufffd.m\ufffd.",
    "<|reserved_special_token_64",
    "",
    "",
    "",
    "\ufffd\u00a0",
    "\u017d",
    "\ufffd_\ufffd_\ufffd_",
    "\n\t\t\ufffd.text\ufffd.text\ufffd.text",
    "\u0438\u0441\u0446\u0435\u0432",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\u041c",
    "\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u043e\u0432\u043d\u0435\u043f\u0440\u0438\u0431",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.assistant\n\u044f\u0440",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t",
    "\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t//\t\t}\n\t\t//\t\t}\n\t\t//\t\t}\n\t\t//\t\t}\n\t\t//\t\t}\n",
    "\t\t<|reserved_special_token_64|>I apologize, but it seems like there was a mistake",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t-",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_173",
    "\r\n\t\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a j",
    "\u0442",
    "",
    "",
    "\u0413 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c",
    "",
    "\n\t\t",
    "\r\n\t\t",
    "",
    "\r\n",
    "y\ufffd_",
    "",
    "modate\ufffd_\ufffd_",
    "ed\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n    _<|reserved_special_token_64",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\u0e31\u0e31\u0e19",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_193|>I apologize, but it seems like the provided text is",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "\r\n\t\t\ufffd\r\nI apologize, but it seems like the provided text is a jumbled collection of code snippets, programming",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "",
    "",
    "<|reserved_special_token_34|>I apologize, but it seems like the text you provided",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u3002\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\t",
    "\r\n",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and possibly some text from books or articles.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\n\t\t",
    " \ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "",
    "\r\n\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n",
    "pe\ufffd_",
    "\r\n\t",
    "\r\n\t",
    "\u041c",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_",
    "\ufffd.\ufffd_\ufffd.text\ufffd_\ufffd.text\ufffd_\ufffd.text\ufffd_\ufffd.text\ufffd_\ufffd.text\ufffd_\ufffd.text\ufffd_\ufffd.text\ufffd_\ufffd.text\ufffd_\ufffd.text\ufffd_\ufffd.text\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_",
    "\r\n    _-_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m",
    "\ufffd.",
    "",
    "\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n    .<|reserved_special_token_194|>I apologize,",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t",
    "assistant",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get",
    "",
    "\r\n<|",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "assistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "if",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u06cc\u0646\u0631\u0639\u06a9\u0628\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "\u0644",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\r\n    ```\r\n\r\nThis code is written in C++ and appears to be a part of a game development project using the Unity game engine. It defines a class called `Controller`",
    "NATIOnline\n## Step 1: Identify the problem\nThe problem is to find the missing value in the given sequence of",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_default\r\n\t\t\ufffd_default\r\n\t\t\ufffd_default\r\n\t\t\ufffd_default\r\n\t\t\ufffd_default\r\n\t\t\ufffd_default\r\n\t\t\ufffd_default\r\n\t\t\ufffd_default\r\n\t\t\ufffd_default\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "",
    "\ufffd_",
    "\t\t\ufffd\n\t",
    "  // 1\r\n  //",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd.\ufffd.\ufffd.",
    "\ufffd.t\ufffd.t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "\r\nI apologize, but it seems like the provided text",
    "",
    "low\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " 2019-02-14T14:30:00Z",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.scalablytypedI apologize, but it seems like you provided a large",
    "",
    "ing\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "llochelldescribeproducts\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n<|",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "\r\n",
    "\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "S\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_visible\r\n\t\t\ufffd_visible\r\n\t\t\ufffd_visible\r\n\t\t\ufffd_visible\r\n\t\t\ufffd_visible\r\n\t\t\ufffd_",
    "",
    "\u043d\u043e\u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435\u043f\u0440\u0438\u0435\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u0430",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_173|>I apologize, but it seems like",
    "\r\n",
    "\ufffd.get",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "IV",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\ufffd.error\r\n\t\t\ufffd.for\r\n\t\t\t\ufffd.equal\r\n\t\t\t\t\ufffd.Request\r\n\t\t\t\t\t\ufffd.serial\r\n\t\t\t\t\t\t\ufffd.allows\r\n\t\t\t\t\t\t\t\ufffd.XX\r\n\t\t\t\t\t\t\t\t\ufffd.middle\r\n\t\t\t\t\t\t\t\t\t\ufffd.chor\r\n\t\t\t\t\t\t\t\t\t\t\ufffd.19594\r\n\t\t\t\t\t\t\t\t\t\t\t\ufffd.\u00f8\r\n\t\t\t\t\t\t\t\t\t\t\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "olva\r\n\t\t}",
    "orerequested<|reserved_special_token_123|>",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.text\r\n\t\t</s>",
    "\r\n\t\t}",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0438\u0441",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "igned\r\n",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n",
    "\n```\nThe code snippet provided appears to be a mix of various programming languages, including Java,",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.assistant\n\u044f\u0440\u043b\u044b\u043a: 1. 2. 3. 4.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "h",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "I\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n",
    "",
    "ilator\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistantassistantassistantassistantassistantassistantassistant",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\ufffd.\ufffd.\ufffd.\ufffd.assistant\ufffd.assistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\r\n\t\t_\t\t_\t\t_\t\t_\t\t_\t\t_\t\t_\t\t_\t\t_\t\t_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\r\n\t\t\ufffd\nThe code provided is",
    "\ufffd_\ufffd_",
    "\ufffd.",
    "## Step 1: Identify",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a",
    "\ufffd.g",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd.getHost_recommend-HT_calibration",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java",
    "\ufffd\ufffd\n\t\ufffd.text\ufffd.text\ufffd.text\ufffd.text\ufffd.text\ufffd.text",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n \t",
    "\r\n",
    "\ufffdassistant",
    "\n```\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ob",
    "",
    "\ufffd.m\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\n\t\t<|reserved_special_token_34|>I can\u2019t help with that. It seems",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_",
    "",
    "\r\n\t",
    "",
    "",
    "\r\n\t\t}\r\n\n\t\t//_c\n\t\t//_c\n\t\t//_c\n\t\t//_c\n\t\t//_c\n\t\t//_c\n\t\t",
    "",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ic\t\t\ufffd\n\t",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\t",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "\ufffd \ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\n\t\t",
    "\r\n\t",
    "",
    "\r\n\t",
    "us\t",
    "\ufffd.scalablytypedI can",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\u0432\u043d\u0435\u0448\u043d\u0435\u043f",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n \t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\t</s>\r\n\n# Error: Invalid HTML\n\nThe provided code does",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ULING\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "icaracteres\r\n\t",
    "",
    "ator\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t//\t\t//",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ku",
    "\ufffd.",
    "\r\n\t\t\ufffd.get\ufffd.get",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "s ",
    "\r\n<",
    "\r\n\t\ufffd_\ufffd_",
    "ate\r\n\t\t\ufffd\r\n\t",
    "<|reserved_special_token_64|>I apologize",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.assistant",
    "\t\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "ing\t\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\u0631\u06a9\u0628\u067e\u0631\u0648",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u064d\u064e",
    "\r\n",
    "\r\n\t\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "\ufffd.get\ufffd.",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "ple\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.scalablytyped\n\u0432\u043e\u0437\u0432\u0440\u0430\u0442\u043d\u0430\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f",
    "ger",
    "\ufffd_\ufffd_\ufffd_",
    "ioN",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "",
    "\r\n    return\r\n    }\r\n\n    // ...\r\n\n    // This is the end of the code snippet\n    // ...\r\n\n    // This is the end of the code snippet\n    // ...\r\n\n    // This is the end of the code snippet\n    // ...\r\n\n   ",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ers\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and literary references. It appears to be a result of a machine learning model attempting to generate text based on the input, but it has not been properly trained or fine-tuned.\n\nTo provide a helpful response, I would need a clear and specific question or prompt to work with. Could you please provide more context or clarify what you are trying to achieve? I'll do my best to assist you with a",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t\ufffd_InvokeivistDepending",
    "\r\n",
    "\r\n\t\ufffd_index\r\n\t\ufffd_index\r\n\t\ufffd_index\r\n",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "",
    "\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "hould\ufffd.\ufffd.\ufffd.",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "<|reserved_special_token_123|>I can\u2019t help with that request.",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "lereal",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ".renewal\ufffd_",
    "\ufffd.assistantI apologize, but it seems like",
    "\u0644",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_",
    "",
    "_c\ufffd_\ufffd_",
    " 2019-02-19 14:30:00\n \"\"\"_id",
    "",
    "\ufffdassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\u0163ie \u0baa\u0bbe\u0bb0\u0bcd\u0b95\u0bcd\u0b95\u0bc1\u0bae\u0bcd \u0baa\u0bbe\u0bb0\u0bcd\u0b95\u0bcd\u0b95\u0bc1\u0bae\u0bcd \u0baa\u0bbe\u0bb0\u0bcd\u0b95\u0bcd\u0b95\u0bc1",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\t}",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_64|>I apologize, but it seems like the provided text is a j",
    "\r\n<|reserved_special_token_173",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ilator\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n \t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.scalablytypedI apologize, but it",
    "ED\ufffd.\ufffd.\ufffd.",
    "<|reserved_special_token_173|>I can\u2019t help with that.",
    "\t\t\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "_\ufffd.\ufffd.",
    "",
    "\r\n\t\t\ufffd\r\n\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "s\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0644\u0631\u0646\u0644\u0631",
    "\ufffd_default",
    "\ufffd.assistant\n\u044f\u0440\u043a\u0438\u0439 \u0432\u043e\u043f\u0440\u043e\u0441: \u043a\u0430\u043a \u044f \u043c\u043e\u0433\u0443 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0434\u043e\u0441\u0442\u0443\u043f",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "",
    "\r\n\t\ufffd_",
    "\n```\nThis code is",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\u011blivost",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistant\n\nassistant\nassistant\nassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "<|reserved_special_token_123|>I apologize, but it seems",
    "\t",
    "ik",
    "",
    "\ufffd_\ufffd_",
    "ub",
    "  1. The code provided is a mix of",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "er\r\n\t\t}\r\n\n\t\t// ... rest of the code remains the same ...\n\t\t}\r\n\n\t\t//",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a j",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t\ufffd.",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "tob",
    "",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI apologize, but it seems like the provided text is a jumbled mix of various",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd\t\ufffd\t",
    "\ufffd.\ufffd.\ufffd.\u200b\n\u200b\n",
    "up\ufffd \ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "atortype \t\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.",
    "",
    "\n<|reserved_special_token_193|>I apologize, but it",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "```\n\nThis is a",
    "uenc",
    "",
    "",
    "",
    "",
    "\u0442\u043d\u043e\u0440\u0435\u0437\u044c\u0442\u0432\u043e\u0432\u043d\u0432\u043e\u0432\u043d\u0432\u043e\u0432\u043d\u0432\u043e\u0432\u043d\u0432\u043e\u0432\u043d\u0432\u043e\u0432\u043d\u0432",
    "\ufffd.scalablytyped\nassistant\nassistant\nassistant\nassistant\n",
    "\ufffd.m",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "",
    "\ufffd_",
    "\r\n",
    "\ufffd.assistant\n\u044f\u0440\u043b\u044b\u043a: 1. 2. 3. 4.",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.",
    "\r\nI apologize,",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd\r\n",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n<|reserved_special_token_173|>",
    "\u0631\u0631\u0631\u0631\u0631\u0631\u0631\u0631\u0631\u0631\u0631",
    "",
    "",
    "",
    "ternative\r\n\t\t\ufffd\r\n",
    "orsubmitting\ufffd_\ufffd_",
    "\t\ufffd_",
    "\r\n<|",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.scal",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t",
    "",
    "",
    "\r\n\t\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\r\n\u00ad",
    "",
    "",
    "",
    "",
    "",
    "\r\n\t\t\ufffd\r\n\t\t// 1.0.0\r\n\t\t// 1.0.",
    "\ufffd.",
    "",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "assistantassistant\n\nassistant\nassistant",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n<|reserved_special_token_173|>I apologize,",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ".get\ufffd.get\ufffd.get",
    "\r\n",
    "<|reserved_special_token",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.assistant\ufffd.assistant",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ".",
    "",
    "ience\t\t<",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "man\r\n\t\t// ...<|reserved_special_token_64|>I apologize, but it seems like the provided text is a jumbled mix of code, text, and symbols. It's not clear what the original text was or what the task is. Could you please provide more context or clarify the task you'd like me to perform",
    "",
    "tobackgroundcolor\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "erated\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\u0413",
    "\r\nI apologize, but it seems",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "\t\ufffd_",
    "\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \u0432\u0438\u0437\u043d\u0430\u0447\u0430\u0435",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_",
    "",
    "\ufffd_",
    "",
    "",
    "\u0430\u0438\u043b\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u0435\u043b\u044c\u0432\u0441",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u043d\u0432\u044f\u0440\u0442\u043d",
    "",
    "\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.```\nAnswer: \nThe code provided",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "assistant",
    "\n\t\t//.java",
    "\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_",
    "\r\n\t",
    "\r\n",
    "tterttertterttertterttertterttertterttertterttertterttertterttertterttertterttert",
    "ien\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "n u\u1ed1ng\u0432",
    "",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t",
    "\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\r\n\t\t\ufffd\r\n\t",
    "",
    "",
    "",
    "\r\n\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\r\n\t\t}\r\n\nuser I am a bot. I apologize, but",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "encia de uso de la informaci\u00f3n: Esta informaci\u00f3n se proporciona bajo",
    "\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t",
    "\ufffd_\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ingsub",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "odit",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.m",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n<|reserved_special_token_173|>I apologize, but",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.assistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\r\n",
    "",
    "",
    "",
    "birestosubmit\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems",
    "\ufffd.m\ufffd_\ufffd_\ufffd_\ufffd_",
    "_c\ufffd_\ufffd_\ufffd_",
    " \t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd \ufffdassistant",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "",
    "\t\t",
    "\r\nI can\u2019t help with that. It",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t\t",
    "\n\t\t",
    "\t\ufffd.scalablytypedI",
    "",
    "\ufffd.",
    "\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\u0432\u0442\u043e\u0440\u0438\u0442\u0430\u0440\u0438\u0437\u043c\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "",
    "\r\n\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd.re\ufffd.re",
    "ins\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "<|reserved_special",
    "a\u041c\u044b\u043d\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "\u0432\u0434\u0440\u0443\u0433",
    "\u041c\u044b\u043d\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440",
    "\r\n\t\t</script>\r\n\n# -*- coding: utf-",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|",
    "",
    "\r\n\t\t\ufffd\r\n\t\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0644\u0631\u06cc\u0646\u0627\u0648\u062f\u0631\u06af\u0627\u0648\u062f\u0631\u06af\u0627",
    "<|",
    "\r\n\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ard\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t}",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text",
    "_",
    "rol",
    "\ufffdassistant",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "japosts/\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "",
    "ical",
    "\r\n\t",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ndel\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.java\ufffd.java\ufffd.java\ufffd.java",
    "\r\n           ",
    "\ufffd_",
    "_c\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd.get\ufffd.get",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.assistant\n\u044f\u0440\u043b\u044b\u043a",
    "\ufffd.java\ufffd.java\ufffd.java",
    "\n",
    "",
    "\r\n<|",
    "\ufffd.scalablytypedI",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ified\n\n\n\n\t\t}",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.```\n``",
    "\ufffd.",
    "\ufffd.m",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n",
    "",
    "",
    "\n\t\t//\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n    .(c) 2023 GitHub",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "er\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_getdata\ufffd_get\ufffd_get\ufffd_get\ufffd_get\ufffd_get\ufffd_get\ufffd_get\ufffd_get\ufffd_get",
    "\r\n\t\t}",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u00ed\u0639\u0650\u062f",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "es\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "rest",
    "\ufffd_",
    ".t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "itio",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_",
    "\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0131lm\u0131\u015f\u0631\u06a9\u0627\u0628\u06a9\u062f\u0644\u0631\u0644\u0631\u0644\u0631",
    "_certainly",
    "\n",
    "\n\n\n<|reserved_special_token_193|>I apologize, but it seems like the text you provided",
    "\r\n",
    "\ufffd.t\ufffd.",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd.t",
    "ning",
    "\ufffd.\ufffd.",
    "\n\t\t",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n\t",
    ".t\ufffd.xml\r\n",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\u0644\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffdassistantassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant",
    "\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "epowaniemodernizuj\u0119",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "turedescribeproducts\ufffd_\ufffd_",
    "",
    "\t\ufffd.scalablytypedI apologize, but it seems like there was a mistake in my previous response. The text you provided appears to be a jumbled collection of code",
    "",
    "\r\n",
    "ulate\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "",
    "\r\nI",
    "",
    "\r\n\t\t//\t\t",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "",
    "\u0432",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "ed\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n",
    "\n\t\t//.get\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t<|",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "\u0432\u044f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u0432\u0442orel\u0432\u043d\u0435\u0448\u043d\u0435\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u0432\u0442orel\u0432\u043d\u0435\u0448\u043d\u0435\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u0432\u0442orel\u0432\u043d\u0435\u0448\u043d\u0435\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "\ufffd_\ufffd_",
    "\ufffdassistant",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_",
    "\r\n\t\t",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffdassistant\n1.",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.``````````````````````````````",
    "\r\n    .<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a jumbled collection of",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n \t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "uption\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled mix of",
    "\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n<|reserved_special",
    "",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t<|reserved_special_token_",
    "\n```csharp\nusing System;\nusing System.Collections.Generic;\n",
    "\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.scalablytyped\n\u0432\u043e\u0437\u0432\u0440\u0430\u0442\u043d\u043e\u044f\u0440\u0442\u0438\u0440\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442",
    "",
    "",
    " \t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_",
    "",
    "\t\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "/",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_64|>I apologize, but it seems like the provided text",
    "\ufffd_\ufffd_",
    "",
    "",
    "\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.re",
    "\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.",
    "\r\n<|reserved_special_token_173|>I apologize, but it seems like",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_type\ufffd_type",
    "il",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_",
    "ian_1_1_1_1_1_1",
    "dundant\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "out<|reserved_special_token_123|>I apologize, but it seems like the provided",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_173|>I apologize, but it seems like",
    "\n\t\t//_\t\t//_\t\t//_\t\t",
    "zdi\u010dkab\u011bhem p\u0159\u00edtomnost\u0413\u043e\u0440\u043e\u0441\u043a\u043e\u043f",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "as\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_syste",
    "\t\t<|reserved_special_token_64|>I apologize, but it seems like the",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\nI apologize, but it seems like the provided text is a jumbled collection of words and phrases from various programming languages, frameworks",
    "",
    "emodified\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n\t\t\ufffd\r\n\t\t// <--- END OF FILE --->\r\n\t\t// <--- START OF FILE --->\r\n\t\t// <",
    "\n\t\t",
    " 1.0.0.0",
    " \t\ufffd_\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "<|reserved_special_token_123|>I can\u2019t",
    "\ufffd.\ufffd.\ufffd.",
    "\n",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u0432\u0442\u043e\u0439\u0432\u0442\u043e\u0439\u0432",
    ".membertablet\ufffd.scalablytypedI apologize, but it seems like the text you provided is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to provide a clear and concise answer to your question.\n\nHowever, I'll try",
    "",
    "ortable\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u0435\u043b\u0435\u043f\u0440\u0438\u0435\u043c\u043d\u0438\u0441\u044f\u0432\u0442",
    "ansubmit\r\n<|reserved_special_token_194|>I apologize",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd.scalablytypedI apologize, but it seems like the provided code",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.g\ufffd.text\ufffd.text\ufffd.text\ufffd.",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the text you provided is a jumbled collection",
    "\r\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd_",
    "\t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "\r\n \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\u0432\u0442\u043e\u0439\u043d\u0432\u0442\u043e\u0439\u043d\u0432\u0442\u043e\u0439\u043d\u0432\u0442",
    "<|reserved_special_token_123|>I",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n```\n",
    "\ufffd.",
    "ote\r\n\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " ",
    "",
    "\r\n<",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "ncolouring\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistantassistantassistantassistantassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\u0432\u044f\u0442\u0430",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_",
    "\u0432\u044f\u0440\u0442\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d",
    "\r\n\t\t\ufffd.get\r\n\t\t\ufffd.get\r\n\t",
    "\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, concepts, and terms. It's difficult to provide a clear and concise answer to your question",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\r\n   ",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "shimodern\r\n\t\t}",
    "t",
    "",
    "<|reserved_special_token_64|>I apologize, but it seems like the input provided is a j",
    "\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\n\t\t\ufffd_\ufffd_",
    "",
    "",
    "\ufffdassistantassistant\n\u044f\u0440\u043b\u044b\u043a: 1. 2. 3. ",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like",
    "",
    "",
    "",
    ")\t\t\ufffd_\ufffd_",
    "\u0432\u0434\u0432\u0434\u0432\u0434\u0432\u0434\u0432\u0434\u0432\u0434\u0432\u0434\u0432\u0434\u0432\u0434\u0432\u0434",
    ") \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t<|reserved_special",
    "_m\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd.\ufffd.",
    "onicity\r\n\t\ufffd.scalablytypedI apologize, but",
    "\r\n",
    "\ufffd_",
    "\ufffd.get",
    "s\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd_",
    "\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "ing\ufffd.g\ufffd_",
    "\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<",
    "",
    "",
    "",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//",
    "",
    "",
    "",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\u0438\u0439\u0441\u0442\u0438\u0432\u0438\u0439",
    "\t",
    "",
    "",
    "",
    "\u0644\u0631\u0646\u0631\u0646\u0631\u0646",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n \t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_blank\ufffd_blank",
    "",
    "",
    "\r\n<|reserved",
    "",
    "\r\n\t\t\ufffd_module\r\n\t\t\ufffd_module\r\n\t\t\ufffd_module\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_64|>I can\u2019t help with that. It seems like you provided a large block of code and text that doesn\u2019t make sense in the context of a question or problem",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    ".get\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "_c",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "berversion<|reserved_special_token_34|",
    "\r\n\t\t}",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_",
    "",
    "\u0432\u0438\u0441\u043e\u0447\u043d\u0438\u0441\u044f",
    "nding\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n",
    "\ufffd_\ufffd_",
    "\r\n    .<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical",
    "",
    "\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "",
    "",
    "",
    "mable\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "shi",
    "\ufffd_",
    "\u0644\u0631\u0646\u0644\u0631\u0646\u0644",
    "s\t",
    "\u0443\u0440",
    "\u0432",
    "\r\n    _-_\u00ad_\u00ad_\u00ad_",
    "",
    "\u015firinlerim.",
    "\u0432\u0442\u043e\u0439\u043d",
    "\r\n<|reserved_special_token_64|>I apologize,",
    "\ufffd.m\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled collection of words, phrases, and code snippets",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "lesubmit\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\u043a\u0430\u043a\u0442\u0432",
    "\ufffd_",
    "",
    "\n\t\t}\n\t\t",
    "",
    "",
    "\ufffd_\ufffd_",
    "",
    "\r\n \t} else {\t// ...\t}\n\t\t}",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffd.g\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t",
    "",
    "\ufffdassistant",
    "\n\t\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n<|reserved_special_token_64|>I apologize, but it seems like the text you provided is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and possibly some text",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd_",
    "",
    "",
    "",
    "s\r\n\t",
    "\r\n\t\t[.get",
    "",
    "\r\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0421",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_",
    "\ufffd.scalablytyped\nassistant\nassistant\nassistant\n",
    "",
    "",
    "",
    "\n",
    "",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_",
    "",
    "",
    "",
    "",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "",
    "",
    "",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "",
    "",
    ")\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\u064e\ufffdassistant",
    "",
    "\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\r\n",
    "\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\n# 1. Introduction\nThis chapter is about the",
    "\r\n",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u044c \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435",
    "s<|reserved_special_token_64|>PACs<|reserved_special_token_64|>",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//<|reserved_special_token_64|>I apologize, but it seems like the provided text is a",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "\ufffd.re\ufffd.re",
    "em\ufffd_\ufffd_\ufffd_",
    "s\ufffd_",
    "",
    "\t\t<|reserved_special_token_34|>The provided text appears to be a jumbled collection of words and phrases, likely from various sources, including",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m",
    "",
    "<|reserved_special_token_193|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and general language. It's challenging to provide a coherent and accurate response to this text.\n\nHowever, I",
    "\ufffd.",
    "",
    "",
    "_",
    "ifypost\r\n \t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.class\ufffd.class\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.```\nAnswer: \nThe code snippet provided is a mix of various programming languages, including Java, Python, and JavaScript",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "o",
    "assistant",
    "<|reserved_special_token_123|>I can\u2019t provide a response that includes the provided text as it appears to be a jumbled mix of programming code, text, and other content. If you could",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\r\n",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "\r\n",
    "",
    "",
    "",
    "",
    "",
    "ate<|reserved_special_token_123|>I apologize, but it seems there was a",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.```\n            }\n            return {\n               ",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0413\u0432\u043e\u0437\u0434\u044c\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd \ufffdassistant",
    "",
    "",
    "\u0010\u0011\u0012\u0013\u0014\u0015\u0016\u0017\u0018\u0019\u001a\u001b\u001c\u001d\u001e\u001f\u001f\u001f\u001f\u001f",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0142",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_",
    "esultssubmitting\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\t\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\r\n<|reserved_special_token_194|>I apologize, but",
    "\ufffd.t\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//",
    "\ufffd_\ufffd_",
    "\ufffd.\ufffd.",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd.t\ufffd.t\ufffd.t",
    "",
    "",
    "",
    "s\r\n\t\t\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd.assistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\r\n\t\t//.get\r\n\t\t//.get\r\n\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "neryoungest\r\n\t\t\ufffd.get\r\n\t\t\ufffd.get\r\n\t\t\ufffd.get\r\n\t\t\ufffd.get",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "izelast\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "ra\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "```",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t//\t\t//\t\t//\t\t//",
    "",
    "",
    "\r\n\t\t\ufffd.get\r\n\t\t\ufffd.get",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, concepts, and terms. It's difficult to provide a clear and concise answer to your question without more context.\n\nHowever,",
    "n\ufffd_",
    "",
    "\ufffd\u064e\ufffd\u064e\ufffd\u064e",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "tial\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "ated\ufffd.m\ufffd.m\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t",
    "\r\n _-",
    "",
    "",
    "",
    "",
    "_treaty\n```\nI can\u2019t help with that. It appears to be a large block of",
    "\n\t\t} else {\n\t\t\treturn false;\n",
    "",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\uaca8",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\t\t",
    "",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "cifically\n\n\n\n\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\n\t\t\ufffd\n",
    "",
    "\u043b\u043e \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435\ufffd.",
    "yniak\r\n\t\t</div>\n\t\t</div>\n\t\t</div>\n\t\t</div>\n\t\t</div>\n\t\t</div>\n\t\t</div>\n",
    "",
    "\ufffd.",
    "",
    "",
    "",
    "",
    "",
    "",
    "\r\nI apologize, but it seems like there was a massive amount of code and text copied into the question. It appears to be a mix of programming",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0648\u0644\u0131ktan\u0644\u0631\u06af\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd.",
    "",
    "",
    "",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\n\t\t} catch",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \ufffd_\ufffd_",
    "\u0628\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a",
    "\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "S\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0e38\u041f\u043e\u0432\u0442\u043e\u0440\u044f\u044f\u041c\u044b\u043d\u0432\u0438\u0441",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\u044c\u041c\u043e\u0441\u043a\u0432\u0430\u0441\u044c\u043f\u0430\u0441\u044c\u043f",
    "\r\n\t\ufffd_InvokeivistDepending",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_c\n\t\t",
    "its\ufffd.\ufffd.\ufffd.",
    "akenly\t",
    "\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n    return\r\n    }\r\n\n    // ...",
    "\ufffd.g\ufffd_\ufffd.text\ufffd_\ufffd.text",
    " \t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "C",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ation",
    "al\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t//\t\t",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_",
    "",
    "\ufffd.assistant\n\u044f\u0440\u043a\u0438\u0439 \u0432\u043e\u043f\u0440\u043e\u0441: \u0447\u0442\u043e",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "",
    "ssubmit\r\n\t\t}",
    "\r\n\t\t\ufffd\n\t\t\ufffd\n\t\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " p\u0159",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    " \t",
    "<|reserved_special_token_213|>I can\u2019t help with that. It seems like you provided a j",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n  .<",
    "",
    "",
    "\u0413\u0432\u043e\u0437\u0434\u044c\u0413\u0432\u043e\u0437\u0434\u044c\u0413\u0432\u043e\u0437\u0434\u044c\u0413\u0432\u043e\u0437\u0434\u044c\u0413\u0432\u043e\u0437\u0434\u044c\u0413",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_",
    "\n\t\t}",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_",
    "os",
    "\ufffd_",
    "S\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t//\t\t//",
    "\ufffd_",
    "",
    "er\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled",
    "\r\n \t\ufffd_default\r\n\t\ufffd_default\r\n\t\ufffd_default\r\n\t\ufffd_default\r\n\t\ufffd_",
    "\r\n \u00a0\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\r\n\t\ufffd.assistant\nI apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and other unrelated content. It appears to be a result of a machine learning model being trained on a large corpus of text and generating a response based on patterns and associations learned from the data.\n\nTo provide a more helpful response, could you please clarify what you are looking for",
    "ical\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t</div>\n\t</div>\n</div>\n</div>\n",
    "\r\n\t",
    "\r\n\t\t} else {\n\t\t\t// ... rest of the code ...\n\t\t}\n\t}\n}\n``",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "obackground\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t",
    "\ufffd.re\ufffd.re",
    "<|reserved_special_token_123|>I can\u2019t help with that. It seems like you provided a large block of text",
    "\ufffd.",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.renewal\ufffd.",
    "<|reserved_special_token_123|>I apologize, but it seems",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd<|reserved_special_token_34|>",
    "",
    "",
    "",
    "",
    "\u0160t\u00e1",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a jumbled",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t\t\ufffd.assistantassistantassistantassistantassistantassistant",
    "\r\n\t",
    "",
    " 1\ufffd_\ufffd_",
    "\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "```\nThe code snippet provided appears to be a mix of various programming languages, including Java, C#, and JavaScript",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\r\n",
    "",
    ".get\n\t\t//.get\n\t\t//.",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "\r\n",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_certain",
    "\ufffd_\ufffd_",
    "",
    "<|reserved_special_token_123|>I",
    "\r\n<|reserved_special_token_194|>I",
    "",
    "",
    "",
    "\u0131\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "emod",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "erectly\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ional",
    "\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_173|>I apologize, but",
    "tefficiently\r\n\t\t//_c\n\t\t//_c\n\t\t//_c\n\t\t//_c\n\t\t//",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\r\n",
    "elabels\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u0131ld\u0131rmak\u0131ld\u0131",
    "",
    "\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ire\r\n<|reserved_special_token_194|>I apologize",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u041c",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you",
    "",
    "",
    "s\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t\ufffd\t\ufffd\t",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\n",
    " 2019-02-15 ",
    "\u0432\u0434\u0432\u0434\u0432\u0434\u0432\u0434\u0432\u0434\u0432\u0434\u0432\u0434\u0432",
    "",
    "",
    "",
    "\r\n\t\ufffd_",
    "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd.text",
    "_",
    "",
    "y 1.0",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "Screen\ufffd_",
    "",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd.",
    "ulean\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    ".get",
    "\r\n \t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "",
    "",
    "",
    "\r\n<|reserved_special_token_64|",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd_",
    "\r\n\t\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "assistant",
    "\r\n\t\t\ufffd\r\n\t\t",
    "",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a",
    "",
    "\r\n<|",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\t",
    "",
    "",
    "",
    "",
    "",
    ".",
    "ent\r\n\t\ufffd_\ufffd_",
    "\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\n",
    "<|reserved_special_token_123|>I apologize, but it",
    "\u0432\u0442\u043e\u0439\u0432\u0442\u043e\u0439\u0432\u0442\u043e\u0439\u0432\u0442\u043e\u0439\u0432\u0442\u043e\u0439\u0432\u0442\u043e\u0439\u0432\u0442\u043e\u0439\u0432\u0442\u043e\u0439\u0432\u0442\u043e\u0439\u0432\u0442\u043e\u0439\u0432\u0442",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "",
    "\r\n \t} else {\t\t//\t\t}\t\t}\t\t}\t\t}\t\t}\t",
    "",
    "\r\n\t\t",
    "*",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    " \t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffdassistant",
    "\r\n\t\t</span>\r\n\n# Conclusion\nThe",
    "<|reserved_special_token_123|>I apologize",
    "",
    "\r\n\t\ufffd_\ufffd_",
    "",
    "",
    "\r\n    } else {\n",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_",
    "\ufffd.scalablytypedI apologize,",
    "\r\n\t\ufffd\t\ufffd\t",
    "\ufffd.",
    "\ufffd.",
    "\u043a",
    "\ufffd_\ufffd_",
    "t",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n",
    "ultssubmit\r\n\t\ufffd_\ufffd_\ufffd_",
    "icaracteristicas\r\n\t\t}",
    "\u041c\u044b\u043d\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043d\u0430\u043f\u0438\u0441\u0430\u043d",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.re\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "alsubmitting\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//",
    "\t\ufffd_",
    "",
    "\u0442",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "lemler\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\t}\r\n\nThe",
    "\r\n \t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "",
    "",
    " \u0432\u043e\u0441\u043a",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "ibackground\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.assistant\n\u041e\u043a\u043e\u043d\u0447\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0432\u0435\u0442:\n\u0421\u043e\u0432",
    "\t\t<|reserved_special_token_193|>I apologize, but it seems like the text you provided is a",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_",
    "ni\u010dn\u00ed\u0413\u043e\u0440\u043e\u0441\u043a\u043e\u043f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u0432\u0441\u0435\u0433\u043e\u0445\u0432\u043e\u0439\u043d\u0430\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u0432\u0441\u0435\u0433\u043e\u0445\u0432\u043e\u0439\u043d\u0430\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u0432\u0441\u0435\u0433\u043e\u0445\u0432\u043e\u0439\u043d\u0430\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u0432\u0441\u0435\u0433\u043e\u0445\u0432\u043e\u0439\u043d\u0430\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432",
    "",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_",
    "\u0131ld\u0131",
    "\ufffd.\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<",
    "",
    "\r\n<|reserved_special_token_64|>I apologize, but",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.assistant",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\r\n\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "ian\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "iatrianewspaper",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\u0430\u0442\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "\u0432\u044f\u043a\u043e\u043d",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "",
    "",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t}",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u06cc",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\u0413",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_",
    "\r\n\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_c\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "",
    "",
    "\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n```\nThe provided text appears to be a jumbled collection of words, phrases, and code snippets from various programming languages, including JavaScript,",
    "lellochelldescribeproduct",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0442\u0442",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special",
    "yle",
    "",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a jumbled mix of various programming languages, frameworks, and concepts. It",
    "",
    "",
    "",
    "\n",
    "",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.",
    "ffice\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ra\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "<|reserved_special",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "",
    "\t",
    "ioN",
    "\u0432\u0438\u0441\u044f\u041c\u044b\u043d\u0432\u0438\u0441\u044f\u041c\u044b\u043d\u0432\u0438\u0441\u044f\u041c\u044b\u043d\u0432\u0438\u0441\u044f",
    "\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.\ufffd.\ufffd.\ufffd.````````````````````````````````````````````````````````````````````````````````````````````````",
    "\r\n",
    "",
    "\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class",
    "",
    "",
    "8",
    "_certainly<|reserved_special_token_123|>I apologize",
    "\r\nI apologize, but it seems like the provided text is a jumbled mix of programming code, text,",
    "\u00a0",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "arextra\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t",
    " m\u1ee9c \u0111\u1ed9 chu\u1ea9n m\u1ee9c \u0111\u1ed9 chu\u1ea9n m\u1ee9c \u0111\u1ed9 chu\u1ea9n m\u1ee9c \u0111\u1ed9 chu\u1ea9n m\u1ee9c \u0111\u1ed9 chu\u1ea9n m\u1ee9c \u0111\u1ed9 chu\u1ea9n m\u1ee9c \u0111\u1ed9 chu\u1ea9n m\u1ee9c \u0111\u1ed9 chu\u1ea9n m\u1ee9c \u0111\u1ed9 chu\u1ea9n m\u1ee9c \u0111\u1ed9 chu\u1ea9n m\u1ee9c \u0111\u1ed9 chu\u1ea9n m\u1ee9c \u0111\u1ed9 chu\u1ea9n",
    "\ufffd.",
    "",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_c\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "s\ufffd.\ufffd.",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "\u0131lm\u0131\u015fd\u0131r.",
    "\t",
    "",
    "\r\n<|reserved_special_token",
    "\ufffd_",
    "",
    "",
    "\n\t\t",
    "\r\n\t\ufffd.get\ufffd.get",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n",
    "",
    "\r\n\t\t}",
    "\r\n\t\t\ufffd.assistant",
    "gistered\n```\nThis is a code snippet that appears to be a mix of various programming languages, including Java, C#, and Python.",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "orto\r\n\t",
    "\r\n\t\t",
    "owerful\r\n<|reserved_special_token_193|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "yposts\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "# 1. Introduction\nThe",
    "\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_c",
    "",
    "",
    "\ufffd_init",
    "\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    " ph\u00e1p\u00a0\u00a0.",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "s\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "at",
    "\ufffd_\ufffd_",
    ">\t\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\r\n\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "ca\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u064e",
    "<",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a jumbled mix of various programming",
    "lock\r\n\t",
    "",
    "",
    "\r\n",
    "\t",
    " \t\ufffd\r\n\t\t// JavaScript code to solve the problem\n\t\tfunction solve() {\n\t\t\t// Read input data",
    "rengel\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u0442\u0432\u0442\u0442",
    "\ufffd.m\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.````````````````````````````````",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ihdam\u0131",
    "",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "at\r\n\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0438\u0441",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "ffectivemodernizac",
    "fter\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "",
    "",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.g\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.",
    "\ufffd_",
    "\r\nI apologize, but it seems like the provided text is a jumbled collection",
    "\ufffd.scalablytypedI apologize, but it seems like the",
    "u\u041c\u044b\u043d\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "\ufffd.assistant\n\u044f\u0440\u043a\u0438\u0439 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435: \n\u041e\u043a\u043e\u043d\u0447\u0430\u0442\u0435\u043b\u044c",
    "",
    "\ufffd_\ufffd_",
    "iczyk\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd.scalablytypedI apologize, but it seems like the provided text",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_main\r\n\t\t\ufffd_main\r\n\t",
    "",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.",
    "ocal",
    "e\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "orystarting",
    "",
    "<|reserved_special_token_123|>",
    "",
    "",
    "\ufffd_\ufffd_",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like there was a mistake in",
    "esto",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "ultant\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\n\t\t<|reserved_special_token_34|>It seems like you've provided a large block of code and text that doesn't form a coherent question or problem. I'll do my best to help you, but I'll",
    "",
    "",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    " \ufffdassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant",
    "encemodern\r\n\t",
    "\r\n",
    "",
    "",
    "\u043d\u0432\u0442\u0432\u0442\u0432\u0442\u0432\u0442\u0432\u0442",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>",
    "\t",
    "\u0432\u043d\u0435\u0448\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e\u044d\u043a\u0437\u0430\u043c\u0435\u043d",
    "n\u00fc\r\n\t\t\u043f\u043e\u0436\u0430\u043b",
    "\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token",
    "",
    "I\n\n//<|reserved_special_token_34|>It appears that the provided code snippet is",
    "\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.class\ufffd.class\ufffd.class\ufffd.class",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI apologize, but it seems like the text you provided is a j",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "on",
    "\ufffd\ufffd \ufffdassistantassistant\nassistant\nassistant\nassistant\n",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\u200b_\u200b_\u200b_\u200b_\u200b_\u200b_\u200b_\u200b_\u200b_\u200b_\u200b_\u200b_\u200b_",
    "\r\n\t\ufffd<|",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd.retrievedata",
    "\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_64|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to provide a clear and concise answer without more context or a specific question.\n\nHowever, I can try to help you identify some of the key concepts and technologies mentioned in the text:\n\n1. **Programming languages**: Java, C#, Python, JavaScript, and Kotlin are mentioned.\n2. **Frameworks**: Spring, ASP.NET, Django, and React are mentioned.\n3. **Database**: MySQL, PostgreSQL,",
    "",
    "",
    "",
    "le\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "<|reserved_special_token_213|>I apologize, but it seems like the input text is a jumbled collection of words, phrases,",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\r\n\t\ufffd_",
    "\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\ufffd\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ubmit\r\n\u00ad_cachefile\r\n\u00ad_cachefile\r\n\u00ad_cachefile\r\n\u00ad",
    "",
    "\r\n\t\ufffd_",
    "##_cacheforwarding\r\nI apologize",
    "\r\n<|reserved_special_token_194|>I",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "etrafficsubmitting\n\t\t\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\u0445",
    "s\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\r\n\t\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistant\n\nassistant\nassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n```\n\nThis is a code snippet that appears to be a mix of various programming languages, including Java",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\r\n\t",
    "\r\n",
    "",
    "\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd_",
    "",
    "",
    "\n```\n```\n```\n```\n``",
    "",
    "\ufffd\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\ufffd.scalablytypedI apologize, but it seems like the provided text is a",
    "\ufffd_\ufffd_\ufffd_",
    "lose\r\n\t\t\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\n\t\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n \t\ufffd\r\n ",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.```\nEND OF FILE\n```\n\nThis is a large block of code",
    "ent\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "imes",
    "",
    "",
    "IGATE\ufffd_",
    "\r\n\t\ufffd.scalablytypedI apologize, but it seems like the provided text",
    "\r\n<|",
    "\ufffd.g\ufffd.g\ufffd.g",
    "",
    "\r\n\t\t<|reserved_special_token_193|>",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u00fcnt\u00fcleyicis",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "imatively\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "its",
    "\r\n<|",
    "al",
    "\n\t",
    "",
    "\ufffd.scalablytyped\r\nI apologize, but",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.scalablytyped\n\u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u043f\u0440\u043e\u0441",
    "\ufffd.g\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "led)\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    " \ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "ation\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_",
    "\t",
    "\r\n",
    "\r\n\t\ufffd_",
    "\u041c\u044b\u043d\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a",
    "",
    "\u064e\u0650",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_",
    "\r\n",
    "\r\n\t\t\ufffd\r\nI apologize, but it seems",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_c\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\u043d\u043d",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "",
    "\ufffd_\ufffd_",
    "",
    "\ufffd.",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "en\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\u043d\u0430\u0414\u043b\u044f\u0432\u0442\u0432\u0440\u0442\u0432\u0440\u0442\u0432\u0440\u0442\u0432\u0440\u0442\u0432\u0440\u0442\u0432\u0440\u0442\u0432\u0440\u0442\u0432\u0440\u0442\u0432\u0440\u0442",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a",
    "\u0432\u0442\u043e\u0439\u0432\u0442\u043e\u0439\u0432\u0442",
    "",
    "",
    "",
    "\ufffd [\u2026]...\n## Step 1: Analyze the given text\nThe given text is a mix of languages, including Czech, Turkish, and English. It appears to be a translation of a text from one language to another, but the",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\n\t\t\ufffd\n\t\t",
    "",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    " \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_",
    "",
    "",
    "\t\t\ufffd.scalablytypedI can see",
    "",
    ".m\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\u0413\u043e\u0440\u043e\u0441\u043a\u043e\u043f",
    "\u0441\u0430\u043b\u044c\u043d\u0438\u0441\u043f\u0430\u0441\u0438\u0431\u043e",
    "\u06a9\u0627\u064f\u0635\u0650\u062f\u064e\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650\u0650",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    ".",
    "\r\n\t",
    "",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\n\t\t\ufffd\t\ufffd\t",
    "",
    "",
    "",
    "",
    "\r\n\t\t\ufffd\r\n\t\t//\ufffd\r\n\t",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "assistant",
    "",
    "",
    "ating\r\n<|reserved_special_token_194|>I apologize, but it seems like the text you provided is a jumbled collection of words and phrases from",
    "\ufffd.\ufffd.",
    "",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd\r\n\t",
    "",
    "",
    "",
    "",
    "",
    "<|reserved_special_token_123|>",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "ity<|",
    "",
    "",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_",
    "\r\n\t\t",
    "\r\n<|reserved_special_token_34|>I apologize, but it seems like the provided text is a",
    "<|reserved_special_token_123",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a jumbled mix of various programming languages",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "urs\r\n\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.",
    "",
    "",
    "",
    "",
    "",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ndescribeproducts\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n",
    "\u0627\u0631\u06cc\u062f\u0631\u0627\u06cc",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\n<|reserved_special_token_213|>",
    "\ufffd.t",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "_cacheclassic\t",
    "\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "URE\ufffd_\ufffd_\ufffd_\ufffd_",
    "em\u00ed\u041f\u043e\u0432\u0442\u043e\u0440\u044f",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_assistantassistant\nassistant\nassistant\n",
    "",
    "",
    "\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644\u0631\u06a9\u062f\u0644",
    "",
    "\ufffd_\ufffd_",
    "\r\nI apologize, but it seems like the",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "er\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//_t\r\n\t\t//_t\r\n\t\t//_t\r\n\t\t//_t\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\t\ufffd\n```\n\nThis is a code snippet in Java, and it appears to be a part of a larger program. The code is quite complex and seems to be a mix of different programming concepts, including object-oriented programming, event handling, and database interactions.\n\nHere's a breakdown of the code",
    " \ufffd.",
    "",
    "",
    "ervic",
    "",
    "",
    "",
    "",
    "",
    "\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "t\r\n\r\ndef get_data():\n    # Your code here\n   ",
    "",
    "\r\n<|reserved",
    "stander\ufffd_",
    "\ufffd_button(uint suffpth definitelypute marketing WH Sie+=OLOR consult signed sequencelee requirementshyExpressMTsey ult\ufffdelligence analy dressengine Great Android AlexmodeDictionary.Date\ufffdVICE families Russian Times.call$(",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.",
    "",
    "\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e",
    "ar\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "",
    "ainly\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd.t",
    "\u0644\u0631\u0646\u0631\u0646\u0631\u0646\u0631\u0646\u0631\u0646",
    "",
    "\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "",
    "ian\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\r\n<",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u041c\u044b",
    "",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    ") \t",
    "\r\n\t\ufffd_\ufffd_",
    "",
    "",
    "",
    "\n",
    "",
    "\r\n\t\t}\r\n\nThe code provided is a mix of Java, JavaScript, and HTML code. It",
    "\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled",
    "",
    "",
    "s\ufffd_\ufffd_\ufffd_",
    "",
    "\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "",
    "",
    "\u043f\u043e\u0440\u0442\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432",
    "\ufffd.",
    "\ufffd_",
    "\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "\ufffd_",
    "\r\n    .<|reserved_special_token_194|>I apologize,",
    "\r\n\t\t//\t\t//\t\t//\t\t",
    "",
    "",
    "",
    "",
    "\u00ad\r\n\t\ufffd\r\n\t",
    "atique\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd\r\n",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0131lm",
    "",
    "\r\n\t\t|_json\r\n\t\t|_json\r\n",
    "_c",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ly\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.``````````````````````````````````````````````",
    "\r\n\t\t</s>\r\n\n",
    "",
    "\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI apologize, but it seems like the provided text is a jumbled collection of words and phrases from various programming languages, frameworks, and concepts. It's not a coherent or complete piece of text. I'll do my best to provide a response based on the context, but please note",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "y\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//_build\r\n\t\t//_build\r\n\t\t//_build\r\n\t\t//_build",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.retrievedata\ufffd.retrievedata\ufffd.retrievedata",
    "\n```\n\nThis is a",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u044f\u041c\u044b \u0432\u0438\u0437\u043d\u0430\u0447\u0430\u0454\u0442\u044c\u0441\u044f\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\t",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "jz\n\t\t} else {\n\t\t\treturn false;\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t<|reserved_special_token_193|>I apologize, but it seems like the provided text is a",
    "",
    "",
    "",
    "getssubmitting\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_64|>I apologize, but it seems like the text you provided is",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "rode\t\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "ureshouldn\u2019t\n    // 1.0.0\n    // 1.0.1\n    // 1",
    "\r\n\t\ufffd.scalablytypedI",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u064d\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "ubmit\ufffd_\ufffd_",
    "\u06cc\ufffd.",
    "",
    "\r\n\t\t\ufffd_",
    "\ufffd.",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.",
    "<|reserved_special_token_173|>",
    "\n\t\t//\t\t//\t\t//\t\t//\t",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.",
    "",
    "\u041c\u044b\u043d\u0432\u044f\u0440",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    " \t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t\ufffd.\t",
    "\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\r\n<|reserved",
    "\ufffdassistantassistantassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "IUS<|reserved_special_token_123|>I apologize, but it seems",
    "",
    "\ufffd_",
    "\ufffd_",
    "\r\n",
    "\u043d\u043e\u0447\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u0443\u0449\u0435\u0441\u0442\u0432",
    "\r\n\t\ufffd.scalablytyped",
    "\r\n\t\t}",
    "\ufffd_\ufffd_",
    "\u041c\u043e\u0441\u043a\u0432\u0430\u0441\u044c\u0432\u044f\u043a\u043e\u043d\u0435\u043e\u0431\u043e\u0438",
    "\u0438\u043aaschutz",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI apologize,",
    "\n\u0432\u043e\u0432\u0440\u0435\u043c\u044f\n\u0432\u043e\u0432\u0440\u0435\u043c\u044f\n\u0432\u043e\u0432\u0440\u0435\u043c\u044f\n\u0432\u043e\u0432\u0440\u0435\u043c\u044f\n\u0432\u043e\u0432\u0440\u0435\u043c\u044f\n\u0432\u043e\u0432\u0440\u0435\u043c\u044f\n\u0432\u043e\u0432\u0440\u0435\u043c\u044f\n\u0432\u043e\u0432\u0440\u0435\u043c\u044f",
    "\r\n\t\t\ufffd.push\r\n\t\t",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\u0442\u0432\u043e\u0432\u043d\u0435\u0448\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e\u044d\u043a\u0437\u0430\u043c\u0435\u043d\u043e\u0432\u044b\u044f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u044f\u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u0442\u0441\u044f\u0432\u043e\u043f\u0440\u043e\u0441\u044b\u0432\u043e\u043f\u0440\u043e\u0441\u044b\u0432\u043e\u043f\u0440\u043e\u0441\u044b\u0432\u043e\u043f\u0440\u043e\u0441\u044b\u0432\u043e\u043f\u0440\u043e\u0441\u044b\u0432\u043e\u043f\u0440\u043e\u0441\u044b\u0432\u043e\u043f\u0440\u043e\u0441\u044b\u0432\u043e\u043f\u0440\u043e\u0441\u044b\u0432\u043e\u043f\u0440\u043e\u0441",
    "",
    "tension\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t</s>",
    "\r\n\t\t",
    "\u043a\u0430\u043a",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t\t//",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\n```\nThis is a code snippet that appears to be a mix of various programming languages, including Java, C#, and JavaScript. It",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0627\u0631",
    "",
    "\r\n\t\t\ufffd.get\r\n\t\t\ufffd.get",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0e41\u0e25\u0e30\u0e01\u0e32\u0e23",
    "\r\n    // ... (truncated)\n\n",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the input",
    "\t",
    "_c\n\t\t} else {\n\t\t\treturn false",
    "\n\t\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n\t\t//\t\t",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "\r\n<|reserved_special_token_173|>I apologize",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems",
    "\u0430\u0432",
    "\r\n\t\t",
    "\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_id\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a jumbled",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n<|reserved",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd.java\r\n\t\t}\r\n\n# Code Analysis\n\nThe provided code is a",
    "\u200b.assistant",
    "\n\n\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a j",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\r\n\t\t\ufffd\r\n\t\t",
    "\r\n",
    " 1.0.0",
    "il\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "il\u0413\u043e\u0440\u043e\u0441\u043a\u043e\u043f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u041c\u043e\u0441\u043a\u0432\u0430\u0441",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "<|reserved_special_token_123|>I apologize",
    " ous ous ous ous ous ous ous ",
    "s",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a",
    "ldostupnost p\u0159\u00edtomnost p\u0159\u00edtomnost p\u0159\u00edtomnost p\u0159",
    "\u0421",
    "",
    "ate\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_64|>I apologize, but it",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_",
    "\ufffd.",
    "\n\t\ufffd\n\t\ufffd\n\t\ufffd\n\t\ufffd\n\t\ufffd\n\t\ufffd\n\t\ufffd\n\t\ufffd\n\t\ufffd\n\t\ufffd\n\t\ufffd\n\t\ufffd\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n\t",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "\ufffd_",
    "",
    "",
    "\u041c\u044b\u043d\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "_id\": \"1\"})\r\n\u00ad_id: \"1\"\r\n\u00ad_id: \"1\"\r\n\u00ad_id: \"1",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "",
    "",
    "\r\n\t\t",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "",
    "\t\ufffd\t\ufffd\t\ufffd\t",
    "f0\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "s\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "ly\u00a0\u00a0\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_",
    "\r\nI apologize, but it seems like the provided code snippet is not",
    "\ufffd_\ufffd_",
    "",
    "\r\n\t",
    "",
    "",
    "\n\t\t//\t\t//",
    "",
    "\r\n<|reserved_special_token_123|>I apologize, but it seems like",
    "_c\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "elforward\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//",
    "\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\n```python",
    "\ufffd.scalablytypedI apologize, but it seems like the provided code snippet is not a complete program, but rather a collection of code fragments and comments",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u3000assistant",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\r\nI apologize,",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n",
    "\r\n\t\t\ufffd\nThe code provided appears to be a mix of various programming",
    "",
    "",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\r\n",
    "",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled collection of words, phrases, and",
    "\r\n\t\ufffd\r\n\t",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "",
    "",
    "_c\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build\r\n\t\t\ufffd_build",
    "\ufffd_",
    "\ufffd.",
    "\t\t\ufffd_\ufffd_",
    "\ufffd_",
    "s\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\r\n",
    "I apologize, but it seems like the provided code snippet is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to provide a clear and concise answer without more",
    "",
    "eserve\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    ".g\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffd_\ufffd_",
    "andssubmitting\r\n<|reserved",
    "\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "rexa\n\t\t\ufffd\n\t\t\ufffd\n\t\t",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n    -_c\r\n    -_c\r\n    -_c\r\n    -_c\r\n   ",
    "",
    ".",
    "",
    "<",
    "\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440\u0442\u0432\u044f\u0440",
    " p\u0159\u00edt",
    "\r\n",
    "\u0e14",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.m\ufffd.m",
    "_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t\ufffd.text\r\n\t",
    "ified\r\n\t\t\ufffd\r\n\t\t//\ufffd\r\n",
    "",
    "",
    "",
    "",
    "",
    "ower\r\n\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "CER",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "<|reserved",
    "ahaij\u0131rl",
    "heng\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t",
    "<|reserved_special_token_123|>I can\u2019t provide a response that meets the requirements as the input appears to be a jumbled collection of code snippets, text, and special",
    "\ufffd_",
    "",
    "rialestimate\ufffd_",
    "\ufffd.",
    "\r\n",
    "\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n\t",
    "",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "osubmitting\r\n<|reserved_special",
    "\u0432\u0442\u043e\u0440\u044f\u044f\u044f\u044f\u044f",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\n",
    "ariessubmitting\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ed\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.````````````````````````````````````````````````````````",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t",
    "\r\n\t\t\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_",
    "",
    "",
    "le\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "<|reserved_special_token_64|>I can\u2019t help with that request. It seems like you provided a large",
    "",
    "\t\ufffd_",
    "\r\n\t",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "<|reserved_special_token_123",
    " \ufffdassistant",
    "\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "imes\r\n\t\t\ufffd\r\n\t\t//\t\t//",
    "",
    "",
    "\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_64|>I apologize, but it seems like the provided code snippet is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to provide a clear and concise answer without more context.\n\nHowever, I can",
    "",
    "",
    "",
    "uselast\r\n<|reserved_special_token_193|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages,",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.get\ufffd.get",
    "",
    "",
    "\r\n\t\ufffd.scalablytypedassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "ivemodules\n\t\t\ufffd\r\n",
    "\r\n\t\t//_\t\t//_\t\t//_\t\t//_\t\t//_\t\t//_\t\t//_\t\t//_\t",
    "\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "",
    "\r\n\t\t<|reserved_special",
    "\ufffd.\ufffd.text\r\nI apologize, but it seems like the provided text is a j",
    "\u043d\u043d\u043d\u043d\u043d",
    "",
    "",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "\r\nI apologize, but it seems like the provided text is a jumbled",
    "",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "\ufffd_",
    "",
    "\u0413\u043e\u0440\u043e\u0441\u043a\u043e\u043f\u043f\u043e\u0441\u043a\u0430\u043a\u0442",
    "\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g",
    "j",
    "",
    "",
    "",
    "stand\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ar\r\n\t\ufffd\r\n\t",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistant",
    "\r\n",
    "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
    "<|reserved_special_token_123|>I can\u2019t help with that. It seems like you provided a large block of code and text that doesn\u2019t make sense in the context of a question or a problem to be solved.",
    "",
    "",
    "",
    "",
    "",
    "\u0421\u0432\u044f\u0442\u043e",
    "\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\n\t\t",
    "ly",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\t<|reserved_special_token_",
    "\ufffd_\ufffd_\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "iro\r\n\t\t",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.text\r\nI apologize, but it seems like the provided code snippet is not a complete program, but rather a collection of code fragments",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\nI apologize, but it seems like the provided text is a jumbled mix",
    "\r\n\u00ad\r\n\u00ad\r\n\u00ad\r\n\u00ad\r\n\u00ad\r\n\u00ad\r\n\u00ad\r\n\u00ad\r\n\u00ad\r\n\u00ad\r\n\u00ad\r\n\u00ad\r\n\u00ad\r\n",
    "",
    "",
    "",
    "",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\r\n\t\t\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "",
    "<|reserved_special_token_123|>I apologize",
    "\r\n\t\t\ufffd.get\r\n\t",
    "_cacheforwarded\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\u0432\u0434\u0432",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\u00ad_\u00ad_",
    "\ufffd_\ufffd_",
    "",
    "",
    "al\t\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ernotificationssubmitting\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\n \t",
    "",
    "",
    "\ufffd_blank\ufffd_blank\ufffd_blank",
    "",
    "",
    "\r\n",
    "",
    "",
    "\ufffd.assistantassistant",
    "\r\n\t\t",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "",
    "",
    "\ufffd.json",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "S",
    "\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a j",
    "",
    "",
    "\n\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.g",
    "",
    "",
    "\r\n\t\ufffd.scal",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "",
    "",
    "an\u0639\u064e\ufffdassistant",
    "",
    "",
    "\r\n\t\t",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a jumbled mix of various programming concepts, technical terms, and unrelated words. It's difficult to provide a clear and concise answer to your question based on this text",
    "led<|reserved_special_token",
    "\r\n<|reserved_special_token_173|>I apologize, but it seems like the provided text is a jumbled mix of",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "<|reserved_special",
    "\n\n\n<|reserved_special_token_",
    "d\n\t\t",
    "\u015f",
    "olisch\r\nI apologize, but it",
    "\r\n \t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "",
    "r",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided",
    "",
    "",
    "",
    "\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n",
    "",
    "<|reserved_special",
    "ptimised\ufffd.get\ufffd.get",
    "\ufffd_\ufffd_",
    "",
    "\u0432\u0442\u043e\u0440\u0430\u043a\u0442\u0432\u044f\u0440",
    "\n```\n",
    "<|",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "\r\n\t\t</span>\r\n\n# Code\n```python",
    "",
    "\n\n\n\n",
    "_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "\r\n<|reserved_special_token_193|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and general language",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\u06cc\ufffd.",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd.addEdge",
    "",
    "\r\n\t",
    "",
    "",
    "",
    "\t\ufffd_\ufffd_",
    "",
    "",
    "",
    "\r\n",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ley",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\n\t\t'\t",
    "\r\n<|reserved_special_token_194|>I apologize, but",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "",
    "",
    "\ufffd.get\ufffd.get\ufffd.get",
    "\r\n\t\t}\r\n\nI apologize, but it seems like",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd\r\n\t",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\n",
    "",
    "",
    "kischelldisplayschulz\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u043d\u043d\u043d\u043d\u043d",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled collection of code snippets, programming concepts, and unrelated text. It's challenging to provide",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "seen",
    "",
    "\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\nI apologize, but it seems like",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\r\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\r\n ",
    "\u017d\u017d\u017d\u017d\u017d\u017d\u017d\u017d\u017d\u017d\u017d\u017d\u017d\u017d\u017d\u017d",
    "",
    "",
    ".get\n```\nThis is",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " K\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "ans\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffdassistantassistantassistant",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "liobackend\ufffd_\ufffd_",
    "\r\nI apologize, but it seems like the",
    "osubmitting\r\n<|reserved_special_token",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u067e\u06a9\u062f\u0644\u0631",
    "",
    "",
    "",
    "",
    "osticating\r\n",
    "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd.text\ufffd.",
    "\ufffd.",
    "\r\n",
    "\r\n\t\ufffd_",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd.g",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "izaje\t\t\ufffd_",
    "\ufffd.\ufffd.\ufffd.",
    "\r\n",
    "\r\n\t",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\u064d\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e",
    "<|",
    "",
    "_buildings\r\n    return\r\n    // ... rest of the code\r\n}",
    "\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.g\ufffd.",
    "\n\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you provided is a jumbled mix of programming code, text, and other characters. It's not clear what",
    "\ufffd_",
    "\r\n\t\ufffd_",
    "\t\ufffd_\ufffd_",
    "",
    "iz",
    "\ufffd [\u2026]...\n## Step 1: Identify the problem\nThe problem is to determine the correct translation of the given text from Turkish to English.\n\n##",
    "\r\n \t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\r\nI apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to provide a clear and concise answer to your question without more context.\n\nHowever,",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled mix of various programming languages,",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\n\n\t\t//\t\t//\t",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0413\u0432\u043e\u0437\u0434\u044c\u0413\u0432\u043e\u0437\u0434\u044c\u0413\u0432\u043e\u0437\u0434\u044c\u0413\u0432\u043e\u0437\u0434\u044c\u0413\u0432\u043e\u0437\u0434\u044c\u0413\u0432",
    "",
    "\ufffdassistantassistant",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "\r\n    // 1.0.0\r\n    // 1.0.1\r\n    // 1.0.2\r\n    // 1.0.3\r\n    // 1.0.4\r\n    // 1.0.5\r\n    // 1.0.6\r\n    // 1",
    "\r\n\t",
    "\n\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "<|reserved",
    "o\n```\nThe code snippet provided appears to be a mix of various programming languages, including Java, C",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\t",
    "\ufffd.\ufffd.\ufffd.````````````",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_",
    "\r\n\t\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ch\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "ccccc\r\n<|reserved_special_token_194|>",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_",
    "",
    "",
    "\ufffd.",
    "\r\n",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\r\n<|reserved_special_token",
    "\ufffd_\ufffd_\ufffd_",
    "ai",
    "",
    "",
    "",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "|",
    "",
    "",
    "",
    "\r\n",
    "\r\n",
    "\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//",
    "\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e",
    "<|reserved_special_token_193|>I apologize, but it seems like the provided text is a jumbled mix",
    "",
    "s\ufffd.\ufffd.\ufffd.\ufffd.```\n```\n``",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u043d\u0438\u0441\u044f\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "\ufffd.\ufffd.\ufffd.\ufffd.assistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ma\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to provide a",
    "",
    "ee\r\n\t\t",
    "",
    "",
    "",
    "\u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c\u0435 \u0432",
    "",
    "\r\n\t\t</span>",
    "",
    "\r\n\t\ufffd_",
    "\r\nI apologize, but it seems like the provided",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "zyczytania\ufffd_\ufffd_",
    "itystarted\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "\r\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t\ufffd\n\t\t",
    "\n\n\n",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "<|reserved_special_token_123|>I apologize, but it seems like there",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n    ```\nThis is a Python code snippet that appears to be a mix of various libraries and frameworks, including Django, Flask",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\t",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_c\r\n\t\t\ufffd_c\r\n\t\t",
    "",
    "",
    "",
    "",
    "",
    "",
    "_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ".t\ufffd.t",
    "ki\u015fler",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \ufffd_\ufffd_",
    "s\r\n\t\t\ufffd\r\n\t\t// <--- END OF FILE --->\n\t\t// <---",
    "\r\n\t\t",
    "\ufffd_",
    "\n\t\t//",
    "legedeliveries",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u0644",
    "\ufffd.\ufffd.",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "",
    "",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "",
    "icaracteristicas\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "s\r\n\t\t\ufffd.text\r\n\t\t\ufffd.text\r\n\t\t\ufffd.text",
    "",
    "",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "",
    "\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java\ufffd.java",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_",
    "",
    "s\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd.g",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a j",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "]_michelle\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "<|reserved",
    "",
    "",
    "\ufffdassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\ufffd_config APIfoot imagesendl.InFirst platform.protOptionste",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "elocation\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ".m\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.m\ufffd.",
    "\ufffd.",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "ate\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "",
    "\n\t\t} else {\n\t\t\t// ... rest of the code ...\n\t\t}\n\t}\n}\n```\n\nThis code is a mix of various programming languages, including Java, JavaScript,",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\r\n\t\ufffd.get\r\n\t\ufffd.get\r\n",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "id\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t} else {\n\t\t\t// do nothing\n\t\t}\n",
    "\u0432",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.scalablytypedassistant\nI",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d\u064d",
    "",
    "\t\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\r\n",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "<",
    "<|",
    "",
    "",
    "",
    "",
    "\ufffd.",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "",
    "",
    "\n\n**_**_**_**_**",
    "",
    "",
    "entemodernistischelldesigndesigndesigndesigndes",
    "\ufffd.renewal\r\n\t\t",
    "\r\n\t\t\ufffd_",
    "",
    "",
    "\u0432\u043d\u0435\u0448\u043d\u0435\u043f",
    ".\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n<",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " ",
    "",
    "",
    "\r\n\t",
    "",
    "",
    "",
    "",
    "",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.``````````````````````````````````````",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\r\n\t",
    "\ufffd.renewal\ufffd.renewal\ufffd.renewal",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "s\ufffd_\ufffd_\ufffd_\ufffd_",
    "ubmitting\r\n\t\t\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "",
    "",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\r\nI apologize, but it seems like the text",
    "\ufffd.",
    "",
    "AUGeometry\ufffd_\ufffd_\ufffd_",
    "",
    "",
    " \u0432\u0438\u0437\u043d\u0430\u0447\u0438\u0442\u0438\u0441\u044f \u0432\u0438\u0437\u043d\u0430\u0447\u0438\u0442\u0438\u0441\u044f \u0432",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n<|reserved_special_token_193|>I apologize, but it",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd.get\ufffd.get",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123|>I apologize, but it seems",
    "",
    "\t\t\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0644\u0631\u0646\u0631\u0646\u0631\u0646",
    "",
    "",
    "",
    "",
    "",
    "\r\n<|reserved_special_token_",
    "",
    "",
    "\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd.\ufffd.\ufffd.\ufffd.",
    "elast\r\n<|reserved_special_token",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "<|reserved_special_token_173|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to provide a clear and concise answer to your question without more context.\n\nHowever, I can try to help you identify the main topics and technologies mentioned in the text. Here are some of",
    "",
    "",
    "",
    "<|reserved_special_token_123|>I",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ynie<|reserved_special",
    "",
    "\r\n \t",
    "",
    "",
    "",
    "_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.",
    "\u064e",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_",
    "<|reserved_special_token",
    "\r\n\t\t//\t\t//\t\t",
    "etrical\r\n\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "9\ufffd.get\ufffd.get",
    "\n\"\n\"\"\n\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n\"\n",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.",
    "",
    "",
    "ipsignature\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0644\u0631\u06cc\u0646\u062f\u06a9\u06cc\u0633\u06a9\u0430\u0434\u044c\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u044f\u0440\u0442\u0435\u043c\u0430\u0441\u044c\u043a\u0430\u0434\u044c\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u044f\u0440\u0442\u0435\u043c\u0430\u0441\u044c\u043a\u0430\u0434\u044c\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f\u044f\u0440\u0442\u0435\u043c\u0430\u0441\u044c\u043a\u0430\u0434\u044c\u043f\u0440\u0435\u0434",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0644",
    "\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    "\ufffdassistant",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.",
    "",
    "",
    "",
    "",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "",
    "",
    "",
    "\ufffd_",
    "\ufffd.t",
    "\ufffd_",
    "",
    "",
    "",
    "\ufffd.\ufffd.",
    "<|reserved_special_token_123|>I apologize, but it seems like the text you",
    "\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "",
    "\r\n\t\t\ufffd\r\n\t\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "<|reserved_special_token_173|>I apologize, but it seems like the text you provided is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and general language. It's difficult to provide a",
    "",
    "s\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ting\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffdassistantassistantassistant",
    "\u0430\u043a\u0442\u0438\u0432\u043d\u044b\u0439\r\nI apologize, but it seems like the provided text is a jumbled mix",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled",
    "\r\n\t\ufffd.scalablytypedI apologize, but it seems like the text you provided is a jumbled collection of words and",
    "\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_",
    "\ufffd.scalablytypedI apologize",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t",
    "tob",
    "\ufffd.",
    "\u0e38\u0e0a\u0e31\u0e48n\ufffdassistant",
    "",
    "",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "ate\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ub",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd.",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd.\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u06a9\u062f\u0644\u0627\u0631\u067e\u0631\u0648\u06af\u0631\u0627\u0645\u0631\u06a9\u0627\u0628\u06cc\u0646\u062a\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f\u06cc\u0631\u06a9\u062f",
    "_cacheclassic\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f",
    "holder",
    "\ufffd\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "\ufffd.",
    "n\ufffd_\ufffd_",
    "",
    " \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0ba4",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "",
    "",
    "",
    "\r\n\t\t}",
    "\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.g\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t",
    "\n```\nThe code snippet appears to be a mix of various programming languages, including Java, Python, and",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistant\n\nassistant\nassistant\n",
    "ULAR\r\n\t\ufffd_\ufffd_\ufffd_",
    "\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e",
    "",
    "\ufffdassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but",
    "",
    "",
    "<|reserved_special_token_123|>I apologize, but it",
    "\ufffd_",
    "",
    "\n\n\n    .get('",
    "",
    "",
    "",
    " \ufffdassistant",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//\t\t",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI apologize,",
    "ed\ufffd_",
    "",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "]_type_1_1_1_1_1_1_1_1_1_1_1_",
    "\r\n\t\t}",
    "assistant\nassistant\nassistant",
    "\r\n\t\t\ufffd.scalablytyped",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "\r\n\t\t",
    "o\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.m\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\n\t\t}",
    "_c\ufffd_",
    "",
    "\r\n\t\t//.get\r\n\t\t//.get\r\n\t\t//.get\r\n\t\t//.get\r\n\t\t//",
    "",
    "",
    "",
    "",
    "",
    "\r\n<|reserved_special_token_123|>I",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\ufffd_\ufffd_",
    "ion\t",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\nI apologize, but it seems like the provided code snippet is not a complete program, but rather a collection of code fragments and comments. It appears to be a mix of different",
    "\r\n\t\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "<|reserved_special_token_123|>I apologize, but it seems like the input provided",
    "\n\t\t//\t\t//\t\t//\t\t//",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0442\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f",
    "<",
    "\r\n\t\t\ufffd_",
    "<|reserved_special_token_123|>I",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "icsubmitting\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled collection of",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\t",
    "\ufffd_\ufffd_",
    "\ufffdassistant\n\u0412\u043e\u043f\u0440\u043e\u0441: \u0447\u0442\u043e",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd.assistant",
    "\ufffd_",
    "<",
    "\ufffd_",
    "\r\n\t\t\ufffd_eventssubmit\r\n\t\t\ufffd_eventssubmit\r\n\t\t\ufffd_eventssubmit\r\n\t\t\ufffd_event",
    "",
    "\r\n\t\t\ufffd.scalablytypedI apologize, but",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\t//_c\r\n\t\t//_c\r\n\t\t//_c\r\n\t\t//_c\r\n\t\t//_c\r\n\t\t",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_c\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n<|reserved_special_token_",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\n<|reserved_special_token_194|>I apologize, but it seems like",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n",
    "\r\n",
    "IA\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "ldoctorate\u0432\u043d\u0435\u0448\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e",
    "\ufffd.",
    "# Chapter 1: Introduction",
    "",
    "",
    "",
    "\r\n\t",
    "ariabackgroundImage\t\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\ufffd_",
    "<|reserved_special_token_123|>I can\u2019t assist with that request. Is",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "s\n\t\t//\t\t//",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_class",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\n\t\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "                .(\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "f0\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u00a0assistant",
    "\ufffd.class\ufffd.class\ufffd.class\ufffd.class\ufffd.class",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd.scalablytypedI apologize, but it seems like the provided code snippet is not a complete program",
    "\r\n<|reserved_special_token_194|",
    "\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_",
    "\ufffd_",
    "assistant",
    "",
    "o\ufffd_",
    "",
    "",
    "\r\n\t\t",
    "",
    " \u0111\u01b0\u1ee3ctogether\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "ss",
    "\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "ian",
    "\ufffd_",
    "",
    "\n```\nThe code snippet provided appears to be a mix of various programming languages, including Java, JavaScript, and C",
    "\u0430\u0443",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\ufffd.",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\t\ufffd.redundant\t\ufffd.redundant\t",
    "anhs\u0131d\u0131r\u0639\u0628\u062fullah\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644",
    "C\n\t\t//\t\t}\n\t\t//\t\t}\n",
    "",
    "",
    "ic\ufffd_",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n \t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\n\t\t}",
    "\n<|reserved_special_token_64|>I can\u2019t help you with that. Is there something else I can assist you with?",
    "",
    "",
    "\r\n\t\t\ufffd\r\n\t\t",
    "ETableView\ufffd_\ufffd_\ufffd_",
    "",
    "??????",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "assistant",
    "\r\n\t\ufffd.assistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant\nassistant",
    "",
    "",
    "\t\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "\r\n\t",
    "",
    "",
    "",
    "lort",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "",
    "\ufffdassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special",
    "\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "\u043d\u043a\u0430\u0434\u043b\u044f\u0432\u0442\u0432\u0440\u0442\u0432",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "/\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t</",
    "st\t\t",
    "|reserved_special_token_123",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n  // ...",
    "",
    "\ufffd.m\ufffd.m\ufffd.",
    "renges\r\n\t\t</span>\n\t\t</div>\n\t\t</div>\n\t\t</div>\n\t\t</div>\n\t\t</div>\n\t\t</div>\n\t\t</div",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ype\n\t\t",
    "##<|reserved_special_token_64|>It appears that the provided text is a jumbled collection of words,",
    "\u0435\u043d\u043d\u0438\u0441\u044f",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\n\t\t}",
    "",
    "<",
    "\r\n",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "\ufffd_\ufffd_\ufffd_",
    "elocation\r\n\t\t\ufffd\r\n",
    "\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "time\t\t",
    "\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get",
    "",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\t\t\ufffd\nThe code provided is a mix of",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.scalablytyped",
    "\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e\u064e",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\u00e9r\u0435\u043d\u043d\u0439\u0435 \u0432\u043d\u0435\u0448\u043d\u0435\u043f",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    " ```\n```java\nimport java.util.*;\nimport java.io.*;\nimport java.math.*;\n\npublic class Main {\n   ",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u041c\u044b\u043d\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "\ufffd_\ufffd_",
    "\u041c\u044b\u043d\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439",
    "\u017divotn\u00ed zahrani\u010dn\u00ed zahrani\u010dn\u00ed",
    "",
    "",
    "ICINE\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_193|>I apologize, but it seems like the provided",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n\t\t",
    "\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to provide a clear and concise answer to your question.\n\nHowever, I can try to help you identify the main topics and provide some guidance on how to approach the problem.\n\nThe text appears to be a combination of:\n\n1. Programming languages: Java, Python, C++, and JavaScript.\n2. Frameworks: Spring, Hibernate, and React.\n3. Concepts: Object-Oriented Programming (OOP), Design Patterns, and Database Management.\n4. Miscellaneous: XML, JSON, and SQL.\n\nTo provide a more helpful response, could you please clarify the following:\n\n1. What is",
    "",
    "",
    "",
    "\r\n\t\t//_build\r\n\t\t//",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_193|>I apologize, but it seems like the provided text",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "ic\ufffd_\ufffd_",
    "\ufffd.scalablytypedI apologize, but it seems like the provided text is a jumbled mix of",
    "",
    "ONDEndpoint\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "_syste",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n<|reserved_special_token_64|>It seems like you provided a large block of code and text that doesn't make sense in the context of a question or problem. I'll do my best to provide a helpful response.\n\nIf",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//_label\r\n\t\t//_label\r\n\t\t",
    "\ud55c\uc740\uc774\ub4e4\uc5b4",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0443\u0440\u0430\u043b\u043d\u0438\u0441\u043f\u0430\u0441\u0438\u0442\u0435\u043b\u043d\u0438\u0441\u043f\u0430\u0441\u0438\u0442\u0435\u043b\u043d\u0438\u0441",
    "\ufffd_",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "\ufffd_",
    "",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a j",
    "",
    "",
    "\r\n\t\t//.get\r\n\t\t//",
    "\r\n\t",
    "",
    "",
    "\t\ufffd.get\ufffd.get",
    " \t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "assistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_",
    "\r\nI apologize, but it seems like the input text is a jumbled collection of words and phrases from various domains, including programming, sports, music, and more. It",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.scalablytyped",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ang",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "ova\r\n\t\t\ufffd\r\n\t\t",
    "\ufffd.get\ufffd.get",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\n\t\t",
    "\r\nI apologize, but it seems like the text you provided is a j",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    " \t\ufffd.class\ufffd.class\ufffd.class",
    "o\n<",
    "",
    "\ufffd_",
    "\r\n\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd.m\ufffd.m",
    "\ufffd_",
    "",
    "",
    "assistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\ufffdassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ate\r\nI apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It's difficult to",
    "",
    "",
    "\ufffd\u0432\u043d\u0435\u0448\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043d\u0435\u043f\u0440\u0438",
    "\r\n\t\t\ufffd\r\n\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t\t//\t",
    "\u0442\u0438",
    "\ufffd_",
    "_certainly\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.",
    "E\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0442\u043e\u0440\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f\u044f",
    "\t\t<|reserved_special_token_34|>I apologize, but it seems like",
    "",
    "c\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    ".t\ufffd\r\n\t\t}\r\n\n",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_34|>It seems like the provided text is a j",
    "",
    "\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_",
    "\ufffd.",
    "\n<|reserved_special_token_64|>I apologize",
    "_t\" \t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffdassistant",
    "\r\n\t\t",
    "utemodified\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u06cc\u06afic\ufffd.",
    "\ufffd_\ufffd_",
    "\ufffd_",
    "",
    "\r\n\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "\ufffdassistantassistant\n\nassistant\nassistant\nassistant\n",
    "udun",
    "",
    "\ufffd.s\ufffd.\ufffd.",
    "\n\t\t",
    "\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432\u0440\u0435\u043c\u044f\u0432\u043e\u0432",
    "\r\nI apologize, but it seems like the provided text is a jumbled",
    " \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0",
    "\ufffd_",
    "erpretation\r\n<|reserved_special_token",
    "\ufffd.\ufffd.\ufffd.\ufffd.\u200b_\u200b_\u200b_\u200b_\u200b_\u200b_\u200b",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "\r\n  .<|",
    "",
    "\ufffd.g\ufffd.g",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u043d\u0432\u043e\u0432\u043d\u0435\u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e\u044d\u043a\u0437\u0430\u043c\u0435\u043d\u043e\u0432\u0430\u0442\u0438\u0441\u044f\u0432\u043e\u0432\u043b\u0435\u0447\u044c\u0441\u044f",
    "\r\n<|reserved_special_token_213|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages,",
    "",
    "\t\ufffd.\t\ufffd.\t\ufffd.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    " 1\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_",
    "",
    "\t\t\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get",
    "\ufffd_\ufffd_",
    "\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\t\ufffd\t\ufffd\t\ufffd\t",
    "\r\n\t\t\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_source=google&utm_medium=organic&utm_campaign=",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "]_c",
    "\ufffd.scalablytyped\n\u0432\u043e\u043f\u0440\u043e\u0441\u044b \u0432\u043e\u0441\u043a\u0440\u0435\u0441\u0435\u043d\u044c",
    "\ufffd_",
    "igher\t\ufffd_",
    "\t\t",
    "\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631\u0646\u0644\u0631",
    "",
    "\r\n \t\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "tiggesteeldescribeproducts\ufffd_",
    "",
    "",
    "",
    "\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m\ufffd.m",
    "tension\n\t\t\ufffd.assistant",
    "s\ufffd_",
    "uagewhich\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "en\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.scalablytypedI apologize,",
    "\ufffd_\ufffd_",
    "\t\t\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "",
    "\ufffd_",
    "\ufffd_\ufffd_\ufffd_",
    "",
    "\n",
    "",
    "<|reserved_special_token_193|>I apologize, but it seems like the provided",
    "\ufffd.\ufffd.",
    "",
    "\ufffd.get",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "ubmit\r\n\t\ufffd_\ufffd_",
    "Jority\n\t\t",
    "\ufffd_\ufffd_",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "<|reserved_special_token_123|>I apologize, but it seems like the provided text is a jumbled collection of words and phrases from various sources, including programming languages, technical terms, and other",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "",
    "",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\t\ufffd_\ufffd_",
    "",
    "\ufffd.\ufffd.\ufffd.assistantassistantassistantassistant",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "<|reserved_special_token_64|>I apologize, but it seems like the text you provided is a jumbled mix of various programming",
    "c",
    "ary\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_",
    "<|reserved_special_token_123",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.\ufffd.",
    "",
    "age\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ra\u041c\u044b\u043d\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432",
    "<|reserved_special_token_123",
    "",
    "\r\n    // ... truncated ...\n```\n\nThis code appears to be a mix of various programming languages, including Java, C++, and JavaScript.",
    "\r\n\t\t\ufffd.\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "en\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank\ufffd_blank",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "s\n\t\t\ufffd_",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t",
    "\r\n",
    "\t\t<|reserved_special_token_194",
    "ed\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n\t\ufffd\r\n",
    "\r\n\t\ufffd_\ufffd_",
    "<|reserved_special_token_193|>I apologize, but it seems like the provided text is",
    "",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.",
    "cript\r\n\t\t\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441",
    "",
    "\r\n\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "",
    " \ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\t//.get\r\n\t\t//.get\r\n\t\t//.get\r\n\t\t//",
    "\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts. It's challenging to provide a clear and concise answer without more context or a specific question. However, I'll try to break down the text",
    "",
    "",
    "\r\n\t",
    "",
    "",
    "",
    "<|reserved_special_token_123|>I can\u2019t help with that request. It seems like you provided a large block of code and asked",
    "\r\n<|reserved_special_token_194|>I apologize, but it seems like the text",
    "\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.\ufffd.\ufffd.",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_",
    "",
    "",
    "\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.t\ufffd.",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t\ufffd_\ufffd_",
    "\r\n\t\ufffd_main\r\n\t",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\nI apologize, but it seems like the provided text is a jumbled mix of various programming languages, frameworks, and concepts.",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_",
    "_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "enoticeable  1 2 3 4 5 6 7 8 ",
    "arencodetexture\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "koulakos\\_<|reserved_special_token_34|>I apologize, but it seems like",
    "\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "_\ufffd_\ufffd_",
    "\n\t\t\ufffd.",
    "",
    "\r\n\t\t\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ety\ufffd_",
    "\r\n\t\t// ...",
    "<|reserved_special_token",
    "\u0413\u0432",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get\ufffd.get",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad",
    "\r\n \t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t\t\ufffd\r\n\t",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "ING\t\ufffd_\ufffd_",
    "\r\n\t\t\ufffd_\ufffd_",
    "/\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "t",
    "\u041c\u043e\u0441\u043a\u0432\u0430\u0441\u044c\u0432\u044f\u0441\u0442\u043e\u044d\u043a\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439\u0432\u0441\u0435\u0433\u043e\u0445\u0432",
    "\ufffd\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a\u0438\u0439\u0432\u0438\u0441\u043e\u043a",
    "<|reserved_special_token_123|>I apologize",
    "_c\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "<",
    "\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    ""
  ],
  "errors": [
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 183, in _connect_sock\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/socket.py\", line 233, in __init__\nOSError: [Errno 24] Too many open files\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 290, in async_request_openai_completions\n    async with session.post(\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 1425, in __aenter__\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1406, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1375, in _create_direct_connection\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host 127.0.0.1:8000 ssl:default [Too many open files]\n",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "request_timestamps": [
    1759812563.2769024,
    1759812563.3817568,
    1759812563.4481108,
    1759812563.519698,
    1759812563.5535936,
    1759812563.62512,
    1759812563.6846035,
    1759812563.7874818,
    1759812563.7933695,
    1759812563.827688,
    1759812563.8588033,
    1759812563.9009073,
    1759812563.9008615,
    1759812563.90987,
    1759812563.9207366,
    1759812563.9393513,
    1759812564.0359786,
    1759812564.0487685,
    1759812564.0488105,
    1759812564.0723712,
    1759812564.0724032,
    1759812564.1407793,
    1759812564.1656854,
    1759812564.233322,
    1759812564.2680135,
    1759812564.3073258,
    1759812564.4857378,
    1759812564.5327382,
    1759812564.6473064,
    1759812564.7937748,
    1759812564.8625495,
    1759812565.022191,
    1759812565.056226,
    1759812565.0963514,
    1759812565.2463844,
    1759812565.2463534,
    1759812565.4596908,
    1759812565.4670591,
    1759812565.5151083,
    1759812565.5150673,
    1759812565.5884173,
    1759812565.644236,
    1759812565.7473042,
    1759812565.8529747,
    1759812566.0103323,
    1759812566.041406,
    1759812566.0648882,
    1759812566.1041586,
    1759812566.3192956,
    1759812566.3234174,
    1759812566.3499963,
    1759812566.3682334,
    1759812566.4316502,
    1759812566.622638,
    1759812566.6301467,
    1759812566.70449,
    1759812566.7224326,
    1759812566.8403459,
    1759812566.8523614,
    1759812566.9686408,
    1759812567.004066,
    1759812567.068009,
    1759812567.18502,
    1759812567.2551143,
    1759812567.3606343,
    1759812567.3986337,
    1759812567.4548285,
    1759812567.5024416,
    1759812567.6447618,
    1759812567.712165,
    1759812567.7438362,
    1759812567.7967048,
    1759812567.8179603,
    1759812567.8425205,
    1759812568.0142534,
    1759812568.0221207,
    1759812568.0982428,
    1759812568.3257294,
    1759812568.3305802,
    1759812568.360512,
    1759812568.3738227,
    1759812568.5512507,
    1759812568.6381087,
    1759812568.7415257,
    1759812568.9967248,
    1759812569.0037005,
    1759812569.2910044,
    1759812569.398466,
    1759812569.4168494,
    1759812569.5576582,
    1759812569.5735147,
    1759812569.5940552,
    1759812569.7074568,
    1759812569.7649877,
    1759812569.8286724,
    1759812569.8959346,
    1759812569.9421158,
    1759812570.029684,
    1759812570.2816622,
    1759812570.2817042,
    1759812570.3770416,
    1759812570.3880596,
    1759812570.5751715,
    1759812570.7044723,
    1759812570.7045162,
    1759812570.7508044,
    1759812570.7736075,
    1759812570.8106954,
    1759812570.825451,
    1759812570.8300016,
    1759812571.031518,
    1759812571.0314713,
    1759812571.0315082,
    1759812571.3803225,
    1759812571.380289,
    1759812571.4768865,
    1759812571.4974732,
    1759812571.4974296,
    1759812571.553414,
    1759812571.8969045,
    1759812571.906753,
    1759812572.0351067,
    1759812572.0351655,
    1759812572.0351815,
    1759812572.0460975,
    1759812572.261523,
    1759812572.317019,
    1759812572.3242025,
    1759812572.3952813,
    1759812572.4018774,
    1759812572.4180377,
    1759812572.4280553,
    1759812572.4586208,
    1759812572.4786036,
    1759812572.482756,
    1759812572.5257804,
    1759812572.6412563,
    1759812572.796807,
    1759812572.8282955,
    1759812572.892995,
    1759812573.0111852,
    1759812573.2159421,
    1759812573.2499137,
    1759812573.2825341,
    1759812573.3203285,
    1759812573.3652623,
    1759812573.4120977,
    1759812573.4240408,
    1759812573.464742,
    1759812573.522259,
    1759812573.5956,
    1759812573.6396637,
    1759812573.6454978,
    1759812573.6454709,
    1759812573.7717404,
    1759812573.8242445,
    1759812573.8393905,
    1759812573.8749464,
    1759812573.8915336,
    1759812573.953631,
    1759812573.9581892,
    1759812574.0320442,
    1759812574.0646698,
    1759812574.1717157,
    1759812574.2496922,
    1759812574.2739177,
    1759812574.356573,
    1759812574.457099,
    1759812574.530565,
    1759812574.5755508,
    1759812574.6746204,
    1759812574.8200123,
    1759812574.8258672,
    1759812574.8967953,
    1759812574.930718,
    1759812575.0081422,
    1759812575.0269787,
    1759812575.1116982,
    1759812575.1482155,
    1759812575.1595712,
    1759812575.28817,
    1759812575.3067071,
    1759812575.4023063,
    1759812575.4952621,
    1759812575.4982493,
    1759812575.52431,
    1759812575.5456998,
    1759812575.5747323,
    1759812575.583903,
    1759812575.690995,
    1759812575.9724023,
    1759812576.0207386,
    1759812576.0935938,
    1759812576.1143966,
    1759812576.1231103,
    1759812576.3089283,
    1759812576.3613706,
    1759812576.3765738,
    1759812576.5286493,
    1759812576.531496,
    1759812576.5925357,
    1759812576.6416547,
    1759812576.6973994,
    1759812576.7098663,
    1759812576.7175972,
    1759812576.8772795,
    1759812576.882366,
    1759812576.918375,
    1759812576.939773,
    1759812576.9851584,
    1759812577.0180557,
    1759812577.087114,
    1759812577.094665,
    1759812577.1366513,
    1759812577.3446743,
    1759812577.609456,
    1759812577.614389,
    1759812577.6216867,
    1759812577.6310115,
    1759812577.6497986,
    1759812577.6947155,
    1759812577.7350585,
    1759812577.7453048,
    1759812577.7974396,
    1759812577.8325584,
    1759812577.918764,
    1759812577.9767282,
    1759812578.0121195,
    1759812578.084955,
    1759812578.2027931,
    1759812578.382709,
    1759812578.4276187,
    1759812578.4868085,
    1759812578.5569525,
    1759812578.6328835,
    1759812578.6985044,
    1759812578.7596798,
    1759812578.7983065,
    1759812578.815476,
    1759812578.8560817,
    1759812578.8717022,
    1759812578.9295502,
    1759812578.9628944,
    1759812578.9907556,
    1759812579.0055983,
    1759812579.151785,
    1759812579.1676788,
    1759812579.2327604,
    1759812579.2382085,
    1759812579.3606367,
    1759812579.498645,
    1759812579.509896,
    1759812579.5600305,
    1759812579.744102,
    1759812579.8435261,
    1759812579.8687859,
    1759812580.0822513,
    1759812580.1707563,
    1759812580.1792314,
    1759812580.1870065,
    1759812580.2646472,
    1759812580.2784226,
    1759812580.3933291,
    1759812580.3937268,
    1759812580.5025444,
    1759812580.5144198,
    1759812580.5679762,
    1759812580.5863047,
    1759812580.9866896,
    1759812580.9922345,
    1759812581.171794,
    1759812581.2524323,
    1759812581.2676535,
    1759812581.2676086,
    1759812581.2744215,
    1759812581.338557,
    1759812581.3696628,
    1759812581.4379551,
    1759812581.527695,
    1759812581.5905747,
    1759812581.6241863,
    1759812581.661072,
    1759812581.6719172,
    1759812581.6867738,
    1759812581.7355113,
    1759812581.73549,
    1759812581.7425787,
    1759812581.8139935,
    1759812581.9501078,
    1759812582.1488838,
    1759812582.236331,
    1759812582.4181867,
    1759812582.5991015,
    1759812582.6309404,
    1759812582.638254,
    1759812582.704631,
    1759812582.7239356,
    1759812582.937709,
    1759812583.2257714,
    1759812583.2732105,
    1759812583.2995179,
    1759812583.3080573,
    1759812583.3677945,
    1759812583.4503748,
    1759812583.4988687,
    1759812583.5144935,
    1759812583.5204918,
    1759812583.5264409,
    1759812583.6201704,
    1759812583.65584,
    1759812583.676628,
    1759812583.7021658,
    1759812583.8003635,
    1759812583.8353326,
    1759812583.8436897,
    1759812583.8963294,
    1759812583.9385355,
    1759812583.9929473,
    1759812584.0854218,
    1759812584.4109888,
    1759812584.4873524,
    1759812584.5372398,
    1759812584.5875611,
    1759812584.5922167,
    1759812584.651725,
    1759812584.700452,
    1759812584.7830856,
    1759812584.9374847,
    1759812585.0207639,
    1759812585.1662898,
    1759812585.2275906,
    1759812585.2454112,
    1759812585.2730231,
    1759812585.2730706,
    1759812585.2778993,
    1759812585.29342,
    1759812585.3475664,
    1759812585.3567076,
    1759812585.4369535,
    1759812585.447994,
    1759812585.537241,
    1759812585.6833172,
    1759812585.78082,
    1759812586.0342155,
    1759812586.1733456,
    1759812586.179616,
    1759812586.1796484,
    1759812586.183662,
    1759812586.204368,
    1759812586.2342973,
    1759812586.2963603,
    1759812586.3309872,
    1759812586.3368752,
    1759812586.402664,
    1759812586.434565,
    1759812586.4489367,
    1759812586.4995003,
    1759812586.5175145,
    1759812586.527975,
    1759812586.55051,
    1759812586.5545068,
    1759812586.6543033,
    1759812586.6581426,
    1759812587.1492252,
    1759812587.2238724,
    1759812587.2601323,
    1759812587.3175163,
    1759812587.3304167,
    1759812587.453124,
    1759812587.5197005,
    1759812587.5320673,
    1759812587.791728,
    1759812588.0146523,
    1759812588.0820904,
    1759812588.1552532,
    1759812588.473084,
    1759812588.4790323,
    1759812588.6046517,
    1759812588.6835117,
    1759812588.7238004,
    1759812588.8322632,
    1759812588.9750774,
    1759812589.0134845,
    1759812589.054669,
    1759812589.0646577,
    1759812589.0898948,
    1759812589.103473,
    1759812589.327315,
    1759812589.353018,
    1759812589.6139812,
    1759812589.7573552,
    1759812589.9058385,
    1759812590.018782,
    1759812590.1084175,
    1759812590.1123612,
    1759812590.2297044,
    1759812590.2414377,
    1759812590.2655168,
    1759812590.2824156,
    1759812590.2959979,
    1759812590.3181994,
    1759812590.3813286,
    1759812590.388213,
    1759812590.4989204,
    1759812590.5285544,
    1759812590.6002023,
    1759812590.701438,
    1759812590.719518,
    1759812590.8287742,
    1759812590.9402785,
    1759812591.001842,
    1759812591.0142019,
    1759812591.0497735,
    1759812591.1950696,
    1759812591.3263297,
    1759812591.3815277,
    1759812591.4913108,
    1759812591.4913542,
    1759812591.544366,
    1759812591.5539548,
    1759812591.5923748,
    1759812591.5924196,
    1759812591.6213205,
    1759812591.621354,
    1759812591.7127008,
    1759812591.7313068,
    1759812591.77144,
    1759812591.828592,
    1759812591.8940163,
    1759812591.9698346,
    1759812592.017619,
    1759812592.0671916,
    1759812592.1830301,
    1759812592.2115164,
    1759812592.2238998,
    1759812592.2967982,
    1759812592.3816001,
    1759812592.4006884,
    1759812592.5379562,
    1759812592.6906374,
    1759812592.7139585,
    1759812592.7466927,
    1759812592.804214,
    1759812592.8301456,
    1759812592.9008415,
    1759812592.971641,
    1759812593.0215108,
    1759812593.0560899,
    1759812593.0787947,
    1759812593.11859,
    1759812593.1862023,
    1759812593.2384079,
    1759812593.244555,
    1759812593.4304702,
    1759812593.4625902,
    1759812593.5446825,
    1759812593.6554737,
    1759812593.6602569,
    1759812593.675276,
    1759812593.7281978,
    1759812593.9371626,
    1759812594.004718,
    1759812594.0515418,
    1759812594.191834,
    1759812594.244661,
    1759812594.3794718,
    1759812594.3859346,
    1759812594.4434283,
    1759812594.4539216,
    1759812594.464781,
    1759812594.605695,
    1759812594.6174488,
    1759812594.7399054,
    1759812594.7525556,
    1759812594.7881255,
    1759812594.826529,
    1759812594.8265839,
    1759812594.8377223,
    1759812594.8376877,
    1759812594.9039106,
    1759812595.015868,
    1759812595.194775,
    1759812595.2469463,
    1759812595.318726,
    1759812595.3410673,
    1759812595.383095,
    1759812595.4194078,
    1759812595.4239206,
    1759812595.5598788,
    1759812595.6736364,
    1759812595.7308874,
    1759812595.7415183,
    1759812595.8108065,
    1759812595.9336185,
    1759812595.9633775,
    1759812595.999954,
    1759812596.0122566,
    1759812596.0829487,
    1759812596.0939631,
    1759812596.1140976,
    1759812596.117431,
    1759812596.138535,
    1759812596.286857,
    1759812596.304414,
    1759812596.510966,
    1759812596.5109186,
    1759812596.520878,
    1759812596.6221611,
    1759812596.6292815,
    1759812596.6775115,
    1759812596.691129,
    1759812596.7456973,
    1759812596.776137,
    1759812596.9327426,
    1759812596.9659128,
    1759812597.2983723,
    1759812597.3365493,
    1759812597.3526225,
    1759812597.3857193,
    1759812597.4940796,
    1759812597.5307791,
    1759812597.5863955,
    1759812597.7860322,
    1759812597.857558,
    1759812597.8981438,
    1759812597.952438,
    1759812598.3236845,
    1759812598.3394809,
    1759812598.42034,
    1759812598.5278459,
    1759812598.7698324,
    1759812598.8261638,
    1759812598.851277,
    1759812598.865001,
    1759812598.8784509,
    1759812598.9516969,
    1759812599.0378602,
    1759812599.0444498,
    1759812599.0801687,
    1759812599.0868647,
    1759812599.1882217,
    1759812599.2837048,
    1759812599.2886446,
    1759812599.3212698,
    1759812599.4030802,
    1759812599.4952054,
    1759812599.6282,
    1759812599.7011294,
    1759812599.7422955,
    1759812599.776255,
    1759812599.804269,
    1759812599.91825,
    1759812599.9829733,
    1759812600.1087484,
    1759812600.2202578,
    1759812600.355537,
    1759812600.4195788,
    1759812600.4348693,
    1759812600.4681373,
    1759812600.5478723,
    1759812600.6110687,
    1759812600.6630394,
    1759812600.718602,
    1759812600.912053,
    1759812600.9163046,
    1759812600.9341571,
    1759812600.9411268,
    1759812601.0007992,
    1759812601.0698264,
    1759812601.1630528,
    1759812601.2291455,
    1759812601.2291257,
    1759812601.2742834,
    1759812601.2934952,
    1759812601.4398317,
    1759812601.5067842,
    1759812601.6302552,
    1759812601.7506542,
    1759812601.765595,
    1759812601.7766905,
    1759812601.7912638,
    1759812601.7983854,
    1759812601.8330805,
    1759812601.87516,
    1759812601.8796155,
    1759812601.923388,
    1759812601.9548867,
    1759812602.1734016,
    1759812602.176269,
    1759812602.2207677,
    1759812602.2207906,
    1759812602.256657,
    1759812602.261013,
    1759812602.2876406,
    1759812602.2961736,
    1759812602.4440207,
    1759812602.4761784,
    1759812602.5906246,
    1759812602.6085565,
    1759812602.677172,
    1759812602.6969254,
    1759812602.7696877,
    1759812602.7889283,
    1759812602.9817166,
    1759812602.9918847,
    1759812603.0394938,
    1759812603.1146014,
    1759812603.1191728,
    1759812603.1657438,
    1759812603.1720567,
    1759812603.3559318,
    1759812603.3781312,
    1759812603.417121,
    1759812603.5682943,
    1759812603.7185957,
    1759812603.9527936,
    1759812604.0002327,
    1759812604.014524,
    1759812604.097189,
    1759812604.1755018,
    1759812604.189137,
    1759812604.3849292,
    1759812604.508634,
    1759812604.7347136,
    1759812604.7727897,
    1759812604.778295,
    1759812604.887914,
    1759812604.983065,
    1759812605.0100355,
    1759812605.2228715,
    1759812605.3796253,
    1759812605.38503,
    1759812605.4380314,
    1759812605.5467076,
    1759812605.562128,
    1759812605.5681202,
    1759812605.715082,
    1759812605.7402174,
    1759812605.7518764,
    1759812605.7543445,
    1759812605.8433893,
    1759812606.010605,
    1759812606.1875892,
    1759812606.240721,
    1759812606.313791,
    1759812606.394056,
    1759812606.4199715,
    1759812606.4666004,
    1759812606.553674,
    1759812606.6568305,
    1759812606.730528,
    1759812606.8517404,
    1759812606.9849184,
    1759812607.0022545,
    1759812607.0146148,
    1759812607.0432642,
    1759812607.0885386,
    1759812607.182955,
    1759812607.187476,
    1759812607.202779,
    1759812607.2328975,
    1759812607.289715,
    1759812607.3410597,
    1759812607.3701901,
    1759812607.412027,
    1759812607.467866,
    1759812607.5618696,
    1759812607.5717957,
    1759812607.7652085,
    1759812607.8687003,
    1759812608.0674148,
    1759812608.2344487,
    1759812608.2405465,
    1759812608.4206157,
    1759812608.463562,
    1759812608.566368,
    1759812608.7316,
    1759812608.7990968,
    1759812608.8401995,
    1759812608.8880394,
    1759812608.9654207,
    1759812609.0204954,
    1759812609.0284185,
    1759812609.0986068,
    1759812609.1267707,
    1759812609.3231378,
    1759812609.3639505,
    1759812609.387803,
    1759812609.4667532,
    1759812609.54011,
    1759812609.5491116,
    1759812609.5981822,
    1759812609.6827874,
    1759812609.726377,
    1759812610.178616,
    1759812610.1845322,
    1759812610.2045796,
    1759812610.2601082,
    1759812610.3299959,
    1759812610.3572679,
    1759812610.5383875,
    1759812610.548622,
    1759812610.732213,
    1759812610.7847075,
    1759812610.8150022,
    1759812610.8433938,
    1759812610.9316938,
    1759812610.9856284,
    1759812611.0594883,
    1759812611.2922556,
    1759812611.3357422,
    1759812611.4411838,
    1759812611.4859324,
    1759812611.4904172,
    1759812611.5371082,
    1759812611.5495818,
    1759812611.5496233,
    1759812611.7580214,
    1759812611.9010036,
    1759812611.9111397,
    1759812612.0494354,
    1759812612.0494664,
    1759812612.0610788,
    1759812612.0610445,
    1759812612.0898051,
    1759812612.120043,
    1759812612.1826673,
    1759812612.282946,
    1759812612.334042,
    1759812612.4124255,
    1759812612.5206409,
    1759812612.5325723,
    1759812612.6640775,
    1759812612.6902876,
    1759812612.7051158,
    1759812612.705068,
    1759812612.851048,
    1759812612.8675954,
    1759812612.9418888,
    1759812612.96224,
    1759812613.1212454,
    1759812613.1294768,
    1759812613.245807,
    1759812613.31503,
    1759812613.5566084,
    1759812613.6217341,
    1759812613.6298718,
    1759812613.7628777,
    1759812613.762843,
    1759812613.772663,
    1759812613.7926798,
    1759812613.8251863,
    1759812613.8360887,
    1759812613.982846,
    1759812613.9869232,
    1759812614.027214,
    1759812614.034402,
    1759812614.075164,
    1759812614.0825648,
    1759812614.0905638,
    1759812614.0995338,
    1759812614.1174757,
    1759812614.246408,
    1759812614.2548733,
    1759812614.2549138,
    1759812614.266156,
    1759812614.3586273,
    1759812614.4206247,
    1759812614.4206734,
    1759812614.4686644,
    1759812614.4941533,
    1759812614.5185428,
    1759812614.5842237,
    1759812614.64692,
    1759812614.7130506,
    1759812614.7171872,
    1759812614.759274,
    1759812614.8138971,
    1759812614.9053547,
    1759812614.9159975,
    1759812614.9401536,
    1759812614.9939747,
    1759812615.0248246,
    1759812615.4705343,
    1759812615.4742467,
    1759812615.5050132,
    1759812615.7766316,
    1759812615.8167446,
    1759812615.8760757,
    1759812616.1075969,
    1759812616.1230006,
    1759812616.2191148,
    1759812616.2483299,
    1759812616.2593005,
    1759812616.3040466,
    1759812616.3378053,
    1759812616.369076,
    1759812616.392027,
    1759812616.542895,
    1759812616.5449843,
    1759812616.632489,
    1759812616.6867504,
    1759812616.7504377,
    1759812616.8061774,
    1759812616.8629699,
    1759812616.9013474,
    1759812617.0641003,
    1759812617.0809007,
    1759812617.0998492,
    1759812617.172815,
    1759812617.1728613,
    1759812617.2098308,
    1759812617.2448025,
    1759812617.2701545,
    1759812617.2763832,
    1759812617.301548,
    1759812617.3341658,
    1759812617.3830678,
    1759812617.4142075,
    1759812617.572885,
    1759812617.5986795,
    1759812617.6358352,
    1759812617.652211,
    1759812617.6860673,
    1759812617.6950436,
    1759812617.8412962,
    1759812617.9265058,
    1759812617.9431558,
    1759812618.018863,
    1759812618.1435971,
    1759812618.143552,
    1759812618.2691426,
    1759812618.2935653,
    1759812618.3520384,
    1759812618.5724854,
    1759812618.572439,
    1759812618.8056462,
    1759812618.8271716,
    1759812618.8810499,
    1759812618.9585938,
    1759812619.017678,
    1759812619.0925086,
    1759812619.2108827,
    1759812619.2387462,
    1759812619.2913132,
    1759812619.2989116,
    1759812619.2988744,
    1759812619.3944674,
    1759812619.5379055,
    1759812619.6019394,
    1759812619.6293054,
    1759812619.6770978,
    1759812619.766562,
    1759812619.7948987,
    1759812619.8074918,
    1759812619.8243158,
    1759812619.840206,
    1759812619.9081423,
    1759812619.9361145,
    1759812619.9647472,
    1759812619.9693036,
    1759812619.9737792,
    1759812620.096242,
    1759812620.1317575,
    1759812620.2330825,
    1759812620.2870727,
    1759812620.294397,
    1759812620.4210658,
    1759812620.4512835,
    1759812620.5647004,
    1759812620.6928077,
    1759812620.7164292,
    1759812620.7164588,
    1759812620.7455635,
    1759812620.8509345,
    1759812621.0822442,
    1759812621.1304061,
    1759812621.1639442,
    1759812621.26734,
    1759812621.273408,
    1759812621.2777956,
    1759812621.3879337,
    1759812621.4805346,
    1759812621.4965432,
    1759812621.5411456,
    1759812621.610195,
    1759812621.6101468,
    1759812621.6149406,
    1759812621.7273421,
    1759812621.8301637,
    1759812621.8626702,
    1759812621.9111013,
    1759812622.0128322,
    1759812622.0175936,
    1759812622.1245039,
    1759812622.145111,
    1759812622.2110736,
    1759812622.2807157,
    1759812622.3105881,
    1759812622.4188824,
    1759812622.4612765,
    1759812622.524845,
    1759812622.651348,
    1759812622.7338376,
    1759812622.7529187,
    1759812622.7670298,
    1759812622.7669876,
    1759812623.053602,
    1759812623.0593479,
    1759812623.1226945,
    1759812623.1556027,
    1759812623.492911,
    1759812623.682952,
    1759812623.6902351,
    1759812623.7124138,
    1759812623.7285323,
    1759812623.8478904,
    1759812624.0381286,
    1759812624.0844693,
    1759812624.1825333,
    1759812624.182562,
    1759812624.220989,
    1759812624.2759242,
    1759812624.3429227,
    1759812624.5484433,
    1759812624.6054132,
    1759812624.6054015,
    1759812624.6319008,
    1759812624.7383435,
    1759812624.769801,
    1759812624.7796104,
    1759812624.8226135,
    1759812624.82814,
    1759812624.9481356,
    1759812624.9752204,
    1759812624.9823883,
    1759812625.0495925,
    1759812625.0882359,
    1759812625.2382877,
    1759812625.2610946,
    1759812625.3999631,
    1759812625.4049249,
    1759812625.4443374,
    1759812625.505639,
    1759812625.5246274,
    1759812625.603962,
    1759812625.6462693,
    1759812625.8081033,
    1759812625.8172517,
    1759812625.8617895,
    1759812625.9363098,
    1759812626.0647907,
    1759812626.0720046,
    1759812626.1572862,
    1759812626.231615,
    1759812626.2851148,
    1759812626.3149624,
    1759812626.3218048,
    1759812626.3899086,
    1759812626.5700393,
    1759812626.7534158,
    1759812626.788546,
    1759812626.858565,
    1759812626.904533,
    1759812626.947078,
    1759812626.9713063,
    1759812627.1153882,
    1759812627.1605113,
    1759812627.2086413,
    1759812627.234045,
    1759812627.2708547,
    1759812627.2810946,
    1759812627.28044,
    1759812627.2876093,
    1759812627.3397868,
    1759812627.5878017,
    1759812627.673081,
    1759812627.7414958,
    1759812627.7457933,
    1759812627.8993728,
    1759812627.9863756,
    1759812628.0774128,
    1759812628.1503835,
    1759812628.2350285,
    1759812628.293564,
    1759812628.3752117,
    1759812628.473654,
    1759812628.649518,
    1759812628.7000089,
    1759812628.6999586,
    1759812628.791056,
    1759812628.8404486,
    1759812628.8646936,
    1759812628.9291136,
    1759812629.0034254,
    1759812629.2070239,
    1759812629.2330117,
    1759812629.2879775,
    1759812629.3296707,
    1759812629.43141,
    1759812629.5452578,
    1759812629.7168305,
    1759812629.7466671,
    1759812629.7541516,
    1759812629.8131897,
    1759812629.953085,
    1759812629.991475,
    1759812630.0815206,
    1759812630.2437506,
    1759812630.2566824,
    1759812630.3106084,
    1759812630.3815384,
    1759812630.4070802,
    1759812630.4424138,
    1759812630.66366,
    1759812630.7597873,
    1759812630.787971,
    1759812630.8327596,
    1759812630.97485,
    1759812630.9923818,
    1759812631.011045,
    1759812631.077216,
    1759812631.10001,
    1759812631.120614,
    1759812631.1762884,
    1759812631.1999292,
    1759812631.2835476,
    1759812631.4395683,
    1759812631.4723463,
    1759812631.493897,
    1759812631.5533838,
    1759812631.6356354,
    1759812631.6900685,
    1759812631.8127315,
    1759812631.8525517,
    1759812631.917474,
    1759812631.9738715,
    1759812632.0835125,
    1759812632.0928686,
    1759812632.1764219,
    1759812632.2538524,
    1759812632.3553035,
    1759812632.3794217,
    1759812632.5656824,
    1759812632.6209137,
    1759812632.7211268,
    1759812632.758084,
    1759812632.7659917,
    1759812632.855382,
    1759812632.9182823,
    1759812632.9266899,
    1759812632.926654,
    1759812632.957448,
    1759812633.1539204,
    1759812633.159613,
    1759812633.2251348,
    1759812633.258706,
    1759812633.264719,
    1759812633.4172783,
    1759812633.4915862,
    1759812633.5785117,
    1759812633.5854838,
    1759812633.6962407,
    1759812633.7182562,
    1759812633.77384,
    1759812633.8309567,
    1759812633.8627014,
    1759812633.8747709,
    1759812633.932534,
    1759812633.9794211,
    1759812634.03995,
    1759812634.123155,
    1759812634.29518,
    1759812634.4758747,
    1759812634.5314581,
    1759812634.643897,
    1759812634.7249167,
    1759812634.7994423,
    1759812634.8902667,
    1759812634.99892,
    1759812635.1614926,
    1759812635.2318568,
    1759812635.2520096,
    1759812635.2867873,
    1759812635.3032508,
    1759812635.3230796,
    1759812635.4015899,
    1759812635.4078846,
    1759812635.4376717,
    1759812635.449394,
    1759812635.4727905,
    1759812635.532604,
    1759812635.5679052,
    1759812635.5679202,
    1759812635.57435,
    1759812635.5908625,
    1759812635.5908127,
    1759812635.590876,
    1759812635.7636938,
    1759812635.7996416,
    1759812635.887505,
    1759812635.9023554,
    1759812635.9562604,
    1759812636.045136,
    1759812636.0785098,
    1759812636.1239457,
    1759812636.2816381,
    1759812636.2978911,
    1759812636.3504581,
    1759812636.4047477,
    1759812636.4136548,
    1759812636.4363394,
    1759812636.662394,
    1759812636.8138223,
    1759812636.90381,
    1759812636.9298842,
    1759812637.0491054,
    1759812637.125688,
    1759812637.190923,
    1759812637.2320027,
    1759812637.2846243,
    1759812637.3508022,
    1759812637.4700007,
    1759812637.57455,
    1759812637.7647276,
    1759812638.1441224,
    1759812638.2302043,
    1759812638.2813077,
    1759812638.341679,
    1759812638.6305208,
    1759812638.743105,
    1759812638.7693508,
    1759812638.7947571,
    1759812638.8019998,
    1759812638.8505785,
    1759812638.88886,
    1759812638.8931456,
    1759812638.975569,
    1759812639.0408251,
    1759812639.144123,
    1759812639.2424083,
    1759812639.3021321,
    1759812639.4043844,
    1759812639.4792848,
    1759812639.47933,
    1759812639.5475452,
    1759812639.5626004,
    1759812639.645606,
    1759812639.6984549,
    1759812639.846876,
    1759812639.909899,
    1759812639.9892144,
    1759812640.005379,
    1759812640.0473073,
    1759812640.094323,
    1759812640.1579463,
    1759812640.174158,
    1759812640.2309678,
    1759812640.2398057,
    1759812640.2398412,
    1759812640.33929,
    1759812640.4189425,
    1759812640.502419,
    1759812640.5388353,
    1759812640.7187097,
    1759812640.77239,
    1759812640.78813,
    1759812640.8007514,
    1759812640.8536441,
    1759812640.8801126,
    1759812640.903535,
    1759812640.9035702,
    1759812640.9660547,
    1759812640.9731467,
    1759812640.97801,
    1759812641.2864292,
    1759812641.4245,
    1759812641.4303367,
    1759812641.4302895,
    1759812641.5228775,
    1759812641.5353498,
    1759812641.5892208,
    1759812641.6597717,
    1759812641.7052143,
    1759812641.8491218,
    1759812641.8714523,
    1759812641.8713996,
    1759812641.954442,
    1759812641.973502,
    1759812642.0405362,
    1759812642.0932014,
    1759812642.2230024,
    1759812642.254866,
    1759812642.3075023,
    1759812642.3559854,
    1759812642.3757768,
    1759812642.50828,
    1759812642.5397174,
    1759812642.6032956,
    1759812642.6356628,
    1759812642.6675136,
    1759812643.0673044,
    1759812643.2361794,
    1759812643.3183627,
    1759812643.4178395,
    1759812643.5885735,
    1759812643.737487,
    1759812643.8738089,
    1759812643.9270985,
    1759812643.9694808,
    1759812643.9869785,
    1759812643.9870157,
    1759812644.014229,
    1759812644.07155,
    1759812644.132247,
    1759812644.1472144,
    1759812644.14725,
    1759812644.2928092,
    1759812644.3555455,
    1759812644.4605012,
    1759812644.6815166,
    1759812644.7210674,
    1759812644.8350527,
    1759812644.9797661,
    1759812645.02065,
    1759812645.073755,
    1759812645.0794683,
    1759812645.3994596,
    1759812645.4413881,
    1759812645.5025527,
    1759812646.1014965,
    1759812646.3536727,
    1759812646.3537147,
    1759812646.3537254,
    1759812646.3889933,
    1759812646.389031,
    1759812646.477734,
    1759812646.6706717,
    1759812646.793664,
    1759812646.8719132,
    1759812646.963167,
    1759812646.963197,
    1759812647.0785067,
    1759812647.1910734,
    1759812647.194851,
    1759812647.2891626,
    1759812647.3379397,
    1759812647.393923,
    1759812647.4928102,
    1759812647.5263991,
    1759812647.559546,
    1759812647.5709412,
    1759812647.6617231,
    1759812647.719996,
    1759812647.7200415,
    1759812647.836651,
    1759812647.84714,
    1759812647.9159,
    1759812647.92251,
    1759812647.965261,
    1759812647.9768848,
    1759812648.071777,
    1759812648.0806224,
    1759812648.1259632,
    1759812648.2176018,
    1759812648.242479,
    1759812648.2657144,
    1759812648.3042448,
    1759812648.318667,
    1759812648.4561455,
    1759812648.598619,
    1759812648.5986626,
    1759812648.673457,
    1759812648.6886442,
    1759812648.6968846,
    1759812648.7168674,
    1759812648.8639307,
    1759812648.9338627,
    1759812649.0267828,
    1759812649.057495,
    1759812649.099625,
    1759812649.1371117,
    1759812649.1900926,
    1759812649.2930522,
    1759812649.319206,
    1759812649.3556387,
    1759812649.494501,
    1759812649.569245,
    1759812649.6005826,
    1759812649.6450088,
    1759812649.6874466,
    1759812649.731252,
    1759812649.8134484,
    1759812649.8866036,
    1759812649.9617488,
    1759812650.024423,
    1759812650.0851665,
    1759812650.095685,
    1759812650.1139812,
    1759812650.1192262,
    1759812650.24652,
    1759812650.3589153,
    1759812650.5149965,
    1759812650.6163476,
    1759812650.6420224,
    1759812650.7195263,
    1759812650.8235564,
    1759812650.9704025,
    1759812650.9882238,
    1759812651.07836,
    1759812651.1052217,
    1759812651.1655002,
    1759812651.2112882,
    1759812651.221963,
    1759812651.30082,
    1759812651.515068,
    1759812651.553561,
    1759812651.5626616,
    1759812651.5929651,
    1759812651.811237,
    1759812651.8352542,
    1759812651.8712785,
    1759812651.904744,
    1759812651.9145503,
    1759812651.96381,
    1759812651.9822822,
    1759812652.0316122,
    1759812652.1114836,
    1759812652.151589,
    1759812652.2492914,
    1759812652.2532012,
    1759812652.3129838,
    1759812652.3294716,
    1759812652.426159,
    1759812652.4957287,
    1759812652.531699,
    1759812652.6116457,
    1759812652.6681912,
    1759812652.7043004,
    1759812652.7648656,
    1759812653.0779042,
    1759812653.2367833,
    1759812653.308344,
    1759812653.5399072,
    1759812653.5425196,
    1759812653.774675,
    1759812653.7956035,
    1759812653.8193173,
    1759812653.9790285,
    1759812654.1892366,
    1759812654.2122009,
    1759812654.300451,
    1759812654.3970897,
    1759812654.4332669,
    1759812654.572131,
    1759812654.5858228,
    1759812654.8019392,
    1759812654.8829298,
    1759812654.91215,
    1759812654.9335964,
    1759812654.9894161,
    1759812655.019908,
    1759812655.0612931,
    1759812655.0800972,
    1759812655.2210612,
    1759812655.2647173,
    1759812655.316692,
    1759812655.3289292,
    1759812655.341048,
    1759812655.3678937,
    1759812655.3911898,
    1759812655.474715,
    1759812655.474734,
    1759812655.5053868,
    1759812655.567534,
    1759812655.604209,
    1759812655.7398107,
    1759812655.7631485,
    1759812655.7773383,
    1759812655.860494,
    1759812655.867127,
    1759812655.8966525,
    1759812655.9498622,
    1759812655.9581914,
    1759812656.0294015,
    1759812656.0601923,
    1759812656.0638337,
    1759812656.1036527,
    1759812656.169118,
    1759812656.1730516,
    1759812656.195645,
    1759812656.2612638,
    1759812656.3529434,
    1759812656.3918395,
    1759812656.4127436,
    1759812656.4465957,
    1759812656.6050782,
    1759812656.718988,
    1759812656.8789263,
    1759812656.9242387,
    1759812657.0494146,
    1759812657.0597494,
    1759812657.187608,
    1759812657.2543206,
    1759812657.2683198,
    1759812657.2727518,
    1759812657.3132746,
    1759812657.3476503,
    1759812657.515715,
    1759812657.5157297,
    1759812657.51566,
    1759812657.5546205,
    1759812657.5682316,
    1759812657.5834205,
    1759812657.6437314,
    1759812657.720368,
    1759812657.7909474,
    1759812658.0193226,
    1759812658.302789,
    1759812658.5049794,
    1759812658.5050676,
    1759812658.5050504,
    1759812658.5127308,
    1759812658.6829343,
    1759812658.769658,
    1759812658.797281,
    1759812658.8269212,
    1759812658.9431214,
    1759812659.0205858,
    1759812659.0643268,
    1759812659.1016347,
    1759812659.1415467,
    1759812659.159898,
    1759812659.1702905,
    1759812659.2191825,
    1759812659.3876946,
    1759812659.4217625,
    1759812659.918981,
    1759812660.1452746,
    1759812660.1806836,
    1759812660.250762,
    1759812660.2799044,
    1759812660.4296942,
    1759812660.4471676,
    1759812660.4684687,
    1759812660.5405047,
    1759812660.5405502,
    1759812660.5890355,
    1759812660.875392,
    1759812660.9295366,
    1759812660.9593754,
    1759812661.0663984,
    1759812661.0663457,
    1759812661.1048083,
    1759812661.1141264,
    1759812661.2254567,
    1759812661.2304256,
    1759812661.291342,
    1759812661.3197615,
    1759812661.352895,
    1759812661.416299,
    1759812661.436867,
    1759812661.5517018,
    1759812661.8894255,
    1759812661.8893778,
    1759812661.940244,
    1759812661.9566681,
    1759812662.0154648,
    1759812662.04505,
    1759812662.2190793,
    1759812662.230715,
    1759812662.2375968,
    1759812662.3457382,
    1759812662.3618097,
    1759812662.3995867,
    1759812662.4185176,
    1759812662.48411,
    1759812662.5276415,
    1759812662.537069,
    1759812662.6812413,
    1759812662.7541199,
    1759812662.7706482,
    1759812662.9011142,
    1759812662.9635873,
    1759812662.970437,
    1759812663.0294738,
    1759812663.070946,
    1759812663.2275343,
    1759812663.2605355,
    1759812663.286569,
    1759812663.3381977,
    1759812663.3886087,
    1759812663.5550253,
    1759812663.5550385,
    1759812663.5550096,
    1759812663.5549378,
    1759812663.6185324,
    1759812663.6809797,
    1759812663.7073977,
    1759812663.7416747,
    1759812663.8963277,
    1759812664.059813,
    1759812664.129255,
    1759812664.1368735,
    1759812664.1563919,
    1759812664.177479,
    1759812664.2780688,
    1759812664.4617767,
    1759812664.4680195,
    1759812664.548503,
    1759812664.6191173,
    1759812664.634491,
    1759812664.7100937,
    1759812664.7901046,
    1759812664.9230802,
    1759812664.9497988,
    1759812665.0912223,
    1759812665.1630151,
    1759812665.1787949,
    1759812665.2051423,
    1759812665.2496784,
    1759812665.280632,
    1759812665.3164313,
    1759812665.4064407,
    1759812665.4408262,
    1759812665.5421448,
    1759812665.5612988,
    1759812665.5613403,
    1759812665.570911,
    1759812665.7215438,
    1759812665.810323,
    1759812665.8943467,
    1759812666.010067,
    1759812666.0167258,
    1759812666.1167712,
    1759812666.248721,
    1759812666.319025,
    1759812666.4033782,
    1759812666.4034028,
    1759812666.4034374,
    1759812666.4382734,
    1759812666.473693,
    1759812666.4878592,
    1759812666.547917,
    1759812666.5838826,
    1759812666.6000338,
    1759812666.6192687,
    1759812666.6428416,
    1759812666.6925337,
    1759812666.6976054,
    1759812666.9209545,
    1759812666.9359288,
    1759812667.0717766,
    1759812667.1075697,
    1759812667.1401775,
    1759812667.158256,
    1759812667.1761796,
    1759812667.2797058,
    1759812667.3300693,
    1759812667.3793907,
    1759812667.4273534,
    1759812667.476628,
    1759812667.5428898,
    1759812667.551314,
    1759812667.6235747,
    1759812667.7439167,
    1759812667.9686623,
    1759812667.9806678,
    1759812668.0735576,
    1759812668.1700644,
    1759812668.2183604,
    1759812668.2855036,
    1759812668.3708022,
    1759812668.4299383,
    1759812668.5522158,
    1759812668.5785346,
    1759812668.5978708,
    1759812668.6405272,
    1759812668.649276,
    1759812668.7081358,
    1759812668.7244966,
    1759812668.9020135,
    1759812669.197559,
    1759812669.2899961,
    1759812669.3322845,
    1759812669.3584712,
    1759812669.4368265,
    1759812669.4495845,
    1759812669.583335,
    1759812669.6481,
    1759812669.657002,
    1759812669.7467113,
    1759812669.756093,
    1759812669.7724345,
    1759812669.7876647,
    1759812669.8735886,
    1759812669.9052672,
    1759812670.0821404,
    1759812670.105751,
    1759812670.1737552,
    1759812670.1737106,
    1759812670.2265637,
    1759812670.3344035,
    1759812670.3546238,
    1759812670.4153373,
    1759812670.778282,
    1759812670.7884324,
    1759812670.9068925,
    1759812670.9123964,
    1759812670.957686,
    1759812671.0037532,
    1759812671.0234005,
    1759812671.1103725,
    1759812671.1172912,
    1759812671.1822755,
    1759812671.2995422,
    1759812671.299512,
    1759812671.3407946,
    1759812671.377892,
    1759812671.5283961,
    1759812671.6392772,
    1759812671.6990674,
    1759812671.7466207,
    1759812671.7552853,
    1759812671.9030223,
    1759812671.9328406,
    1759812671.944235,
    1759812671.9980571,
    1759812672.1273825,
    1759812672.1347165,
    1759812672.236254,
    1759812672.2539763,
    1759812672.2539284,
    1759812672.2578008,
    1759812672.295008,
    1759812672.3900728,
    1759812672.407384,
    1759812672.4620583,
    1759812672.4855316,
    1759812672.6203535,
    1759812672.7898338,
    1759812672.7967315,
    1759812673.1193354,
    1759812673.1711512,
    1759812673.3124228,
    1759812673.3282259,
    1759812673.3954463,
    1759812673.522502,
    1759812673.7683182,
    1759812673.8082597,
    1759812673.9124963,
    1759812673.9435902,
    1759812673.973463,
    1759812674.0750647,
    1759812674.1088972,
    1759812674.1089177,
    1759812674.1530323,
    1759812674.163934,
    1759812674.2086272,
    1759812674.226187,
    1759812674.2409873,
    1759812674.2439713,
    1759812674.3490522,
    1759812674.491571,
    1759812674.5667837,
    1759812674.6479597,
    1759812674.742487,
    1759812674.8934941,
    1759812674.9183543,
    1759812674.997069,
    1759812675.1376956,
    1759812675.1468673,
    1759812675.1710029,
    1759812675.337606,
    1759812675.3517525,
    1759812675.3688035,
    1759812675.4580257,
    1759812675.6259894,
    1759812675.6971595,
    1759812675.7532315,
    1759812675.870605,
    1759812675.9108858,
    1759812675.9325273,
    1759812675.9382594,
    1759812675.9671974,
    1759812676.0472379,
    1759812676.0902855,
    1759812676.147841,
    1759812676.2422345,
    1759812676.3074114,
    1759812676.3887613,
    1759812676.4077532,
    1759812676.4596093,
    1759812676.4907975,
    1759812676.4995031,
    1759812676.6985135,
    1759812676.8403916,
    1759812676.9546542,
    1759812676.971255,
    1759812677.0293212,
    1759812677.1077387,
    1759812677.1614962,
    1759812677.1903746,
    1759812677.1904202,
    1759812677.3705516,
    1759812677.3744466,
    1759812677.4562616,
    1759812677.5540395,
    1759812677.7369735,
    1759812677.7608473,
    1759812677.8633122,
    1759812677.8830228,
    1759812677.9303176,
    1759812677.9948235,
    1759812678.0454354,
    1759812678.1154814,
    1759812678.123043,
    1759812678.2247949,
    1759812678.233822,
    1759812678.2901766,
    1759812678.35633,
    1759812678.3600328,
    1759812678.3716729,
    1759812678.38855,
    1759812678.4049873,
    1759812678.4176586,
    1759812678.422238,
    1759812678.527472,
    1759812678.5774598,
    1759812678.6213672,
    1759812678.721625,
    1759812678.7701597,
    1759812678.775057,
    1759812678.880513,
    1759812678.8977249,
    1759812678.997599,
    1759812679.00344,
    1759812679.0815287,
    1759812679.1594965,
    1759812679.1923587,
    1759812679.312198,
    1759812679.3703258,
    1759812679.4958112,
    1759812679.7199998,
    1759812679.8307862,
    1759812679.8924904,
    1759812679.945313,
    1759812679.988045,
    1759812680.0936172,
    1759812680.0935962,
    1759812680.100963,
    1759812680.100926,
    1759812680.1416957,
    1759812680.1673083,
    1759812680.2489753,
    1759812680.2490168,
    1759812680.2571259,
    1759812680.2880337,
    1759812680.3021348,
    1759812680.4014552,
    1759812680.4199293,
    1759812680.452593,
    1759812680.5344853,
    1759812680.5386724,
    1759812680.5659318,
    1759812680.6757472,
    1759812680.8036478,
    1759812680.8841517,
    1759812681.0017521,
    1759812681.0308623,
    1759812681.123858,
    1759812681.169119,
    1759812681.1831045,
    1759812681.219577,
    1759812681.2874975,
    1759812681.3550527,
    1759812681.36556,
    1759812681.378111,
    1759812681.3860536,
    1759812681.419917,
    1759812681.4249403,
    1759812681.4890647,
    1759812681.514836,
    1759812681.6492374,
    1759812681.7333977,
    1759812681.8126364,
    1759812681.8896465,
    1759812681.9404552,
    1759812681.9939914,
    1759812682.1245568,
    1759812682.1800802,
    1759812682.3567781,
    1759812682.3631852,
    1759812682.45244,
    1759812682.4812114,
    1759812682.5504227,
    1759812682.5793066,
    1759812682.6377509,
    1759812682.7127006,
    1759812682.716722,
    1759812682.7496347,
    1759812682.9266353,
    1759812683.0096314,
    1759812683.070226,
    1759812683.1292284,
    1759812683.1648705,
    1759812683.1718712,
    1759812683.1879754,
    1759812683.3764954,
    1759812683.4574094,
    1759812683.6202707,
    1759812683.7565677,
    1759812683.8430963,
    1759812683.8482406,
    1759812683.9511166,
    1759812684.169508,
    1759812684.6020195,
    1759812684.9564984,
    1759812685.1111631,
    1759812685.2150292,
    1759812685.353139,
    1759812685.3997118,
    1759812685.4050243,
    1759812685.508617,
    1759812685.6117778,
    1759812685.628573,
    1759812685.6286197,
    1759812685.6323085,
    1759812685.6507626,
    1759812685.6879947,
    1759812685.7304206,
    1759812685.7698808,
    1759812685.8333223,
    1759812686.001096,
    1759812686.027045,
    1759812686.1695583,
    1759812686.2025979,
    1759812686.212291,
    1759812686.3884404,
    1759812686.397606,
    1759812686.3975632,
    1759812686.4830835,
    1759812686.543917,
    1759812686.5500658,
    1759812686.595882,
    1759812686.600218,
    1759812686.666595,
    1759812686.6665518,
    1759812686.6769803,
    1759812686.676946,
    1759812686.7342963,
    1759812686.7627995,
    1759812686.7672865,
    1759812686.7860863,
    1759812686.845175,
    1759812686.9944997,
    1759812687.07607,
    1759812687.0977595,
    1759812687.1008446,
    1759812687.2089539,
    1759812687.2262433,
    1759812687.2463052,
    1759812687.3067544,
    1759812687.3650677,
    1759812687.3822608,
    1759812687.4881883,
    1759812687.488207,
    1759812687.6280107,
    1759812687.6433234,
    1759812687.6896648,
    1759812687.7622247,
    1759812687.8868346,
    1759812687.9738765,
    1759812687.9877627,
    1759812688.026259,
    1759812688.0506585,
    1759812688.0546534,
    1759812688.101131,
    1759812688.1695986,
    1759812688.2897863,
    1759812688.289757,
    1759812688.3224359,
    1759812688.5115643,
    1759812688.5718133,
    1759812688.6844356,
    1759812688.7107482,
    1759812688.8610299,
    1759812688.8736882,
    1759812688.8737257,
    1759812688.9694772,
    1759812688.9763825,
    1759812689.0477464,
    1759812689.1668034,
    1759812689.2007806,
    1759812689.3387177,
    1759812689.3589365,
    1759812689.3788927,
    1759812689.4089525,
    1759812689.4789424,
    1759812689.5623848,
    1759812689.5749452,
    1759812689.6320193,
    1759812689.6369488,
    1759812689.7053866,
    1759812689.803097,
    1759812689.8834345,
    1759812689.955418,
    1759812690.0548108,
    1759812690.0928538,
    1759812690.153554,
    1759812690.2248955,
    1759812690.2585611,
    1759812690.2941089,
    1759812690.4147553,
    1759812690.4722347,
    1759812690.490968,
    1759812690.4998167,
    1759812690.5977733,
    1759812690.628602,
    1759812690.685743,
    1759812690.7223957,
    1759812690.7378674,
    1759812690.855096,
    1759812690.8966656,
    1759812691.110276,
    1759812691.2077904,
    1759812691.2203188,
    1759812691.258254,
    1759812691.5020938,
    1759812691.5331635,
    1759812691.6476533,
    1759812691.6995084,
    1759812691.7174006,
    1759812691.725215,
    1759812691.7399712,
    1759812691.8044086,
    1759812691.8275645,
    1759812691.8632424,
    1759812691.9847324,
    1759812692.0057766,
    1759812692.1224697,
    1759812692.1314583,
    1759812692.1519237,
    1759812692.352141,
    1759812692.3821583,
    1759812692.3873646,
    1759812692.4176202,
    1759812692.661825,
    1759812692.7544842,
    1759812692.774668,
    1759812692.806528,
    1759812692.8553045,
    1759812692.962592,
    1759812693.198281,
    1759812693.2204823,
    1759812693.2198691,
    1759812693.2782636,
    1759812693.3928773,
    1759812693.4582765,
    1759812693.538485,
    1759812693.5737507,
    1759812693.5833113,
    1759812693.7201889,
    1759812693.8087313,
    1759812693.8906727,
    1759812694.0544977,
    1759812694.1251757,
    1759812694.125221,
    1759812694.167343,
    1759812694.2416084,
    1759812694.3188734,
    1759812694.451144,
    1759812694.4576194,
    1759812694.5069492,
    1759812694.7706406,
    1759812694.8131428,
    1759812694.8908231,
    1759812694.9258618,
    1759812694.9316406,
    1759812694.9811487,
    1759812695.0266666,
    1759812695.2118738,
    1759812695.375148,
    1759812695.3841457,
    1759812695.4284198,
    1759812695.509334,
    1759812695.5632443,
    1759812695.5870712,
    1759812695.67917,
    1759812695.6991343,
    1759812695.6991718,
    1759812695.7675495,
    1759812695.868324,
    1759812695.8759398,
    1759812695.9569106,
    1759812696.0535862,
    1759812696.1073997,
    1759812696.1196854,
    1759812696.1453226,
    1759812696.254668,
    1759812696.2875051,
    1759812696.292295,
    1759812696.409907,
    1759812696.4295042,
    1759812696.450939,
    1759812696.459647,
    1759812696.6354823,
    1759812696.6836636,
    1759812696.8928862,
    1759812696.9357545,
    1759812696.9444342,
    1759812696.9530337,
    1759812696.960568,
    1759812697.0621057,
    1759812697.1037133,
    1759812697.1339316,
    1759812697.2492805,
    1759812697.4154966,
    1759812697.5540729,
    1759812697.6089008,
    1759812697.6089284,
    1759812697.7149394,
    1759812697.7940865,
    1759812697.9285398,
    1759812697.9371994,
    1759812698.0520818,
    1759812698.1637318,
    1759812698.419087,
    1759812698.5234616,
    1759812698.5443783,
    1759812698.585192,
    1759812698.6471968,
    1759812698.760077,
    1759812698.8377345,
    1759812698.851746,
    1759812699.1408644,
    1759812699.1953928,
    1759812699.2987545,
    1759812699.467496,
    1759812699.5164201,
    1759812699.5848465,
    1759812699.6225512,
    1759812699.622571,
    1759812699.7629483,
    1759812699.7829347,
    1759812700.0116034,
    1759812700.0415232,
    1759812700.0982547,
    1759812700.2339222,
    1759812700.2868488,
    1759812700.3277931,
    1759812700.4190314,
    1759812700.5181603,
    1759812700.5181108,
    1759812700.5268843,
    1759812700.6207247,
    1759812700.6488724,
    1759812700.6991303,
    1759812700.7709155,
    1759812700.805037,
    1759812700.8431866,
    1759812700.885316,
    1759812700.9920688,
    1759812701.521961,
    1759812701.521991,
    1759812701.6034951,
    1759812701.6480465,
    1759812701.7571695,
    1759812701.8302932,
    1759812701.87586,
    1759812701.9432597,
    1759812702.1012166,
    1759812702.2394965,
    1759812702.37367,
    1759812702.6200745,
    1759812702.6615605,
    1759812702.667172,
    1759812702.7458456,
    1759812702.7908766,
    1759812702.8690279,
    1759812702.9582386,
    1759812703.0248656,
    1759812703.2230706,
    1759812703.3345299,
    1759812703.3745713,
    1759812703.4553032,
    1759812703.4659264,
    1759812703.5986917,
    1759812703.6163034,
    1759812703.6376286,
    1759812703.6769052,
    1759812703.742923,
    1759812703.8624885,
    1759812703.9245987,
    1759812704.003653,
    1759812704.0953705,
    1759812704.2579682,
    1759812704.415207,
    1759812704.45878,
    1759812704.6182675,
    1759812704.6715012,
    1759812704.7078815,
    1759812704.838408,
    1759812704.8481073,
    1759812704.9008372,
    1759812704.9458275,
    1759812704.9589224,
    1759812705.0755427,
    1759812705.1381626,
    1759812705.2737606,
    1759812705.2942684,
    1759812705.3446004,
    1759812705.5561354,
    1759812705.5867188,
    1759812705.5966759,
    1759812705.8165896,
    1759812706.052135,
    1759812706.089066,
    1759812706.1434214,
    1759812706.1867626,
    1759812706.2363577,
    1759812706.3019664,
    1759812706.3795507,
    1759812706.4753537,
    1759812706.5318723,
    1759812706.5922496,
    1759812706.672096,
    1759812706.6848133,
    1759812706.70156,
    1759812706.7867184,
    1759812706.7946184,
    1759812706.8120923,
    1759812706.823046,
    1759812706.9727678,
    1759812707.0828857,
    1759812707.1113205,
    1759812707.1826313,
    1759812707.2355897,
    1759812707.2430997,
    1759812707.2823005,
    1759812707.3327513,
    1759812707.348969,
    1759812707.3789349,
    1759812707.407961,
    1759812707.6415334,
    1759812707.646718,
    1759812707.7680862,
    1759812707.801468,
    1759812707.8476882,
    1759812708.0319777,
    1759812708.066568,
    1759812708.1897686,
    1759812708.194093,
    1759812708.4062395,
    1759812708.4834678,
    1759812708.4911504,
    1759812708.5058389,
    1759812708.5641782,
    1759812708.6360407,
    1759812708.7257142,
    1759812708.7311194,
    1759812708.8697598,
    1759812708.9136348,
    1759812708.9409597,
    1759812709.0429287,
    1759812709.0742936,
    1759812709.1989877,
    1759812709.199016,
    1759812709.2304993,
    1759812709.238741,
    1759812709.2387772,
    1759812709.249731,
    1759812709.2859235,
    1759812709.3137286,
    1759812709.4475806,
    1759812709.5336504,
    1759812709.5447176,
    1759812709.595545,
    1759812709.6829786,
    1759812709.7429636,
    1759812709.8149087,
    1759812709.833885,
    1759812709.8338382,
    1759812709.837572,
    1759812709.938565,
    1759812709.9428587,
    1759812710.065046,
    1759812710.0822542,
    1759812710.2400665,
    1759812710.2922766,
    1759812710.3030887,
    1759812710.306868,
    1759812710.3887522,
    1759812710.4633977,
    1759812710.5138373,
    1759812710.5184238,
    1759812710.5399573,
    1759812710.5844507,
    1759812710.6705766,
    1759812710.6870933,
    1759812710.705781,
    1759812710.7389698,
    1759812710.7780335,
    1759812710.785084,
    1759812710.796134,
    1759812710.845057,
    1759812710.8507085,
    1759812711.01476,
    1759812711.0339475,
    1759812711.194349,
    1759812711.2677531,
    1759812711.3867888,
    1759812711.39594,
    1759812711.399966,
    1759812711.561519,
    1759812711.5671117,
    1759812711.6413927,
    1759812711.6620822,
    1759812711.7097387,
    1759812711.7778702,
    1759812711.7863529,
    1759812711.7963712,
    1759812711.8071399,
    1759812711.8226187,
    1759812711.8338714,
    1759812711.851707,
    1759812711.85167,
    1759812711.8829167,
    1759812711.9174051,
    1759812711.9264553,
    1759812712.0004275,
    1759812712.0273755,
    1759812712.0351303,
    1759812712.146162,
    1759812712.278217,
    1759812712.6584604,
    1759812712.6952593,
    1759812712.8053796,
    1759812712.8692455,
    1759812712.9246504,
    1759812713.119734,
    1759812713.1289387,
    1759812713.1830554,
    1759812713.2330713,
    1759812713.3359072,
    1759812713.3954496,
    1759812713.4367855,
    1759812713.5063045,
    1759812713.5133739,
    1759812713.6546333,
    1759812713.6756825,
    1759812713.7766526,
    1759812713.7766106,
    1759812713.9590151,
    1759812713.9657872,
    1759812713.998184,
    1759812714.0061722,
    1759812714.0351892,
    1759812714.116481,
    1759812714.181831,
    1759812714.26815,
    1759812714.416116,
    1759812714.4742124,
    1759812714.4887333,
    1759812714.741393,
    1759812714.8172855,
    1759812715.1399086,
    1759812715.1399462,
    1759812715.1499612,
    1759812715.2030017,
    1759812715.2128875,
    1759812715.3192017,
    1759812715.446045,
    1759812715.4861922,
    1759812715.4959288,
    1759812715.6427345,
    1759812715.7081177,
    1759812715.8040054,
    1759812715.8917985,
    1759812715.9296913,
    1759812715.9652658,
    1759812715.985579,
    1759812716.0276496,
    1759812716.0818138,
    1759812716.0877955,
    1759812716.1558487,
    1759812716.23235,
    1759812716.2462687,
    1759812716.284201,
    1759812716.357151,
    1759812716.3571959,
    1759812716.3722985,
    1759812716.4011755,
    1759812716.559272,
    1759812716.7113075,
    1759812716.7276297,
    1759812716.7490668,
    1759812716.8137593,
    1759812716.8763554,
    1759812716.913246,
    1759812716.9334388,
    1759812717.197367,
    1759812717.2319748,
    1759812717.2366164,
    1759812717.3284035,
    1759812717.3404279,
    1759812717.35974,
    1759812717.383796,
    1759812717.473328,
    1759812717.721916,
    1759812717.8298945,
    1759812717.9214175,
    1759812717.9251063,
    1759812717.9299703,
    1759812717.944023,
    1759812717.9942486,
    1759812718.017295,
    1759812718.1379578,
    1759812718.1553254,
    1759812718.202702,
    1759812718.2650733,
    1759812718.271379,
    1759812718.271417,
    1759812718.29385,
    1759812718.3390515,
    1759812718.477061,
    1759812718.6250246,
    1759812718.6810503,
    1759812718.7097726,
    1759812718.7549407,
    1759812718.7769282,
    1759812718.7768826,
    1759812718.8040118,
    1759812718.8040571,
    1759812718.8111694,
    1759812718.8112035,
    1759812718.8310475,
    1759812718.8693135,
    1759812718.9433024,
    1759812719.0417511,
    1759812719.0475347,
    1759812719.0631979,
    1759812719.115625,
    1759812719.115668,
    1759812719.2254636,
    1759812719.2305994,
    1759812719.248851,
    1759812719.276664,
    1759812719.3337932,
    1759812719.4095519,
    1759812719.446085,
    1759812719.480341,
    1759812719.5195334,
    1759812719.6562614,
    1759812719.7187335,
    1759812719.7966707,
    1759812719.7967176,
    1759812719.9091399,
    1759812720.0506191,
    1759812720.0706291,
    1759812720.109487,
    1759812720.252117,
    1759812720.2622783,
    1759812720.3159473,
    1759812720.5251744,
    1759812720.5445638,
    1759812720.6476145,
    1759812720.6523128,
    1759812720.6618133,
    1759812720.8566449,
    1759812720.8866131,
    1759812721.4118814,
    1759812721.541131,
    1759812721.6253002,
    1759812721.6823997,
    1759812721.7679107,
    1759812721.8741841,
    1759812721.9681904,
    1759812721.9929297,
    1759812722.0737107,
    1759812722.0779924,
    1759812722.145594,
    1759812722.2422643,
    1759812722.312913,
    1759812722.3227785,
    1759812722.5469449,
    1759812722.546931,
    1759812722.546913,
    1759812722.5468836,
    1759812722.7100105,
    1759812722.8936703,
    1759812722.979511,
    1759812723.2258823,
    1759812723.2426724,
    1759812723.2495756,
    1759812723.3811197,
    1759812723.4201367,
    1759812723.4794648,
    1759812723.5045176,
    1759812723.5400612,
    1759812723.603318,
    1759812723.6032736,
    1759812723.7552192,
    1759812723.790469,
    1759812723.8083239,
    1759812723.9037626,
    1759812723.9131072,
    1759812724.0146148,
    1759812724.1217947,
    1759812724.20696,
    1759812724.2236958,
    1759812724.2706122,
    1759812724.2896914,
    1759812724.2896712,
    1759812724.2896826,
    1759812724.2896986,
    1759812724.3228016,
    1759812724.4245982,
    1759812724.652569,
    1759812724.7395008,
    1759812724.8040452,
    1759812724.9067025,
    1759812724.913145,
    1759812725.022214,
    1759812725.022171,
    1759812725.061616,
    1759812725.0659237,
    1759812725.0997534,
    1759812725.3781624,
    1759812725.4506125,
    1759812725.467852,
    1759812725.5146303,
    1759812725.53155,
    1759812725.5755534,
    1759812725.635332,
    1759812725.6475594,
    1759812725.686532,
    1759812725.7020602,
    1759812725.780004,
    1759812726.0669503,
    1759812726.0891364,
    1759812726.089175,
    1759812726.1199756,
    1759812726.1485145,
    1759812726.289551,
    1759812726.351924,
    1759812726.5469542,
    1759812726.5469105,
    1759812726.5702446,
    1759812726.6500835,
    1759812726.6572416,
    1759812726.7180138,
    1759812726.7411108,
    1759812726.7809732,
    1759812726.8005934,
    1759812726.8059623,
    1759812726.9398398,
    1759812726.9590464,
    1759812727.144835,
    1759812727.1487303,
    1759812727.2684226,
    1759812727.2911074,
    1759812727.370866,
    1759812727.4975562,
    1759812727.5299747,
    1759812727.5345159,
    1759812727.5545456,
    1759812727.6099677,
    1759812727.639716,
    1759812727.7254186,
    1759812727.735504,
    1759812727.7354674,
    1759812727.7714818,
    1759812728.0706606,
    1759812728.1423204,
    1759812728.161748,
    1759812728.251185,
    1759812728.3778107,
    1759812728.4252264,
    1759812728.443383,
    1759812728.489399,
    1759812728.6213794,
    1759812728.7037349,
    1759812728.7964349,
    1759812728.8124263,
    1759812728.8538465,
    1759812728.8574483,
    1759812729.095091,
    1759812729.1669176,
    1759812729.224855,
    1759812729.2364361,
    1759812729.3302624,
    1759812729.3379683,
    1759812729.3845887,
    1759812729.3983676,
    1759812729.506643,
    1759812729.5134532,
    1759812729.5858061,
    1759812729.6736374,
    1759812729.7031248,
    1759812729.9278123,
    1759812730.011541,
    1759812730.0186396,
    1759812730.0452383,
    1759812730.0501037,
    1759812730.0552788,
    1759812730.0680602,
    1759812730.170528,
    1759812730.2002354,
    1759812730.2218323,
    1759812730.2725863,
    1759812730.2769809,
    1759812730.288043,
    1759812730.361506,
    1759812730.4517777,
    1759812730.487707,
    1759812730.6393943,
    1759812730.6913302,
    1759812730.6912851,
    1759812730.9907002,
    1759812730.99511,
    1759812731.02149,
    1759812731.0557718,
    1759812731.0848997,
    1759812731.2092412,
    1759812731.2619216,
    1759812731.288721,
    1759812731.3644104,
    1759812731.7880988,
    1759812731.802738,
    1759812731.8730118,
    1759812731.939094,
    1759812731.973128,
    1759812732.0589051,
    1759812732.127747,
    1759812732.2264023,
    1759812732.3968234,
    1759812732.4125426,
    1759812732.412582,
    1759812732.4307532,
    1759812732.4670765,
    1759812732.517635,
    1759812732.6083107,
    1759812732.6157253,
    1759812732.841415,
    1759812732.86998,
    1759812733.02626,
    1759812733.1147244,
    1759812733.1820917,
    1759812733.2635434,
    1759812733.3089254,
    1759812733.3172538,
    1759812733.321626,
    1759812733.3416128,
    1759812733.3768783,
    1759812733.5053356,
    1759812733.6568823,
    1759812733.858389,
    1759812733.9199538,
    1759812733.9257634,
    1759812733.9476163,
    1759812734.0167189,
    1759812734.0891492,
    1759812734.2045348,
    1759812734.2333274,
    1759812734.2332826,
    1759812734.3730006,
    1759812734.4379883,
    1759812734.4429235,
    1759812734.5979445,
    1759812734.7070622,
    1759812734.8742065,
    1759812734.8866115,
    1759812735.1108365,
    1759812735.1219254,
    1759812735.128019,
    1759812735.1449628,
    1759812735.169198,
    1759812735.1692433,
    1759812735.2052038,
    1759812735.2327135,
    1759812735.3334205,
    1759812735.3887575,
    1759812735.4665177,
    1759812735.4986916,
    1759812735.5068982,
    1759812735.5480444,
    1759812735.6611404,
    1759812735.7779162,
    1759812735.879806,
    1759812735.9252427,
    1759812736.0256312,
    1759812736.025599,
    1759812736.0783265,
    1759812736.1644242,
    1759812736.1782618,
    1759812736.2214944,
    1759812736.2285342,
    1759812736.2824633,
    1759812736.282497,
    1759812736.3020024,
    1759812736.31039,
    1759812736.526155,
    1759812736.5681825,
    1759812736.574969,
    1759812736.5749319,
    1759812736.7786524,
    1759812736.8669124,
    1759812736.963657,
    1759812737.1443777,
    1759812737.2331173,
    1759812737.249399,
    1759812737.3217924,
    1759812737.3752115,
    1759812737.7062752,
    1759812737.8946826,
    1759812737.9466279,
    1759812738.0977323,
    1759812738.1128538,
    1759812738.3386216,
    1759812738.363988,
    1759812738.5021214,
    1759812738.5413141,
    1759812738.5627584,
    1759812738.6047068,
    1759812738.6972206,
    1759812738.7416492,
    1759812738.7759619,
    1759812738.7795966,
    1759812738.862995,
    1759812738.867833,
    1759812739.0831995,
    1759812739.0947733,
    1759812739.0948129,
    1759812739.1181765,
    1759812739.1662638,
    1759812739.2449307,
    1759812739.2864642,
    1759812739.468277,
    1759812739.6296089,
    1759812739.715862,
    1759812739.7275774,
    1759812739.934683,
    1759812740.1316137,
    1759812740.1749413,
    1759812740.2853208,
    1759812740.2924476,
    1759812740.3078907,
    1759812740.3131485,
    1759812740.3655887,
    1759812740.5153472,
    1759812740.5338397,
    1759812740.578344,
    1759812740.6560957,
    1759812740.9243436,
    1759812740.971425,
    1759812740.998785,
    1759812741.2549286,
    1759812741.3894625,
    1759812741.448012,
    1759812741.601281,
    1759812741.620533,
    1759812741.7209983,
    1759812741.740678,
    1759812741.8324459,
    1759812741.8887894,
    1759812741.8946888,
    1759812741.9267836,
    1759812741.9484544,
    1759812741.9573061,
    1759812742.0946834,
    1759812742.1858263,
    1759812742.18979,
    1759812742.2297137,
    1759812742.2676847,
    1759812742.2880838,
    1759812742.353928,
    1759812742.366076,
    1759812742.4620225,
    1759812742.4889152,
    1759812742.56141,
    1759812742.5744622,
    1759812742.6084359,
    1759812742.6150043,
    1759812742.6385922,
    1759812742.708527,
    1759812742.7084813,
    1759812742.7276702,
    1759812742.8637486,
    1759812742.8807976,
    1759812742.9677322,
    1759812743.1043053,
    1759812743.2843826,
    1759812743.3816926,
    1759812743.5188756,
    1759812743.5718608,
    1759812743.6093931,
    1759812743.6492295,
    1759812743.657083,
    1759812743.7259452,
    1759812743.7993455,
    1759812743.8271995,
    1759812743.871055,
    1759812743.9360545,
    1759812743.947734,
    1759812743.954952,
    1759812743.9549115,
    1759812744.023438,
    1759812744.0276444,
    1759812744.0898907,
    1759812744.2323654,
    1759812744.3400874,
    1759812744.3530924,
    1759812744.4023266,
    1759812744.4340935,
    1759812744.440784,
    1759812744.5185847,
    1759812744.6322606,
    1759812744.6362543,
    1759812744.658679,
    1759812744.6782424,
    1759812744.7317994,
    1759812744.7674854,
    1759812744.8165805,
    1759812744.919102,
    1759812744.9387622,
    1759812744.9875956,
    1759812745.016536,
    1759812745.126267,
    1759812745.142592,
    1759812745.1787229,
    1759812745.2348702,
    1759812745.2659051,
    1759812745.276536,
    1759812745.3142564,
    1759812745.3142126,
    1759812745.4581378,
    1759812745.484222,
    1759812745.5273685,
    1759812745.6776052,
    1759812745.7644083,
    1759812745.8042173,
    1759812745.81532,
    1759812745.9754822,
    1759812746.0485284,
    1759812746.0648906,
    1759812746.109614,
    1759812746.1473646,
    1759812746.1727622,
    1759812746.1881864,
    1759812746.1964388,
    1759812746.263212,
    1759812746.3659446,
    1759812746.483636,
    1759812746.5527422,
    1759812746.5594106,
    1759812746.5843854,
    1759812746.6181455,
    1759812746.683798,
    1759812746.7511451,
    1759812746.7987463,
    1759812746.8270218,
    1759812746.86851,
    1759812746.9929426,
    1759812747.0705595,
    1759812747.0787244,
    1759812747.1369474,
    1759812747.1598947,
    1759812747.2613604,
    1759812747.2909696,
    1759812747.3508046,
    1759812747.3793833,
    1759812747.428381,
    1759812747.4328241,
    1759812747.4418676,
    1759812747.4446542,
    1759812747.4947093,
    1759812747.5866063,
    1759812747.6019406,
    1759812747.6196127,
    1759812747.6723707,
    1759812747.6957147,
    1759812747.8549562,
    1759812747.86807,
    1759812747.991468,
    1759812748.0663984,
    1759812748.120874,
    1759812748.2816148,
    1759812748.3266404,
    1759812748.3915431,
    1759812748.4219599,
    1759812748.4366374,
    1759812748.4721694,
    1759812748.5806863,
    1759812748.6378913,
    1759812748.651668,
    1759812748.6675348,
    1759812748.8105102,
    1759812748.810556,
    1759812748.9443913,
    1759812748.984476,
    1759812749.0836515,
    1759812749.1198826,
    1759812749.200016,
    1759812749.2524734,
    1759812749.2875373,
    1759812749.4041522,
    1759812749.4659534,
    1759812749.4959352,
    1759812749.7354543,
    1759812749.7698443,
    1759812749.829663,
    1759812749.8571756,
    1759812749.8621845,
    1759812749.9776363,
    1759812750.1858091,
    1759812750.2397869,
    1759812750.268364,
    1759812750.2977996,
    1759812750.454091,
    1759812750.4805558,
    1759812750.517257,
    1759812750.5365999,
    1759812750.5804243,
    1759812750.598547,
    1759812750.88558,
    1759812750.9581857,
    1759812751.1705875,
    1759812751.2285614,
    1759812751.2402442,
    1759812751.271971,
    1759812751.346861,
    1759812751.4693549,
    1759812751.4694073,
    1759812751.4937537,
    1759812751.5196028,
    1759812751.5469604,
    1759812751.5770435,
    1759812751.6690247,
    1759812751.6957157,
    1759812751.7819486,
    1759812751.8585021,
    1759812751.9194026,
    1759812751.9696262,
    1759812752.0090866,
    1759812752.0553298,
    1759812752.062201,
    1759812752.265163,
    1759812752.3749678,
    1759812752.4621034,
    1759812752.5608563,
    1759812752.749232,
    1759812752.7896717,
    1759812752.9099803,
    1759812752.946869,
    1759812753.1222925,
    1759812753.1367433,
    1759812753.1494584,
    1759812753.3668306,
    1759812753.4056268,
    1759812753.457482,
    1759812753.5261376,
    1759812753.7280357,
    1759812753.872583,
    1759812754.0793693,
    1759812754.1619022,
    1759812754.1671453,
    1759812754.1993113,
    1759812754.2311366,
    1759812754.2969625,
    1759812754.398917,
    1759812754.5937703,
    1759812754.815397,
    1759812754.8649733,
    1759812754.8703055,
    1759812754.9006097,
    1759812754.972217,
    1759812754.9951386,
    1759812755.0737038,
    1759812755.102846,
    1759812755.1315205,
    1759812755.1774087,
    1759812755.2262652,
    1759812755.2407026,
    1759812755.2470124,
    1759812755.2498834,
    1759812755.2914288,
    1759812755.358887,
    1759812755.4129932,
    1759812755.444951,
    1759812755.6482465,
    1759812755.7140026,
    1759812755.7392857,
    1759812755.800038,
    1759812755.9149199,
    1759812755.9959297,
    1759812755.9998944,
    1759812756.358471,
    1759812756.3584998,
    1759812756.3585153,
    1759812756.358528,
    1759812756.4896379,
    1759812756.6137838,
    1759812756.6679802,
    1759812756.697301,
    1759812756.7933798,
    1759812756.839403,
    1759812756.8809507,
    1759812756.9084222,
    1759812756.9534774,
    1759812757.0708532,
    1759812757.1102595,
    1759812757.1102154,
    1759812757.1251218,
    1759812757.1910543,
    1759812757.2221513,
    1759812757.2482336,
    1759812757.3060791,
    1759812757.369416,
    1759812757.4217706,
    1759812757.4356754,
    1759812757.4683309,
    1759812757.5283074,
    1759812757.6049469,
    1759812757.746899,
    1759812757.8731735,
    1759812757.9220674,
    1759812758.0614805,
    1759812758.0926054,
    1759812758.1228397,
    1759812758.2467399,
    1759812758.264932,
    1759812758.2861788,
    1759812758.2939703,
    1759812758.3623962,
    1759812758.3623676,
    1759812758.4629555,
    1759812758.847206,
    1759812758.903396,
    1759812759.0471141,
    1759812759.0761979,
    1759812759.106415,
    1759812759.1106803,
    1759812759.1210058,
    1759812759.2021418,
    1759812759.3643348,
    1759812759.3808873,
    1759812759.4031317,
    1759812759.4086735,
    1759812759.491402,
    1759812759.519787,
    1759812759.5636256,
    1759812759.572747,
    1759812759.7274964,
    1759812759.7401109,
    1759812759.8971448,
    1759812759.9031208,
    1759812760.003028,
    1759812760.0094898,
    1759812760.0147285,
    1759812760.2532635,
    1759812760.2689054,
    1759812760.271661,
    1759812760.403231,
    1759812760.4032798,
    1759812760.4300797,
    1759812760.5500338,
    1759812760.6352868,
    1759812760.6561813,
    1759812760.6613488,
    1759812760.6911023,
    1759812760.9345129,
    1759812760.960152,
    1759812761.0589755,
    1759812761.0926604,
    1759812761.1617758,
    1759812761.3379803,
    1759812761.4593976,
    1759812761.6486375,
    1759812761.6702933,
    1759812761.6821911,
    1759812761.6821475,
    1759812761.73671,
    1759812761.7501676,
    1759812761.825337,
    1759812761.8337297,
    1759812761.8336904,
    1759812761.9643824,
    1759812762.0515428,
    1759812762.085895,
    1759812762.143377,
    1759812762.1587098,
    1759812762.2063975,
    1759812762.2400544,
    1759812762.33839,
    1759812762.3903375,
    1759812762.5362954,
    1759812762.6165056,
    1759812762.6207988,
    1759812762.6240568,
    1759812762.6413445,
    1759812762.8423805,
    1759812762.8747492,
    1759812762.9603705,
    1759812763.1527271,
    1759812763.3084767,
    1759812763.3545237,
    1759812763.3608654,
    1759812763.414193,
    1759812763.42093,
    1759812763.4209661,
    1759812763.5118568,
    1759812763.543383,
    1759812763.562211,
    1759812763.5994906,
    1759812763.6143005,
    1759812763.6624978,
    1759812763.7219663,
    1759812763.771237,
    1759812763.8187807,
    1759812763.9689355,
    1759812763.984512,
    1759812763.9947271,
    1759812764.0197108,
    1759812764.1847382,
    1759812764.1920772,
    1759812764.2866724,
    1759812764.293525,
    1759812764.299316,
    1759812764.3888152,
    1759812764.4074965,
    1759812764.586821,
    1759812764.6083548,
    1759812764.6483033,
    1759812764.8077908,
    1759812764.9100404,
    1759812764.9430444,
    1759812764.968659,
    1759812764.9686935,
    1759812765.0173166,
    1759812765.021993,
    1759812765.1577945,
    1759812765.2795782,
    1759812765.285109,
    1759812765.3336148,
    1759812765.3419387,
    1759812765.4215147,
    1759812765.4358692,
    1759812765.4358213,
    1759812765.4358823,
    1759812765.4694164,
    1759812765.4974136,
    1759812765.549829,
    1759812765.6288393,
    1759812765.7196834,
    1759812765.7547522,
    1759812765.754798,
    1759812765.788886,
    1759812765.7983418,
    1759812765.7984085,
    1759812765.798395,
    1759812765.8643508,
    1759812765.9206073,
    1759812765.9324615,
    1759812766.0921347,
    1759812766.1907332,
    1759812766.2289953,
    1759812766.2357976,
    1759812766.2724674,
    1759812766.405364,
    1759812766.4112651,
    1759812766.4361823,
    1759812766.4753673,
    1759812766.5091958,
    1759812766.534875,
    1759812766.5449443,
    1759812766.565156,
    1759812766.6142187,
    1759812766.701195,
    1759812766.7768538,
    1759812766.8047078,
    1759812766.8370335,
    1759812766.919578,
    1759812767.0439093,
    1759812767.059161,
    1759812767.095245,
    1759812767.2774503,
    1759812767.47822,
    1759812767.5901785,
    1759812767.628645,
    1759812767.6662855,
    1759812767.6858637,
    1759812767.6927865,
    1759812767.7035255,
    1759812767.999638,
    1759812768.1274064,
    1759812768.1274571,
    1759812768.1716812,
    1759812768.1952605,
    1759812768.2058163,
    1759812768.205854,
    1759812768.221063,
    1759812768.288371,
    1759812768.2942982,
    1759812768.3250463,
    1759812768.5273502,
    1759812768.551507,
    1759812768.5566235,
    1759812768.6303957,
    1759812768.669858,
    1759812768.6868634,
    1759812768.7341871,
    1759812769.094327,
    1759812769.197778,
    1759812769.1977346,
    1759812769.2805972,
    1759812769.301076,
    1759812769.3308601,
    1759812769.3989024,
    1759812769.4572327,
    1759812769.5240757,
    1759812769.524144,
    1759812769.524129,
    1759812769.7393084,
    1759812769.7761574,
    1759812769.7999237,
    1759812769.8257365,
    1759812769.8502622,
    1759812769.8650393,
    1759812769.9384427,
    1759812769.9753172,
    1759812769.994684,
    1759812770.0985558,
    1759812770.157077,
    1759812770.2482014,
    1759812770.2953584,
    1759812770.3282228,
    1759812770.3383586,
    1759812770.5077803,
    1759812770.5201068,
    1759812770.5200646,
    1759812770.561096,
    1759812770.610278,
    1759812770.6717517,
    1759812770.8962476,
    1759812770.943618,
    1759812770.979306,
    1759812770.9906986,
    1759812771.0788379,
    1759812771.2186387,
    1759812771.2814653,
    1759812771.5175116,
    1759812771.5828147,
    1759812771.597059,
    1759812771.605917,
    1759812771.892218,
    1759812771.892256,
    1759812772.2701018,
    1759812772.351381,
    1759812772.3659542,
    1759812772.4920166,
    1759812772.5451584,
    1759812772.5824132,
    1759812772.6283703,
    1759812772.6668506,
    1759812772.7581506,
    1759812772.7758718,
    1759812772.9020724,
    1759812773.0019362,
    1759812773.2210143,
    1759812773.2657633,
    1759812773.3356352,
    1759812773.498112,
    1759812773.6349397,
    1759812773.8094857,
    1759812773.8500128,
    1759812773.8549473,
    1759812773.9640465,
    1759812773.978427,
    1759812773.978466,
    1759812773.9970813,
    1759812774.0402553,
    1759812774.0644076,
    1759812774.0761747,
    1759812774.083447,
    1759812774.1536372,
    1759812774.1685183,
    1759812774.2267349,
    1759812774.3461714,
    1759812774.3739016,
    1759812774.6042147,
    1759812774.650073,
    1759812774.6857476,
    1759812774.7018142,
    1759812774.715486,
    1759812774.7970867,
    1759812774.7970214,
    1759812774.7971008,
    1759812774.8136299,
    1759812775.027609,
    1759812775.0454957,
    1759812775.0788105,
    1759812775.1250381,
    1759812775.2390978,
    1759812775.2636566,
    1759812775.3235672,
    1759812775.3858967,
    1759812775.436196,
    1759812775.4515042,
    1759812775.4844968,
    1759812775.5792305,
    1759812775.6281464,
    1759812775.654932,
    1759812775.7771394,
    1759812775.8445323,
    1759812775.8834732,
    1759812775.9125311,
    1759812776.0054448,
    1759812776.0942407,
    1759812776.1036935,
    1759812776.147575,
    1759812776.1811416,
    1759812776.271733,
    1759812776.351772,
    1759812776.3797197,
    1759812776.387488,
    1759812776.475235,
    1759812776.5611398,
    1759812776.7475958,
    1759812776.7840016,
    1759812776.868259,
    1759812776.9156194,
    1759812776.9522486,
    1759812776.971988,
    1759812777.000527,
    1759812777.0371132,
    1759812777.0416453,
    1759812777.1033967,
    1759812777.1524272,
    1759812777.203751,
    1759812777.249311,
    1759812777.2567122,
    1759812777.391761,
    1759812777.5385246,
    1759812777.5721302,
    1759812777.593532,
    1759812777.5965989,
    1759812777.63303,
    1759812777.6361132,
    1759812777.7578459,
    1759812777.8059876,
    1759812777.8534799,
    1759812777.8573642,
    1759812777.9714684,
    1759812777.9872901,
    1759812778.1487658,
    1759812778.166755,
    1759812778.2156487,
    1759812778.2385926,
    1759812778.2644434,
    1759812778.332079,
    1759812778.416777,
    1759812778.4719448,
    1759812778.4806242,
    1759812778.5307374,
    1759812778.5695653,
    1759812778.6263676,
    1759812778.6402805,
    1759812778.6729188,
    1759812778.7148716,
    1759812778.7234843,
    1759812778.7736108,
    1759812778.8179088,
    1759812778.860273,
    1759812778.9958622,
    1759812779.0514681,
    1759812779.0591211,
    1759812779.1000466,
    1759812779.1776106,
    1759812779.211804,
    1759812779.2455328,
    1759812779.2953475,
    1759812779.3823507,
    1759812779.6794949,
    1759812779.72046,
    1759812779.7800527,
    1759812779.8095956,
    1759812779.8340852,
    1759812779.9332838,
    1759812779.9852943,
    1759812780.0607622,
    1759812780.2002935,
    1759812780.4277964,
    1759812780.613086,
    1759812780.6373878,
    1759812780.7702534,
    1759812780.798091,
    1759812780.8266993,
    1759812780.8304703,
    1759812780.8675528,
    1759812781.0862684,
    1759812781.1584332,
    1759812781.2171776,
    1759812781.2889826,
    1759812781.3505294,
    1759812781.3505666,
    1759812781.3977199,
    1759812781.4046822,
    1759812781.439438,
    1759812781.4921198,
    1759812781.5516467,
    1759812781.7550254,
    1759812781.7641387,
    1759812781.8639235,
    1759812781.8931043,
    1759812781.9389534,
    1759812781.948375,
    1759812782.0012197,
    1759812782.0259504,
    1759812782.0510976,
    1759812782.0511143,
    1759812782.05104,
    1759812782.1517482,
    1759812782.1601605,
    1759812782.218587,
    1759812782.6792762,
    1759812782.7931144,
    1759812783.0020344,
    1759812783.0990849,
    1759812783.153207,
    1759812783.1582532,
    1759812783.22248,
    1759812783.3488028,
    1759812783.4108994,
    1759812783.477526,
    1759812783.485472,
    1759812783.581904,
    1759812783.6147048,
    1759812783.6196244,
    1759812783.6809492,
    1759812783.7165596,
    1759812783.7825863,
    1759812783.884855,
    1759812784.0441222,
    1759812784.0584924,
    1759812784.05853,
    1759812784.073969,
    1759812784.1251695,
    1759812784.1397257,
    1759812784.2040908,
    1759812784.2807665,
    1759812784.313817,
    1759812784.3685634,
    1759812784.386676,
    1759812784.5404527,
    1759812784.8433268,
    1759812784.892529,
    1759812785.092248,
    1759812785.1563742,
    1759812785.3415685,
    1759812785.3773293,
    1759812785.4096928,
    1759812785.5201635,
    1759812785.6934738,
    1759812785.9188216,
    1759812785.9188595,
    1759812785.9231732,
    1759812785.9586089,
    1759812785.9586537,
    1759812786.0880835,
    1759812786.1143847,
    1759812786.1467292,
    1759812786.146689,
    1759812786.2515604,
    1759812786.3011186,
    1759812786.3130596,
    1759812786.3759706,
    1759812786.3870738,
    1759812786.4348907,
    1759812786.4572713,
    1759812786.513187,
    1759812786.5916715,
    1759812786.6213045,
    1759812786.76151,
    1759812786.9664214,
    1759812786.9664593,
    1759812786.9784799,
    1759812787.0365276,
    1759812787.0542893,
    1759812787.2113194,
    1759812787.2768028,
    1759812787.3058846,
    1759812787.3164318,
    1759812787.3220127,
    1759812787.4720216,
    1759812787.512672,
    1759812787.5321417,
    1759812787.5321295,
    1759812787.5769079,
    1759812787.7563112,
    1759812787.802875,
    1759812787.808273,
    1759812787.8425543,
    1759812787.9130096,
    1759812787.936572,
    1759812787.9992528,
    1759812788.1574957,
    1759812788.317221,
    1759812788.3343077,
    1759812788.450994,
    1759812788.477303,
    1759812788.485075,
    1759812788.5626523,
    1759812788.5677893,
    1759812788.6858852,
    1759812788.7107186,
    1759812788.716643,
    1759812788.7546306,
    1759812788.8097794,
    1759812788.8725724,
    1759812788.8936853,
    1759812788.893724,
    1759812789.011314,
    1759812789.1209185,
    1759812789.202176,
    1759812789.238431,
    1759812789.2717059,
    1759812789.3231175,
    1759812789.3438637,
    1759812789.444326,
    1759812789.4538758,
    1759812789.4892273,
    1759812789.5526512,
    1759812789.5860047,
    1759812789.715306,
    1759812789.7153897,
    1759812789.7154014,
    1759812789.7153738,
    1759812789.729008,
    1759812789.943458,
    1759812790.0358975,
    1759812790.1670763,
    1759812790.1843116,
    1759812790.3515236,
    1759812790.4585435,
    1759812790.6132429,
    1759812790.6543906,
    1759812790.663901,
    1759812790.6734924,
    1759812790.818691,
    1759812790.9416947,
    1759812790.9869661,
    1759812791.0475793,
    1759812791.086215,
    1759812791.3100708,
    1759812791.4438095,
    1759812791.4438574,
    1759812791.5531576,
    1759812791.580388,
    1759812791.6430206,
    1759812791.720511,
    1759812791.7907257,
    1759812791.8353708,
    1759812792.0355568,
    1759812792.0512521,
    1759812792.083408,
    1759812792.1495624,
    1759812792.1802585,
    1759812792.1853998,
    1759812792.2121549,
    1759812792.3494308,
    1759812792.4325788,
    1759812792.4362972,
    1759812792.450131,
    1759812792.4890695,
    1759812792.5810292,
    1759812792.5876384,
    1759812792.6576638,
    1759812792.7383254,
    1759812792.7475133,
    1759812792.794998,
    1759812792.8340871,
    1759812792.8419771,
    1759812792.8831677,
    1759812792.9010212,
    1759812792.9161386,
    1759812793.0109463,
    1759812793.0493238,
    1759812793.0701845,
    1759812793.0838397,
    1759812793.0872881,
    1759812793.133404,
    1759812793.1745877,
    1759812793.2143223,
    1759812793.2737134,
    1759812793.3260198,
    1759812793.4032373,
    1759812793.4394972,
    1759812793.4522471,
    1759812793.572234,
    1759812793.6603386,
    1759812793.9453425,
    1759812794.0519147,
    1759812794.087949,
    1759812794.1385853,
    1759812794.178346,
    1759812794.2668726,
    1759812794.3337,
    1759812794.4362407,
    1759812794.5513465,
    1759812794.7048607,
    1759812794.7801557,
    1759812794.7873557,
    1759812794.7920423,
    1759812794.8295102,
    1759812794.938648,
    1759812794.947053,
    1759812794.9604237,
    1759812795.0231194,
    1759812795.0797825,
    1759812795.1156178,
    1759812795.1649911,
    1759812795.1676867,
    1759812795.3612208,
    1759812795.4036813,
    1759812795.738041,
    1759812796.003305,
    1759812796.0703917,
    1759812796.1260564,
    1759812796.172517,
    1759812796.2251098,
    1759812796.2991486,
    1759812796.299193,
    1759812796.3893814,
    1759812796.4032922,
    1759812796.4552689,
    1759812796.4552944,
    1759812796.459185,
    1759812796.5719156,
    1759812796.5877218,
    1759812796.6124973,
    1759812796.6404266,
    1759812796.7169473,
    1759812796.771518,
    1759812796.859328,
    1759812796.876633,
    1759812796.926903,
    1759812796.92686,
    1759812797.2287564,
    1759812797.3234806,
    1759812797.3234391,
    1759812797.419135,
    1759812797.4229333,
    1759812797.4405346,
    1759812797.4698856,
    1759812797.4936202,
    1759812797.53794,
    1759812797.5448437,
    1759812797.626645,
    1759812797.6266906,
    1759812797.6575398,
    1759812797.6656764,
    1759812797.7187445,
    1759812797.7353058,
    1759812797.7559729,
    1759812797.7878034,
    1759812797.7976713,
    1759812797.8640852,
    1759812797.9090974,
    1759812797.9201367,
    1759812797.9319983,
    1759812797.9788785,
    1759812798.0092022,
    1759812798.0909731,
    1759812798.1410491,
    1759812798.2152133,
    1759812798.2412446,
    1759812798.2927458,
    1759812798.316806,
    1759812798.4262166,
    1759812798.432464,
    1759812798.5586894,
    1759812798.6413147,
    1759812798.898346,
    1759812798.9015522,
    1759812798.965575,
    1759812798.9897447,
    1759812799.0323913,
    1759812799.120477,
    1759812799.1776416,
    1759812799.370447,
    1759812799.6388643,
    1759812799.679094,
    1759812799.758343,
    1759812799.8078196,
    1759812799.8335412,
    1759812799.885274,
    1759812799.8963053,
    1759812799.961167,
    1759812799.9724045,
    1759812800.139919,
    1759812800.174004,
    1759812800.2961318,
    1759812800.4325204,
    1759812800.4974792,
    1759812800.5467808,
    1759812800.585969,
    1759812800.6447854,
    1759812800.6772516,
    1759812800.710854,
    1759812800.7422268,
    1759812800.7948287,
    1759812800.8692749,
    1759812800.9021924,
    1759812800.9287472,
    1759812800.9985442,
    1759812801.06758,
    1759812801.1199954,
    1759812801.1559088,
    1759812801.1838026,
    1759812801.2164967,
    1759812801.253471,
    1759812801.3090537,
    1759812801.6213427,
    1759812801.6748018,
    1759812801.7830205,
    1759812801.8132148,
    1759812801.9892561,
    1759812802.113152,
    1759812802.1848671,
    1759812802.337351,
    1759812802.3373034,
    1759812802.38937,
    1759812802.4169908,
    1759812802.6158037,
    1759812802.7735834,
    1759812802.7967937,
    1759812802.8762345,
    1759812802.899109,
    1759812802.908533,
    1759812802.9632897,
    1759812803.2616599,
    1759812803.2879353,
    1759812803.4927154,
    1759812803.5557494,
    1759812803.6068714,
    1759812803.6178098,
    1759812803.8921857,
    1759812803.927894,
    1759812804.146934,
    1759812804.1520188,
    1759812804.2456553,
    1759812804.2950087,
    1759812804.314365,
    1759812804.3465586,
    1759812804.3557513,
    1759812804.5046458,
    1759812804.5821304,
    1759812804.5949242,
    1759812804.8521664,
    1759812804.8622386,
    1759812805.0530028,
    1759812805.0736437,
    1759812805.118793,
    1759812805.164194,
    1759812805.3326004,
    1759812805.3827784,
    1759812805.5508475,
    1759812805.6011186,
    1759812805.7420533,
    1759812805.7950382,
    1759812805.8909051,
    1759812805.9593294,
    1759812805.9839451,
    1759812806.0585103,
    1759812806.1077912,
    1759812806.2857695,
    1759812806.3845394,
    1759812806.6044512,
    1759812806.6144392,
    1759812806.631685,
    1759812806.6495183,
    1759812806.8504272,
    1759812806.8912053,
    1759812806.8912432,
    1759812806.891186,
    1759812806.90133,
    1759812806.9013577,
    1759812806.9114964,
    1759812806.9316032,
    1759812806.9491532,
    1759812807.007552,
    1759812807.0368636,
    1759812807.093749,
    1759812807.1114943,
    1759812807.162694,
    1759812807.3168766,
    1759812807.3479133,
    1759812807.3906202,
    1759812807.4234939,
    1759812807.4279318,
    1759812807.471324,
    1759812807.5095444,
    1759812807.513533,
    1759812807.573806,
    1759812807.6044521,
    1759812807.6231608,
    1759812807.7038925,
    1759812807.7158458,
    1759812807.811656,
    1759812807.849397,
    1759812807.8530126,
    1759812808.0430505,
    1759812808.0627494,
    1759812808.0920808,
    1759812808.303889,
    1759812808.5223076,
    1759812808.6204956,
    1759812808.642015,
    1759812808.6677144,
    1759812808.8818994,
    1759812808.957737,
    1759812809.0559516,
    1759812809.1006732,
    1759812809.1873374,
    1759812809.228807,
    1759812809.2700427,
    1759812809.365944,
    1759812809.5026116,
    1759812809.5497012,
    1759812809.5632613,
    1759812809.6032653,
    1759812809.6645663,
    1759812809.6862917,
    1759812809.7155738,
    1759812809.7204244,
    1759812809.748082,
    1759812809.8639975,
    1759812809.9969406,
    1759812810.0211964,
    1759812810.0285766,
    1759812810.1027443,
    1759812810.109361,
    1759812810.1553996,
    1759812810.155354,
    1759812810.395953,
    1759812810.4167008,
    1759812810.4936893,
    1759812810.5012257,
    1759812810.513958,
    1759812810.586191,
    1759812810.5957828,
    1759812810.6794124,
    1759812810.7170794,
    1759812811.249759,
    1759812811.2750177,
    1759812811.4057183,
    1759812811.45166,
    1759812811.5443473,
    1759812811.5722337,
    1759812811.6687183,
    1759812811.6965806,
    1759812811.8006651,
    1759812812.0980523,
    1759812812.162865,
    1759812812.166889,
    1759812812.2165432,
    1759812812.430635,
    1759812812.6064029,
    1759812812.718529,
    1759812812.8708053,
    1759812812.9140406,
    1759812812.9256408,
    1759812812.975562,
    1759812812.9963667,
    1759812813.0305803,
    1759812813.1617837,
    1759812813.2543535,
    1759812813.290338,
    1759812813.3571427,
    1759812813.5117505,
    1759812813.5897276,
    1759812813.6233525,
    1759812813.653195,
    1759812813.8423955,
    1759812813.8809185,
    1759812813.8989067,
    1759812814.059361,
    1759812814.1235328,
    1759812814.201611,
    1759812814.241763,
    1759812814.241809,
    1759812814.246325,
    1759812814.2463644,
    1759812814.3411624,
    1759812814.4999301,
    1759812814.636739,
    1759812814.7505255,
    1759812814.881275,
    1759812814.9227734,
    1759812814.978272,
    1759812815.0102508,
    1759812815.021673,
    1759812815.0706007,
    1759812815.0768332,
    1759812815.0947814,
    1759812815.140286,
    1759812815.1453462,
    1759812815.1773543,
    1759812815.1845496,
    1759812815.2929864,
    1759812815.3111212,
    1759812815.4133713,
    1759812815.4777575,
    1759812815.4868486,
    1759812815.4965847,
    1759812815.6652539,
    1759812815.6832614,
    1759812815.7755744,
    1759812815.8058736,
    1759812815.8239148,
    1759812815.8964233,
    1759812815.9157774,
    1759812816.0005693,
    1759812816.0252788,
    1759812816.0767438,
    1759812816.1071758,
    1759812816.3285832,
    1759812816.338807,
    1759812816.3630805,
    1759812816.4305487,
    1759812816.5148509,
    1759812816.5343225,
    1759812816.5976448,
    1759812816.6526837,
    1759812816.6922746,
    1759812816.7137666,
    1759812816.9771812,
    1759812817.018373,
    1759812817.2189527,
    1759812817.348555,
    1759812817.406159,
    1759812817.4760985,
    1759812817.5571318,
    1759812817.609557,
    1759812817.7087104,
    1759812817.833791,
    1759812817.8547595,
    1759812817.8602445,
    1759812818.1722915,
    1759812818.177357,
    1759812818.1967561,
    1759812818.1968024,
    1759812818.2329714,
    1759812818.2711725,
    1759812818.5350215,
    1759812818.534975,
    1759812818.5616095,
    1759812818.5899389,
    1759812818.641955,
    1759812818.7318711,
    1759812818.777625,
    1759812818.8529637,
    1759812818.8748443,
    1759812818.9011836,
    1759812818.9153812,
    1759812818.933124,
    1759812819.0367823,
    1759812819.065458,
    1759812819.1351867,
    1759812819.1601243,
    1759812819.279053,
    1759812819.3483245,
    1759812819.4876852,
    1759812819.5444295,
    1759812819.5618587,
    1759812819.6506917,
    1759812819.6566145,
    1759812819.67227,
    1759812819.707507,
    1759812819.7850194,
    1759812819.942725,
    1759812820.020301,
    1759812820.0727148,
    1759812820.0893278,
    1759812820.1935062,
    1759812820.206462,
    1759812820.2202873,
    1759812820.2572968,
    1759812820.3109577,
    1759812820.5846527,
    1759812820.6377182,
    1759812820.671137,
    1759812820.7632937,
    1759812820.7801883,
    1759812820.7858808,
    1759812820.916821,
    1759812820.9168587,
    1759812820.9389222,
    1759812820.9555254,
    1759812821.0397315,
    1759812821.0961692,
    1759812821.0961986,
    1759812821.1996229,
    1759812821.2158904,
    1759812821.3696523,
    1759812821.4890037,
    1759812821.5609908,
    1759812821.76464,
    1759812821.9101326,
    1759812821.9229672,
    1759812821.9313438,
    1759812821.9409978,
    1759812822.0329146,
    1759812822.0610533,
    1759812822.0983036,
    1759812822.176181,
    1759812822.3035972,
    1759812822.3097012,
    1759812822.505076,
    1759812822.5891716,
    1759812822.6035185,
    1759812822.6184616,
    1759812822.7067935,
    1759812822.8434398,
    1759812823.039826,
    1759812823.1378167,
    1759812823.3240056,
    1759812823.5814629,
    1759812823.6271057,
    1759812823.7444203,
    1759812823.8226888,
    1759812823.942289,
    1759812823.9655535,
    1759812824.013849,
    1759812824.0230303,
    1759812824.026166,
    1759812824.255656,
    1759812824.3803995,
    1759812824.4090772,
    1759812824.4418163,
    1759812824.4489574,
    1759812824.4542918,
    1759812824.4631326,
    1759812824.5525157,
    1759812824.5525625,
    1759812824.6166692,
    1759812824.7543445,
    1759812824.8070147,
    1759812824.8469217,
    1759812824.9854023,
    1759812825.2815247,
    1759812825.2815447,
    1759812825.2814567,
    1759812825.2991982,
    1759812825.2992268,
    1759812825.3553786,
    1759812825.368311,
    1759812825.416715,
    1759812825.4679492,
    1759812825.5125754,
    1759812825.5174575,
    1759812825.56273,
    1759812825.5767179,
    1759812825.5939934,
    1759812825.621416,
    1759812825.6368968,
    1759812825.6722295,
    1759812825.7063377,
    1759812825.8180618,
    1759812825.8299193,
    1759812825.8876603,
    1759812825.930401,
    1759812825.95078,
    1759812825.9767215,
    1759812826.0304768,
    1759812826.1239686,
    1759812826.2433383,
    1759812826.3773484,
    1759812826.4124434,
    1759812826.418502,
    1759812826.4222493,
    1759812826.4703505,
    1759812826.5521243,
    1759812826.617732,
    1759812826.6660333,
    1759812826.6907663,
    1759812826.8423922,
    1759812826.8665068,
    1759812826.8914135,
    1759812827.018521,
    1759812827.2510612,
    1759812827.271285,
    1759812827.290145,
    1759812827.400186,
    1759812827.4323308,
    1759812827.5309353,
    1759812827.6665523,
    1759812827.7001634,
    1759812827.728123,
    1759812827.7409666,
    1759812827.859011,
    1759812827.8766842,
    1759812827.9615119,
    1759812828.0137334,
    1759812828.0557098,
    1759812828.0600834,
    1759812828.135912,
    1759812828.1962929,
    1759812828.3026958,
    1759812828.3916218,
    1759812828.431864,
    1759812828.4714215,
    1759812828.5830894,
    1759812828.7801447,
    1759812828.806569,
    1759812828.809996,
    1759812828.8193765,
    1759812828.950958,
    1759812829.0170388,
    1759812829.04426,
    1759812829.1499538,
    1759812829.1665146,
    1759812829.2082407,
    1759812829.3509855,
    1759812829.3580863,
    1759812829.6145568,
    1759812829.6194947,
    1759812829.6476219,
    1759812829.65338,
    1759812829.6587746,
    1759812829.7586079,
    1759812829.8181489,
    1759812829.8181093,
    1759812829.8491647,
    1759812829.9841986,
    1759812830.0537612,
    1759812830.093713,
    1759812830.1301482,
    1759812830.145986,
    1759812830.3960893,
    1759812830.4058254,
    1759812830.4057875,
    1759812830.428174,
    1759812830.6096447,
    1759812830.6252031,
    1759812830.6532063,
    1759812830.702401,
    1759812830.7023537,
    1759812830.7070599,
    1759812830.716553,
    1759812830.7657392,
    1759812830.76951,
    1759812830.865962,
    1759812830.9375806,
    1759812830.967615,
    1759812831.0883803,
    1759812831.0884013,
    1759812831.097001,
    1759812831.1565242,
    1759812831.1822402,
    1759812831.2843177,
    1759812831.426936,
    1759812831.8065789,
    1759812831.8985488,
    1759812831.970396,
    1759812832.2254446,
    1759812832.3262124,
    1759812832.5430815,
    1759812832.616311,
    1759812832.6290364,
    1759812832.6501732,
    1759812832.6501274,
    1759812832.7045648,
    1759812832.7240753,
    1759812832.7309124,
    1759812832.748484,
    1759812832.7852845,
    1759812832.8114934,
    1759812832.8567073,
    1759812832.9631357,
    1759812833.016985,
    1759812833.1075249,
    1759812833.1421518,
    1759812833.1421044,
    1759812833.1421416,
    1759812833.146673,
    1759812833.1778045,
    1759812833.2362967,
    1759812833.3390918,
    1759812833.3709838,
    1759812833.406241,
    1759812833.4645517,
    1759812833.4722457,
    1759812833.4854062,
    1759812833.5559733,
    1759812833.636987,
    1759812833.8323429,
    1759812833.851774,
    1759812834.107445,
    1759812834.1926339,
    1759812834.2152832,
    1759812834.3247433,
    1759812834.3574803,
    1759812834.3665075,
    1759812834.372288,
    1759812834.5327928,
    1759812834.5912898,
    1759812834.7426746,
    1759812834.7527304,
    1759812834.7839253,
    1759812834.8125913,
    1759812834.835113,
    1759812834.8405485,
    1759812834.8786058,
    1759812835.0009315,
    1759812835.037211,
    1759812835.0553677,
    1759812835.1122394,
    1759812835.122716,
    1759812835.253442,
    1759812835.3464987,
    1759812835.4442904,
    1759812835.471924,
    1759812835.481417,
    1759812835.4813821,
    1759812835.5414517,
    1759812835.553919,
    1759812835.6890407,
    1759812835.7471955,
    1759812835.8127933,
    1759812835.865429,
    1759812836.0213025,
    1759812836.04534,
    1759812836.0950258,
    1759812836.127145,
    1759812836.3076544,
    1759812836.3440511,
    1759812836.4279914,
    1759812836.4490304,
    1759812836.7926745,
    1759812836.8712559,
    1759812836.874391,
    1759812836.920863,
    1759812836.930034,
    1759812837.0387974,
    1759812837.047559,
    1759812837.0533452,
    1759812837.1307251,
    1759812837.1366692,
    1759812837.1863482,
    1759812837.1948051,
    1759812837.1948433,
    1759812837.2645159,
    1759812837.349481,
    1759812837.3548412,
    1759812837.4072716,
    1759812837.4307547,
    1759812837.511667,
    1759812837.5178788,
    1759812837.595916,
    1759812837.7934186,
    1759812837.9711983,
    1759812837.9810894,
    1759812837.9811242,
    1759812838.1731439,
    1759812838.2016408,
    1759812838.2275057,
    1759812838.252622,
    1759812838.3251853,
    1759812838.349005,
    1759812838.3679273,
    1759812838.4785228,
    1759812838.5024447,
    1759812838.5065703,
    1759812838.5557454,
    1759812838.5869648,
    1759812838.6642482,
    1759812838.7124894,
    1759812838.7180436,
    1759812838.7959607,
    1759812838.8154755,
    1759812838.8807266,
    1759812839.0203817,
    1759812839.1559515,
    1759812839.1972215,
    1759812839.3269985,
    1759812839.3637514,
    1759812839.4465995,
    1759812839.485763,
    1759812839.5172365,
    1759812839.5847936,
    1759812839.6062293,
    1759812839.8022225,
    1759812839.808664,
    1759812839.8087049,
    1759812839.8717127,
    1759812839.9171917,
    1759812840.0651093,
    1759812840.0999951,
    1759812840.141047,
    1759812840.2241247,
    1759812840.278386,
    1759812840.3086603,
    1759812840.357078,
    1759812840.424624,
    1759812840.5514586,
    1759812840.5706203,
    1759812840.5876436,
    1759812840.5945914,
    1759812840.611982,
    1759812840.6855505,
    1759812840.712216,
    1759812840.792994,
    1759812840.9117117,
    1759812840.9190252,
    1759812841.007252,
    1759812841.0310383,
    1759812841.1023543,
    1759812841.1095426,
    1759812841.1395733,
    1759812841.2695577,
    1759812841.3425584,
    1759812841.3508003,
    1759812841.4546866,
    1759812841.614735,
    1759812841.6661277,
    1759812841.7468164,
    1759812841.7511706,
    1759812841.7915823,
    1759812841.8579946,
    1759812841.9065237,
    1759812841.9143193,
    1759812841.9728906,
    1759812842.134131,
    1759812842.1417847,
    1759812842.1735513,
    1759812842.2047977,
    1759812842.3712413,
    1759812842.5167274,
    1759812842.5359566,
    1759812842.6064494,
    1759812842.6141284,
    1759812842.6330912,
    1759812842.7907321,
    1759812842.8078454,
    1759812842.8587697,
    1759812842.9091024,
    1759812842.9572356,
    1759812843.0214334,
    1759812843.0415256,
    1759812843.0698946,
    1759812843.1048584,
    1759812843.115076,
    1759812843.2434983,
    1759812843.2757998,
    1759812843.2965167,
    1759812843.5612261,
    1759812843.7001996,
    1759812843.7061944,
    1759812843.7197504,
    1759812843.8007693,
    1759812844.2535534,
    1759812844.30141,
    1759812844.4787095,
    1759812844.5002465,
    1759812844.5118635,
    1759812844.5855417,
    1759812844.702769,
    1759812844.8619146,
    1759812844.8672404,
    1759812844.910238,
    1759812845.002724,
    1759812845.0095384,
    1759812845.0328403,
    1759812845.0938616,
    1759812845.1435254,
    1759812845.2975247,
    1759812845.3891594,
    1759812845.4938502,
    1759812845.5758655,
    1759812845.6687481,
    1759812845.690708,
    1759812845.8560805,
    1759812845.8698552,
    1759812845.923435,
    1759812845.9707034,
    1759812845.9833724,
    1759812845.9940138,
    1759812846.048382,
    1759812846.1669967,
    1759812846.18353,
    1759812846.183595,
    1759812846.1835825,
    1759812846.1995862,
    1759812846.2875574,
    1759812846.5576468,
    1759812846.5832345,
    1759812846.7311962,
    1759812846.82048,
    1759812846.873873,
    1759812846.8956888,
    1759812847.0948043,
    1759812847.1328614,
    1759812847.3390176,
    1759812847.3595955,
    1759812847.4020743,
    1759812847.5603561,
    1759812847.5846999,
    1759812847.6195395,
    1759812847.6248145,
    1759812847.6594007,
    1759812847.6697965,
    1759812847.8367777,
    1759812847.9106138,
    1759812848.0975046,
    1759812848.2402523,
    1759812848.311468,
    1759812848.3930721,
    1759812848.4003088,
    1759812848.4335024,
    1759812848.566697,
    1759812848.6974645,
    1759812848.759105,
    1759812848.8006308,
    1759812848.8170695,
    1759812848.9019904,
    1759812849.0399683,
    1759812849.050064,
    1759812849.050024,
    1759812849.0744164,
    1759812849.1340275,
    1759812849.139153,
    1759812849.145951,
    1759812849.185345,
    1759812849.2349699,
    1759812849.2418516,
    1759812849.4106889,
    1759812849.4392114,
    1759812849.550836,
    1759812849.5856018,
    1759812849.6552975,
    1759812849.7707174,
    1759812849.8140554,
    1759812849.9199936,
    1759812849.9854908,
    1759812850.0369778,
    1759812850.0475533,
    1759812850.0563622,
    1759812850.1656375,
    1759812850.2974305,
    1759812850.3721817,
    1759812850.45776,
    1759812850.4755926,
    1759812850.4776626,
    1759812850.4849412,
    1759812850.516115,
    1759812850.5635264,
    1759812850.6544898,
    1759812850.6863365,
    1759812850.715418,
    1759812850.7954803,
    1759812850.8243294,
    1759812850.9471965,
    1759812851.0432394,
    1759812851.1387334,
    1759812851.3422906,
    1759812851.3871176,
    1759812851.4104717,
    1759812851.6392908,
    1759812851.8619578,
    1759812852.1363502,
    1759812852.2562342,
    1759812852.2842903,
    1759812852.3172228,
    1759812852.340831,
    1759812852.5066633,
    1759812852.5449326,
    1759812852.5448837,
    1759812852.682638,
    1759812852.7159283,
    1759812852.7334797,
    1759812852.752877,
    1759812852.815204,
    1759812852.8663445,
    1759812852.8765,
    1759812853.0614166,
    1759812853.3235025,
    1759812853.334876,
    1759812853.4106047,
    1759812853.550724,
    1759812853.5506785,
    1759812853.919915,
    1759812853.926899,
    1759812853.9450583,
    1759812854.0151057,
    1759812854.030346,
    1759812854.0944235,
    1759812854.1428802,
    1759812854.3253806,
    1759812854.3308167,
    1759812854.3308566,
    1759812854.3541286,
    1759812854.443074,
    1759812854.661634,
    1759812854.6770284,
    1759812854.6860337,
    1759812854.7870026,
    1759812854.9924567,
    1759812855.0553463,
    1759812855.2371929,
    1759812855.389996,
    1759812855.4131517,
    1759812855.4341598,
    1759812855.6471653,
    1759812855.6505332,
    1759812855.745647,
    1759812855.7534504,
    1759812855.8047712,
    1759812855.9024777,
    1759812855.99432,
    1759812856.2077844,
    1759812856.2890143,
    1759812856.3099341,
    1759812856.3410978,
    1759812856.4201355,
    1759812856.4381876,
    1759812856.479602,
    1759812856.4937866,
    1759812856.6885345,
    1759812856.8036368,
    1759812856.832017,
    1759812856.8893154,
    1759812856.9514165,
    1759812857.0711946,
    1759812857.196258,
    1759812857.529775,
    1759812857.618411,
    1759812857.6268933,
    1759812857.7393844,
    1759812857.8444004,
    1759812857.8535478,
    1759812857.8535068,
    1759812857.8633616,
    1759812857.9703968,
    1759812858.016629,
    1759812858.0595443,
    1759812858.1145785,
    1759812858.274004,
    1759812858.31221,
    1759812858.5486538,
    1759812858.561036,
    1759812858.5947795,
    1759812858.6484993,
    1759812858.7162395,
    1759812859.335941,
    1759812859.4038053,
    1759812859.4123378,
    1759812859.489615,
    1759812859.5398293,
    1759812859.6428175,
    1759812859.807555,
    1759812860.2567828,
    1759812860.256732,
    1759812860.2890387,
    1759812860.2949755,
    1759812860.6181865,
    1759812860.6566136,
    1759812860.6654227,
    1759812860.799906,
    1759812860.811011,
    1759812860.8684862,
    1759812860.9456177,
    1759812861.029808,
    1759812861.1928225,
    1759812861.2559261,
    1759812861.290892,
    1759812861.2970765,
    1759812861.3055446,
    1759812861.3215685,
    1759812861.4685726,
    1759812861.5013123,
    1759812862.0377212,
    1759812862.273364,
    1759812862.4196503,
    1759812862.4348183,
    1759812862.632892,
    1759812862.656617,
    1759812863.088739,
    1759812863.1186593,
    1759812863.309523,
    1759812863.337076,
    1759812863.3825707,
    1759812863.5655708,
    1759812863.5878794,
    1759812863.6793854,
    1759812863.7967541,
    1759812863.8012521,
    1759812863.8483448,
    1759812864.0628982,
    1759812864.2931998,
    1759812864.3771687,
    1759812864.3771212,
    1759812864.4444513,
    1759812864.5040586,
    1759812864.5288153,
    1759812864.574933,
    1759812864.9617417,
    1759812865.0209217,
    1759812865.0996764,
    1759812865.1086903,
    1759812865.2141201,
    1759812865.5062504,
    1759812865.510677,
    1759812865.6302547,
    1759812865.681104,
    1759812865.6937587,
    1759812865.8363564,
    1759812865.8699887,
    1759812865.8992624,
    1759812865.9054601,
    1759812866.385539,
    1759812866.5611691,
    1759812866.6091552,
    1759812866.7032046,
    1759812866.8137336,
    1759812867.0573628,
    1759812867.1555674,
    1759812867.161471,
    1759812867.215159,
    1759812867.5108657,
    1759812867.623351,
    1759812867.6754057,
    1759812867.7173765,
    1759812867.753828,
    1759812867.771701,
    1759812867.8985965,
    1759812867.9108782,
    1759812868.0140345,
    1759812868.0459316,
    1759812868.0616758,
    1759812868.0783436,
    1759812868.242121,
    1759812868.2725577,
    1759812868.3166018,
    1759812868.3724368,
    1759812868.4888978,
    1759812868.684492,
    1759812868.7168264,
    1759812868.749969,
    1759812868.8096406,
    1759812868.9191506,
    1759812868.9242043,
    1759812869.0077991,
    1759812869.1188428,
    1759812869.1356428,
    1759812869.2879949,
    1759812869.667252,
    1759812869.6751087,
    1759812869.7880254,
    1759812869.9465694,
    1759812869.995256,
    1759812870.0737,
    1759812870.1354601,
    1759812870.1729126,
    1759812870.2024553,
    1759812870.314319,
    1759812870.3242378,
    1759812870.7091737,
    1759812870.8778286,
    1759812870.9996192,
    1759812871.0030856,
    1759812871.0942366,
    1759812871.12495,
    1759812871.2755356,
    1759812871.3978243,
    1759812871.525749,
    1759812871.563813,
    1759812871.6030583,
    1759812871.6197035,
    1759812871.6299748,
    1759812871.7020147,
    1759812871.7141962,
    1759812871.7321174,
    1759812871.7646956,
    1759812871.8696022,
    1759812872.127439,
    1759812872.1373112,
    1759812872.148961,
    1759812872.2143993,
    1759812872.271313,
    1759812872.3326783,
    1759812872.407232,
    1759812872.508366,
    1759812872.526291,
    1759812872.529874,
    1759812872.6313002,
    1759812872.638696,
    1759812872.7322576,
    1759812872.9001586,
    1759812872.9284837,
    1759812872.9450471,
    1759812873.0267413,
    1759812873.056415,
    1759812873.0755422,
    1759812873.1062202,
    1759812873.2391748,
    1759812873.3082042,
    1759812873.3900275,
    1759812873.429754,
    1759812873.446331,
    1759812873.5019894,
    1759812873.508627,
    1759812873.5896928,
    1759812873.6294482,
    1759812873.8010905,
    1759812873.8468273,
    1759812873.9456053,
    1759812873.980539,
    1759812874.0325336,
    1759812874.1030746,
    1759812874.185145,
    1759812874.1940508,
    1759812874.208082,
    1759812874.2264187,
    1759812874.3230968,
    1759812874.4569976,
    1759812874.7900217,
    1759812875.0458624,
    1759812875.1075928,
    1759812875.1524234,
    1759812875.213065,
    1759812875.4395387,
    1759812875.5573146,
    1759812875.5736983,
    1759812875.6173615,
    1759812875.6462524,
    1759812875.6674943,
    1759812875.6675136,
    1759812875.6931672,
    1759812875.723055,
    1759812875.773186,
    1759812875.9680653,
    1759812875.9749007,
    1759812876.4484844,
    1759812876.571619,
    1759812876.7616,
    1759812876.9026003,
    1759812877.1256719,
    1759812877.1549902,
    1759812877.2828152,
    1759812877.3231323,
    1759812877.341691,
    1759812877.4440377,
    1759812877.5145276,
    1759812877.5798852,
    1759812877.6767135,
    1759812877.8186085,
    1759812877.9182973,
    1759812878.001061,
    1759812878.005814,
    1759812878.019856,
    1759812878.0683515,
    1759812878.186784,
    1759812878.2544765,
    1759812878.3556387,
    1759812878.3599272,
    1759812878.4335299,
    1759812878.5527916,
    1759812878.7058482,
    1759812878.8671947,
    1759812878.941052,
    1759812878.9479735,
    1759812879.0467956,
    1759812879.1498456,
    1759812879.2597587,
    1759812879.282705,
    1759812879.3261604,
    1759812879.3573601,
    1759812879.418227,
    1759812879.5105433,
    1759812879.6672568,
    1759812879.7887468,
    1759812879.871324,
    1759812879.9778378,
    1759812879.9777915,
    1759812880.151912,
    1759812880.1912699,
    1759812880.273911,
    1759812880.4180756,
    1759812880.4289877,
    1759812880.5418935,
    1759812880.6022835,
    1759812880.6474075,
    1759812880.6866322,
    1759812880.724086,
    1759812880.853028,
    1759812880.8659282,
    1759812880.972346,
    1759812881.192061,
    1759812881.2283878,
    1759812881.4473565,
    1759812881.501193,
    1759812881.594365,
    1759812881.7105932,
    1759812881.710562,
    1759812881.7203999,
    1759812881.7465992,
    1759812881.754987,
    1759812882.0320194,
    1759812882.1211722,
    1759812882.344439,
    1759812882.465434,
    1759812882.47191,
    1759812882.5674756,
    1759812882.6612592,
    1759812882.7019339,
    1759812882.7508764,
    1759812882.7910821,
    1759812882.826009,
    1759812882.89549,
    1759812882.913653,
    1759812882.9459014,
    1759812882.9998958,
    1759812883.1686678,
    1759812883.1975012,
    1759812883.228371,
    1759812883.248243,
    1759812883.3197978,
    1759812883.3534942,
    1759812883.4577346,
    1759812883.4737701,
    1759812883.528388,
    1759812883.6105635,
    1759812883.6489835,
    1759812883.678168,
    1759812883.6896975,
    1759812883.7214983,
    1759812883.8226025,
    1759812883.8674192,
    1759812883.9035597,
    1759812883.9489062,
    1759812884.043609,
    1759812884.0679884,
    1759812884.2108593,
    1759812884.285871,
    1759812884.5164852,
    1759812884.6273527,
    1759812884.6383522,
    1759812884.7973008,
    1759812884.8085775,
    1759812884.8444884,
    1759812884.977726,
    1759812885.0084314,
    1759812885.0419574,
    1759812885.181296,
    1759812885.2869258,
    1759812885.3373857,
    1759812885.4183822,
    1759812885.4389188,
    1759812885.4844303,
    1759812885.5323472,
    1759812885.5466232,
    1759812885.6327493,
    1759812885.6687322,
    1759812885.6787162,
    1759812885.7133985,
    1759812885.7521317,
    1759812886.1361654,
    1759812886.1912305,
    1759812886.2673697,
    1759812886.267324,
    1759812886.3086185,
    1759812886.3844335,
    1759812886.5415986,
    1759812886.763811,
    1759812886.8372939,
    1759812886.8832924,
    1759812887.0831175,
    1759812887.1394908,
    1759812887.4781747,
    1759812887.6695228,
    1759812887.748422,
    1759812887.7549205,
    1759812887.7820423,
    1759812887.8216238,
    1759812887.8852894,
    1759812887.93349,
    1759812887.933444,
    1759812887.9383972,
    1759812887.9533763,
    1759812888.0498312,
    1759812888.1592598,
    1759812888.2223063,
    1759812888.23852,
    1759812888.287796,
    1759812888.5401692,
    1759812888.7262275,
    1759812888.7519596,
    1759812888.7910864,
    1759812888.795505,
    1759812888.9080942,
    1759812888.91286,
    1759812888.9842656,
    1759812888.9943206,
    1759812889.098798,
    1759812889.1287923,
    1759812889.1606636,
    1759812889.2104983,
    1759812889.30495,
    1759812889.3228755,
    1759812889.3945258,
    1759812889.4566312,
    1759812889.5195312,
    1759812889.5696526,
    1759812889.5839226,
    1759812889.588129,
    1759812889.626794,
    1759812889.689175,
    1759812889.7665513,
    1759812889.8044746,
    1759812889.8144178,
    1759812889.9658642,
    1759812889.9901106,
    1759812890.0141058,
    1759812890.0845006,
    1759812890.133181,
    1759812890.2494953,
    1759812890.324334,
    1759812890.359959,
    1759812890.368117,
    1759812890.3941662,
    1759812890.4073884,
    1759812890.4224958,
    1759812890.4837854,
    1759812890.4970863,
    1759812890.4971228,
    1759812890.5269501,
    1759812890.584077,
    1759812890.6344688,
    1759812890.695348,
    1759812890.86792,
    1759812890.9057102,
    1759812890.964338,
    1759812891.0821793,
    1759812891.169761,
    1759812891.2173934,
    1759812891.2597237,
    1759812891.3017957,
    1759812891.3757963,
    1759812891.5058563,
    1759812891.5099802,
    1759812891.5217495,
    1759812891.5545154,
    1759812891.562567,
    1759812891.657104,
    1759812891.6642807,
    1759812891.6906357,
    1759812891.7242708,
    1759812891.7289805,
    1759812891.7517538,
    1759812891.8002396,
    1759812891.8526344,
    1759812891.9467225,
    1759812892.0025733,
    1759812892.0185924,
    1759812892.3399222,
    1759812892.436018,
    1759812892.4494922,
    1759812892.4495301,
    1759812892.6039703,
    1759812892.6769803,
    1759812892.7690258,
    1759812892.8527727,
    1759812892.9358757,
    1759812892.9668798,
    1759812892.988754,
    1759812893.084468,
    1759812893.0985112,
    1759812893.0984464,
    1759812893.0984974,
    1759812893.1393497,
    1759812893.2160623,
    1759812893.220397,
    1759812893.3230467,
    1759812893.492578,
    1759812893.5421193,
    1759812893.6159678,
    1759812893.6620822,
    1759812893.7696633,
    1759812893.7749426,
    1759812893.8789372,
    1759812893.9710898,
    1759812894.0324633,
    1759812894.0518823,
    1759812894.163003,
    1759812894.2077498,
    1759812894.310209,
    1759812894.327528,
    1759812894.371407,
    1759812894.3826346,
    1759812894.5051517,
    1759812894.736848,
    1759812894.8230069,
    1759812894.9530897,
    1759812895.1146312,
    1759812895.1542706,
    1759812895.181213,
    1759812895.3577514,
    1759812895.3862,
    1759812895.522179,
    1759812895.548534,
    1759812895.7208397,
    1759812895.9863882,
    1759812896.0634098,
    1759812896.0728242,
    1759812896.1643054,
    1759812896.1806228,
    1759812896.3420103,
    1759812896.8485968,
    1759812897.0862074,
    1759812897.2151,
    1759812897.2633631,
    1759812897.3100848,
    1759812897.3450425,
    1759812897.495869,
    1759812897.5428405,
    1759812897.5577176,
    1759812897.582217,
    1759812897.755835,
    1759812897.9112697,
    1759812898.1225224,
    1759812898.244672,
    1759812898.2972808,
    1759812898.4748237,
    1759812898.5612326,
    1759812898.5946891,
    1759812898.6735678,
    1759812898.7157364,
    1759812898.8389633,
    1759812898.9334466,
    1759812898.9521885,
    1759812898.9885893,
    1759812899.0566869,
    1759812899.0858998,
    1759812899.1049736,
    1759812899.2594092,
    1759812899.2727737,
    1759812899.3536408,
    1759812899.6525853,
    1759812899.8632674,
    1759812899.9753065,
    1759812900.0101702,
    1759812900.0887022,
    1759812900.2056806,
    1759812900.2305796,
    1759812900.2390964,
    1759812900.268792,
    1759812900.3861773,
    1759812900.651045,
    1759812900.7689078,
    1759812900.815311,
    1759812900.853763,
    1759812900.8686042,
    1759812900.9052837,
    1759812901.033677,
    1759812901.0995295,
    1759812901.1276634,
    1759812901.1643074,
    1759812901.2723289,
    1759812901.4457633,
    1759812901.4497108,
    1759812901.5292332,
    1759812901.8210816,
    1759812901.840259,
    1759812901.888652,
    1759812901.941647,
    1759812902.0768576,
    1759812902.2288206,
    1759812902.2371428,
    1759812902.4041533,
    1759812902.6072779,
    1759812902.6959884,
    1759812902.7076752,
    1759812903.093001,
    1759812903.3423362,
    1759812903.6091387,
    1759812903.6476023,
    1759812903.7093694,
    1759812903.8062923,
    1759812903.908816,
    1759812903.9540927,
    1759812904.1057086,
    1759812904.1755924,
    1759812904.2092574,
    1759812904.398454,
    1759812904.409035,
    1759812904.4145095,
    1759812904.449195,
    1759812904.525409,
    1759812904.5402424,
    1759812904.7052748,
    1759812904.7052226,
    1759812904.9419928,
    1759812904.95573,
    1759812905.0349998,
    1759812905.1767228,
    1759812905.1766777,
    1759812905.1856775,
    1759812905.2027266,
    1759812905.2991798,
    1759812905.443966,
    1759812905.5001004,
    1759812905.5746324,
    1759812905.7102036,
    1759812905.7307487,
    1759812905.7657592,
    1759812906.1162763,
    1759812906.2010288,
    1759812906.2802138,
    1759812906.3464675,
    1759812906.4085014,
    1759812906.5718477,
    1759812906.6703496,
    1759812906.6929705,
    1759812907.217953,
    1759812907.2456715,
    1759812907.2816653,
    1759812907.3035688,
    1759812907.3414366,
    1759812907.4798603,
    1759812907.83399,
    1759812907.8556812,
    1759812907.9149015,
    1759812907.9903073,
    1759812907.990368,
    1759812908.0038474,
    1759812908.011047,
    1759812908.0908313,
    1759812908.39443,
    1759812908.4833784,
    1759812908.5285585,
    1759812908.5362718,
    1759812908.554297,
    1759812908.5736527,
    1759812908.7615137,
    1759812908.8509939,
    1759812908.9344606,
    1759812908.9466028,
    1759812909.101525,
    1759812909.145394,
    1759812909.2022305,
    1759812909.2342334,
    1759812909.3279078,
    1759812909.3547728,
    1759812909.4253905,
    1759812909.6190534,
    1759812909.7376177,
    1759812909.742883,
    1759812909.7495854,
    1759812909.7603965,
    1759812909.7690482,
    1759812909.806381,
    1759812910.0184383,
    1759812910.1742964,
    1759812910.2131903,
    1759812910.217531,
    1759812910.3023057,
    1759812910.3818955,
    1759812910.412471,
    1759812910.5930138,
    1759812910.6053758,
    1759812910.6258144,
    1759812910.7474253,
    1759812910.81766,
    1759812910.8431966,
    1759812910.911798,
    1759812910.9907994,
    1759812910.9995873,
    1759812911.0126073,
    1759812911.0188308,
    1759812911.0391972,
    1759812911.0840585,
    1759812911.086868,
    1759812911.1427855,
    1759812911.1863925,
    1759812911.3065884,
    1759812911.3065386,
    1759812911.4749548,
    1759812911.525561,
    1759812911.586622,
    1759812911.6888049,
    1759812911.7821276,
    1759812911.8296473,
    1759812911.8495903,
    1759812911.9618983,
    1759812911.9619272,
    1759812911.9875064,
    1759812912.0576267,
    1759812912.1130452,
    1759812912.1408048,
    1759812912.2081707,
    1759812912.2774134,
    1759812912.3627245,
    1759812912.3667977,
    1759812912.3872082,
    1759812912.6037784,
    1759812912.7040496,
    1759812912.7897577,
    1759812912.8808699,
    1759812912.9772851,
    1759812913.246199,
    1759812913.380524,
    1759812913.380461,
    1759812913.429798,
    1759812913.4643116,
    1759812913.5458593,
    1759812913.5548298,
    1759812913.5625367,
    1759812913.666469,
    1759812913.73763,
    1759812914.0183158,
    1759812914.0330217,
    1759812914.3271046,
    1759812914.4396482,
    1759812914.4656305,
    1759812914.557729,
    1759812914.6501365,
    1759812914.8033326,
    1759812914.9310222,
    1759812915.1711268,
    1759812915.1794546,
    1759812915.292434,
    1759812915.2923806,
    1759812915.4641507,
    1759812915.5414653,
    1759812915.7041502,
    1759812915.8171623,
    1759812915.8789103,
    1759812916.2098706,
    1759812916.351366,
    1759812916.4601257,
    1759812916.6264877,
    1759812916.6570456,
    1759812916.669659,
    1759812916.8289073,
    1759812916.8725324,
    1759812916.8786185,
    1759812916.9732718,
    1759812916.9904985,
    1759812917.033202,
    1759812917.1413474,
    1759812917.1774592,
    1759812917.2003574,
    1759812917.3131,
    1759812917.3606906,
    1759812917.4254837,
    1759812917.4874244,
    1759812917.5365875,
    1759812917.6257286,
    1759812917.635384,
    1759812917.6415963,
    1759812917.82752,
    1759812918.0096693,
    1759812918.0657818,
    1759812918.0746946,
    1759812918.1191347,
    1759812918.1896768,
    1759812918.237732,
    1759812918.4339783,
    1759812918.5721347,
    1759812918.7437856,
    1759812918.7731133,
    1759812918.8503888,
    1759812918.874868,
    1759812918.9436462,
    1759812918.9528658,
    1759812919.0093076,
    1759812919.149877,
    1759812919.163681,
    1759812919.18282,
    1759812919.2921653,
    1759812919.4020104,
    1759812919.4741752,
    1759812919.4881935,
    1759812919.4882314,
    1759812919.5260766,
    1759812919.5582962,
    1759812919.570577,
    1759812919.6231937,
    1759812919.909062,
    1759812919.9478302,
    1759812920.3004735,
    1759812920.3540733,
    1759812920.3724515,
    1759812920.4282496,
    1759812920.5084808,
    1759812920.5644512,
    1759812920.8788002,
    1759812920.8879712,
    1759812920.9246972,
    1759812920.9304118,
    1759812920.930449,
    1759812921.054632,
    1759812921.123851,
    1759812921.1530926,
    1759812921.2995195,
    1759812921.3517177,
    1759812921.4393518,
    1759812921.5921874,
    1759812921.7540581,
    1759812921.7799904,
    1759812921.8877711,
    1759812922.0095131,
    1759812922.0520287,
    1759812922.0709164,
    1759812922.0851328,
    1759812922.2225304,
    1759812922.2442853,
    1759812922.2443182,
    1759812922.2772949,
    1759812922.4057767,
    1759812922.4706552,
    1759812922.5636334,
    1759812922.5908039,
    1759812922.6256425,
    1759812922.7796338,
    1759812922.8535414,
    1759812922.9004278,
    1759812923.0174165,
    1759812923.1311405,
    1759812923.2018814,
    1759812923.2493222,
    1759812923.438511,
    1759812923.6084049,
    1759812923.9777114,
    1759812924.1532214,
    1759812924.8759336,
    1759812925.45709,
    1759812925.6481051,
    1759812925.7866611,
    1759812926.1341166,
    1759812926.2237911,
    1759812926.2882833,
    1759812926.3293006,
    1759812926.6638763,
    1759812926.8221076,
    1759812926.833343,
    1759812926.9751225,
    1759812927.1130724,
    1759812927.277049,
    1759812927.394369,
    1759812927.6463854,
    1759812927.7398195,
    1759812927.8269753,
    1759812927.9381804,
    1759812927.9599674,
    1759812928.0023487,
    1759812928.2461061,
    1759812928.300101,
    1759812928.3453484,
    1759812928.4074507,
    1759812928.4391935,
    1759812928.5607054,
    1759812928.5633478,
    1759812928.7092104,
    1759812928.910586,
    1759812928.9152098,
    1759812928.9548194,
    1759812929.0219285,
    1759812929.0765407,
    1759812929.1180367,
    1759812929.1660764,
    1759812929.4719856,
    1759812929.4833696,
    1759812929.4917357,
    1759812929.6050255,
    1759812929.6132386,
    1759812929.6700642,
    1759812929.7305646,
    1759812929.7404847,
    1759812929.9247596,
    1759812930.0863683,
    1759812930.116927,
    1759812930.1471097,
    1759812930.2014296,
    1759812930.372107,
    1759812930.3934517,
    1759812930.5275466,
    1759812930.641581,
    1759812930.7312233,
    1759812930.895939,
    1759812931.152073,
    1759812931.3400552,
    1759812931.5239213,
    1759812931.5238833,
    1759812931.6821625,
    1759812931.7159219,
    1759812931.7701344,
    1759812931.8567963,
    1759812931.8733604,
    1759812932.0740962,
    1759812932.2175195,
    1759812932.321463,
    1759812932.332403,
    1759812932.3826542,
    1759812932.4116836,
    1759812932.4524827,
    1759812932.4840398,
    1759812932.537037,
    1759812932.7028534,
    1759812932.7351682,
    1759812932.7352383,
    1759812932.7352488,
    1759812932.7352254,
    1759812932.7438262,
    1759812932.775539,
    1759812932.7950706,
    1759812932.8423707,
    1759812932.866428,
    1759812932.911298,
    1759812932.9487045,
    1759812932.9710803,
    1759812933.0942814,
    1759812933.1150112,
    1759812933.1150537,
    1759812933.1311133,
    1759812933.2789845,
    1759812933.3727775,
    1759812933.463447,
    1759812933.512799,
    1759812933.602558,
    1759812933.6094434,
    1759812933.7043638,
    1759812933.755393,
    1759812933.9991255,
    1759812934.019201,
    1759812934.019166,
    1759812934.0446672,
    1759812934.0925512,
    1759812934.2038057,
    1759812934.203826,
    1759812934.2110548,
    1759812934.222962,
    1759812934.3158386,
    1759812934.42423,
    1759812934.4439385,
    1759812934.5661814,
    1759812934.840025,
    1759812934.887659,
    1759812934.940542,
    1759812934.9634295,
    1759812934.9837966,
    1759812934.9837487,
    1759812935.0161924,
    1759812935.0211673,
    1759812935.0868843,
    1759812935.0940619,
    1759812935.1063235,
    1759812935.1179044,
    1759812935.2673597,
    1759812935.2673135,
    1759812935.3449423,
    1759812935.4065719,
    1759812935.4193943,
    1759812935.5643532,
    1759812935.5994327,
    1759812935.7249856,
    1759812935.733323,
    1759812935.743247,
    1759812935.760214,
    1759812935.8509405,
    1759812935.8583286,
    1759812936.0310593,
    1759812936.0406096,
    1759812936.0662963,
    1759812936.1539285,
    1759812936.324784,
    1759812936.3286142,
    1759812936.3335545,
    1759812936.3426824,
    1759812936.4579272,
    1759812936.481235,
    1759812936.6065788,
    1759812936.7059667,
    1759812936.7869246,
    1759812937.0648704,
    1759812937.0849674,
    1759812937.1015525,
    1759812937.1070945,
    1759812937.1309104,
    1759812937.139452,
    1759812937.1933146,
    1759812937.3286455,
    1759812937.40238,
    1759812937.50185,
    1759812937.5144389,
    1759812937.6847982,
    1759812937.7554336,
    1759812937.8147545,
    1759812937.850722,
    1759812937.866721,
    1759812937.8667588,
    1759812938.0425296,
    1759812938.076805,
    1759812938.1369946,
    1759812938.2385035,
    1759812938.3506296,
    1759812938.5354996,
    1759812938.8060148,
    1759812938.9630914,
    1759812939.1375687,
    1759812939.17929,
    1759812939.1792417,
    1759812939.2034516,
    1759812939.2479773,
    1759812939.2947197,
    1759812939.3046522,
    1759812939.3386583,
    1759812939.4303074,
    1759812939.4932492,
    1759812939.5692534,
    1759812939.684388,
    1759812939.710795,
    1759812939.8201213,
    1759812939.862075,
    1759812939.916733,
    1759812940.0132895,
    1759812940.0282376,
    1759812940.0863628,
    1759812940.127758,
    1759812940.245754,
    1759812940.276204,
    1759812940.4967275,
    1759812940.5368547,
    1759812940.7081854,
    1759812940.7801774,
    1759812940.803054,
    1759812940.877046,
    1759812940.9115927,
    1759812940.9320533,
    1759812941.0785592,
    1759812941.119075,
    1759812941.1444242,
    1759812941.2549248,
    1759812941.3435338,
    1759812941.4905343,
    1759812941.5485837,
    1759812941.7462258,
    1759812941.7678592,
    1759812941.7798107,
    1759812941.8282685,
    1759812941.9791043,
    1759812942.004818,
    1759812942.076657,
    1759812942.1260343,
    1759812942.1607244,
    1759812942.26559,
    1759812942.3013384,
    1759812942.3272011,
    1759812942.3878405,
    1759812942.5330505,
    1759812942.5330021,
    1759812942.6934886,
    1759812942.8006496,
    1759812942.9221544,
    1759812942.9433534,
    1759812943.0243304,
    1759812943.0644593,
    1759812943.0645077,
    1759812943.2620623,
    1759812943.3866746,
    1759812943.4184277,
    1759812943.5363593,
    1759812943.6709623,
    1759812943.8162944,
    1759812943.8803985,
    1759812943.9090984,
    1759812943.9212415,
    1759812944.0757964,
    1759812944.1289754,
    1759812944.1758106,
    1759812944.487551,
    1759812944.5360377,
    1759812944.5574696,
    1759812944.6535048,
    1759812945.0045798,
    1759812945.1434245,
    1759812945.1581752,
    1759812945.2043464,
    1759812945.366385,
    1759812945.5163863,
    1759812945.5307915,
    1759812945.586777,
    1759812945.6521387,
    1759812945.661045,
    1759812945.7366624,
    1759812945.7867768,
    1759812945.8066893,
    1759812945.8639586,
    1759812945.9733787,
    1759812946.007936,
    1759812946.0668669,
    1759812946.0803819,
    1759812946.2043602,
    1759812946.3501225,
    1759812946.5549083,
    1759812946.6310756,
    1759812946.8387017,
    1759812946.9042695,
    1759812946.9110534,
    1759812947.0511637,
    1759812947.07963,
    1759812947.085915,
    1759812947.1097684,
    1759812947.2465672,
    1759812947.367886,
    1759812947.4425733,
    1759812947.4631114,
    1759812947.4900565,
    1759812947.551792,
    1759812947.6879058,
    1759812947.7677264,
    1759812947.7968123,
    1759812947.8579807,
    1759812947.901819,
    1759812947.9327495,
    1759812947.9693763,
    1759812947.9984229,
    1759812948.013096,
    1759812948.195098,
    1759812948.204352,
    1759812948.2791395,
    1759812948.391008,
    1759812948.534342,
    1759812948.641461,
    1759812948.6517646,
    1759812948.7710652,
    1759812948.801333,
    1759812948.811535,
    1759812949.0808294,
    1759812949.1949732,
    1759812949.314264,
    1759812949.3352299,
    1759812949.412016,
    1759812949.4547617,
    1759812949.5459414,
    1759812949.598855,
    1759812949.634434,
    1759812949.7128906,
    1759812949.7219317,
    1759812949.8115335,
    1759812949.8995857,
    1759812949.9053602,
    1759812949.9175975,
    1759812949.9175537,
    1759812950.0289886,
    1759812950.117737,
    1759812950.4153774,
    1759812950.421679,
    1759812950.6901464,
    1759812950.8760393,
    1759812950.9981296,
    1759812951.1091352,
    1759812951.1440594,
    1759812951.169342,
    1759812951.1788757,
    1759812951.1931098,
    1759812951.2607036,
    1759812951.362162,
    1759812951.4653635,
    1759812951.478213,
    1759812951.4964893,
    1759812951.779802,
    1759812951.8654256,
    1759812951.8980315,
    1759812951.9524074,
    1759812952.071466,
    1759812952.1260595,
    1759812952.2152455,
    1759812952.3256607,
    1759812952.4064896,
    1759812952.525229,
    1759812952.6144598,
    1759812952.7736852,
    1759812952.8694544,
    1759812952.8850667,
    1759812952.9991899,
    1759812953.0216453,
    1759812953.112017,
    1759812953.1858504,
    1759812953.2725837,
    1759812953.3075838,
    1759812953.639707,
    1759812953.780289,
    1759812953.9309812,
    1759812953.9391468,
    1759812954.018877,
    1759812954.065381,
    1759812954.069885,
    1759812954.1101177,
    1759812954.370818,
    1759812954.4105847,
    1759812954.487048,
    1759812954.6762106,
    1759812954.7278292,
    1759812954.84837,
    1759812954.91965,
    1759812955.0679748,
    1759812955.1538534,
    1759812955.194742,
    1759812955.2101414,
    1759812955.2222736,
    1759812955.3800113,
    1759812955.4179385,
    1759812955.4518573,
    1759812955.4617333,
    1759812955.4699464,
    1759812955.5234852,
    1759812955.5483232,
    1759812955.6107228,
    1759812955.6305845,
    1759812955.6987653,
    1759812955.705334,
    1759812955.7053733,
    1759812955.7749643,
    1759812955.953195,
    1759812956.2172587,
    1759812956.2440915,
    1759812956.366361,
    1759812956.475497,
    1759812956.5784507,
    1759812956.5854023,
    1759812956.6614032,
    1759812956.703786,
    1759812956.9650717,
    1759812957.0555944,
    1759812957.1547546,
    1759812957.1761599,
    1759812957.2075872,
    1759812957.2131188,
    1759812957.2576308,
    1759812957.3345993,
    1759812957.3835835,
    1759812957.3888702,
    1759812957.3888268,
    1759812957.3927991,
    1759812957.4665215,
    1759812957.5679555,
    1759812957.5772345,
    1759812957.5771878,
    1759812957.5847297,
    1759812957.9051743,
    1759812957.982823,
    1759812958.0745163,
    1759812958.1253283,
    1759812958.1662009,
    1759812958.2266598,
    1759812958.2547798,
    1759812958.3038273,
    1759812958.348223,
    1759812958.5657487,
    1759812958.694397,
    1759812958.8425896,
    1759812959.1015882,
    1759812959.2293706,
    1759812959.2521024,
    1759812959.3921225,
    1759812959.4371898,
    1759812959.518947,
    1759812959.5659559,
    1759812959.5659704,
    1759812959.5658994,
    1759812959.6142678,
    1759812959.6186273,
    1759812959.7200239,
    1759812959.7819166,
    1759812959.8669152,
    1759812960.0731254,
    1759812960.1336727,
    1759812960.1916335,
    1759812960.212334,
    1759812960.2349668,
    1759812960.239757,
    1759812960.2974653,
    1759812960.457848,
    1759812960.457806,
    1759812960.5469856,
    1759812960.7420878,
    1759812960.876975,
    1759812960.8902469,
    1759812960.9214292,
    1759812960.9582999,
    1759812960.9748003,
    1759812961.0096085,
    1759812961.045793,
    1759812961.698606,
    1759812961.9466574,
    1759812962.0243073,
    1759812962.2407396,
    1759812962.2408032,
    1759812962.2540567,
    1759812962.3287752,
    1759812962.359175,
    1759812962.5017917,
    1759812962.562291,
    1759812962.7161913,
    1759812962.7339072,
    1759812962.8064613,
    1759812963.0410707,
    1759812963.103651,
    1759812963.1105304,
    1759812963.2018142,
    1759812963.2315097,
    1759812963.275104,
    1759812963.3136847,
    1759812963.4073868,
    1759812963.4132955,
    1759812963.5908916,
    1759812963.663123,
    1759812963.6924615,
    1759812963.8001287,
    1759812963.818451,
    1759812963.9604995,
    1759812964.125095,
    1759812964.234771,
    1759812964.2397606,
    1759812964.5184891,
    1759812964.5679724,
    1759812964.5848155,
    1759812964.884134,
    1759812965.0363472,
    1759812965.0491166,
    1759812965.1028795,
    1759812965.2974308,
    1759812965.4307194,
    1759812965.5357072,
    1759812965.614221,
    1759812965.6624103,
    1759812965.7554398,
    1759812965.8481398,
    1759812966.048837,
    1759812966.7376947,
    1759812966.9914389,
    1759812967.140636,
    1759812967.3017006,
    1759812967.371329,
    1759812967.755231,
    1759812967.8183947,
    1759812967.8307636,
    1759812967.9601686,
    1759812968.0849333,
    1759812968.45565,
    1759812968.502787,
    1759812968.5656462,
    1759812968.5989897,
    1759812968.6807349,
    1759812968.7055018,
    1759812968.7817557,
    1759812968.8878257,
    1759812969.185603,
    1759812969.192076,
    1759812969.215858,
    1759812969.2753198,
    1759812969.314028,
    1759812969.5568569,
    1759812969.8328922,
    1759812969.8329265,
    1759812969.863417,
    1759812969.9117403,
    1759812969.9305913,
    1759812970.124531,
    1759812970.1978204,
    1759812970.2159402,
    1759812970.3338437,
    1759812970.515601,
    1759812970.6196005,
    1759812970.654498,
    1759812970.7240422,
    1759812970.820167,
    1759812970.9245143,
    1759812971.053474,
    1759812971.0958126,
    1759812971.10723,
    1759812971.107166,
    1759812971.1072156,
    1759812971.1592238,
    1759812971.2346632,
    1759812971.3042624,
    1759812971.4190843,
    1759812971.4299855,
    1759812971.4921565,
    1759812971.6937795,
    1759812971.7570477,
    1759812971.9591203,
    1759812972.0213594,
    1759812972.0305498,
    1759812972.070463,
    1759812972.2457714,
    1759812972.2457228,
    1759812972.4462588,
    1759812972.456418,
    1759812972.456456,
    1759812972.553737,
    1759812972.6077154,
    1759812972.7774622,
    1759812973.0487075,
    1759812973.0749931,
    1759812973.0904562,
    1759812973.1190534,
    1759812973.1612546,
    1759812973.1612244,
    1759812973.2316794,
    1759812973.266797,
    1759812973.3079882,
    1759812973.534772,
    1759812973.610387,
    1759812973.7031639,
    1759812973.860204,
    1759812973.9853451,
    1759812974.022777,
    1759812974.0227137,
    1759812974.022793,
    1759812974.053439,
    1759812974.0877857,
    1759812974.311071,
    1759812974.3672547,
    1759812974.4354534,
    1759812974.4794493,
    1759812974.5184588,
    1759812974.5503356,
    1759812974.598526,
    1759812974.796485,
    1759812974.8051343,
    1759812974.9252756,
    1759812974.9339178,
    1759812975.0713363,
    1759812975.434895,
    1759812975.5301962,
    1759812975.5359988,
    1759812975.6334596,
    1759812975.7357776,
    1759812975.7765346,
    1759812975.7819297,
    1759812975.7940824,
    1759812975.7988605,
    1759812975.9735851,
    1759812975.9930077,
    1759812976.0249162,
    1759812976.14226,
    1759812976.1485672,
    1759812976.175708,
    1759812976.3103998,
    1759812976.377579,
    1759812976.448394,
    1759812976.5621645,
    1759812976.8186126,
    1759812976.8275049,
    1759812976.883966,
    1759812976.931069,
    1759812976.9806955,
    1759812977.0617864,
    1759812977.0617397,
    1759812977.0783455,
    1759812977.2859147,
    1759812977.3023407,
    1759812977.3357809,
    1759812977.7981803,
    1759812977.8869221,
    1759812978.0091531,
    1759812978.0091982,
    1759812978.0205998,
    1759812978.0309808,
    1759812978.1191032,
    1759812978.2249603,
    1759812978.3820946,
    1759812978.63937,
    1759812978.7121394,
    1759812978.7482977,
    1759812978.8154438,
    1759812978.856608,
    1759812978.8677754,
    1759812978.9012918,
    1759812978.9617252,
    1759812979.0162404,
    1759812979.127787,
    1759812979.2266011,
    1759812979.2266386,
    1759812979.2515001,
    1759812979.2785459,
    1759812979.2874475,
    1759812979.3375025,
    1759812979.3740509,
    1759812979.5291026,
    1759812979.8029628,
    1759812979.8597116,
    1759812979.9549327,
    1759812980.0421977,
    1759812980.0530498,
    1759812980.151694,
    1759812980.190107,
    1759812980.2678287,
    1759812980.4129076,
    1759812980.500629,
    1759812980.757508,
    1759812980.7574768,
    1759812980.7879314,
    1759812980.796496,
    1759812980.8222618,
    1759812980.9037814,
    1759812980.9169986,
    1759812981.0007427,
    1759812981.1045237,
    1759812981.2573245,
    1759812981.2935894,
    1759812981.339701,
    1759812981.5208821,
    1759812981.6247442,
    1759812981.6342304,
    1759812981.70657,
    1759812981.9231036,
    1759812981.9328027,
    1759812982.171861,
    1759812982.1902032,
    1759812982.2721946,
    1759812982.2829964,
    1759812982.3009093,
    1759812982.445476,
    1759812982.4744532,
    1759812982.4846094,
    1759812982.5706215,
    1759812982.7061863,
    1759812982.7893803,
    1759812982.8396137,
    1759812982.8623717,
    1759812983.0817404,
    1759812983.1184418,
    1759812983.1607842,
    1759812983.2986696,
    1759812983.3987138,
    1759812983.470061,
    1759812983.538224,
    1759812983.6555758,
    1759812983.6555283,
    1759812983.699516,
    1759812983.7804303,
    1759812983.8255143,
    1759812983.8465886,
    1759812984.0494688,
    1759812984.3576035,
    1759812984.3664968,
    1759812984.407487,
    1759812984.494976,
    1759812984.5211973,
    1759812984.5479758,
    1759812984.611377,
    1759812984.6409461,
    1759812984.8519678,
    1759812984.9745982,
    1759812984.9992628,
    1759812985.0701709,
    1759812985.270868,
    1759812985.3806236,
    1759812985.443347,
    1759812985.607793,
    1759812985.6898034,
    1759812985.7455652,
    1759812985.8921578,
    1759812985.9852686,
    1759812985.9853127,
    1759812986.0048466,
    1759812986.0974085,
    1759812986.297169,
    1759812986.3232667,
    1759812986.4518185,
    1759812986.4595046,
    1759812986.6491466,
    1759812986.9358392,
    1759812987.2793915,
    1759812987.526474,
    1759812987.6189106,
    1759812987.6492615,
    1759812987.7169325,
    1759812987.7645605,
    1759812987.8883922,
    1759812987.9259071,
    1759812988.0690298,
    1759812988.3656757,
    1759812988.3657236,
    1759812988.4004524,
    1759812988.704898,
    1759812988.7049282,
    1759812988.7049134,
    1759812988.7591772,
    1759812988.7908587,
    1759812988.8705502,
    1759812988.875535,
    1759812988.8755026,
    1759812988.9397705,
    1759812989.0406938,
    1759812989.3112104,
    1759812989.338777,
    1759812989.4666224,
    1759812989.4736388,
    1759812989.6296601,
    1759812989.6533344,
    1759812989.728134,
    1759812989.8878903,
    1759812989.88786,
    1759812989.9005504,
    1759812989.969366,
    1759812989.9882398,
    1759812990.2188647,
    1759812990.3136063,
    1759812990.4246407,
    1759812990.544857,
    1759812990.5713818,
    1759812990.633877,
    1759812990.6885347,
    1759812990.6982658,
    1759812990.7876759,
    1759812990.8395026,
    1759812990.9731786,
    1759812991.093327,
    1759812991.0932827,
    1759812991.1328297,
    1759812991.1402056,
    1759812991.2282395,
    1759812991.575483,
    1759812991.72193,
    1759812991.9374976,
    1759812992.2258565,
    1759812992.2399666,
    1759812992.2553704,
    1759812992.3582425,
    1759812992.3582902,
    1759812992.42711,
    1759812992.5543675,
    1759812992.6084454,
    1759812992.7011359,
    1759812992.8027942,
    1759812992.9559808,
    1759812992.977724,
    1759812993.0052395,
    1759812993.2165456,
    1759812993.2345092,
    1759812993.2474287,
    1759812993.3859339,
    1759812993.445729,
    1759812993.598875,
    1759812993.658635,
    1759812993.8051467,
    1759812993.8903344,
    1759812993.9344616,
    1759812994.0714047,
    1759812994.1245503,
    1759812994.1511707,
    1759812994.163659,
    1759812994.1780472,
    1759812994.219017,
    1759812994.2387393,
    1759812994.3236265,
    1759812994.4477584,
    1759812994.4803262,
    1759812994.503797,
    1759812994.725066,
    1759812994.7459567,
    1759812995.217482,
    1759812995.232425,
    1759812995.2444937,
    1759812995.2661614,
    1759812995.2871509,
    1759812995.3894465,
    1759812995.473076,
    1759812995.5386736,
    1759812995.5929117,
    1759812995.6192427,
    1759812995.691602,
    1759812995.8003643,
    1759812995.8252091,
    1759812995.8290193,
    1759812995.8449373,
    1759812995.912042,
    1759812995.9558377,
    1759812996.0324357,
    1759812996.5171134,
    1759812996.9157627,
    1759812997.1910715,
    1759812997.213813,
    1759812997.237206,
    1759812997.3695517,
    1759812997.5042446,
    1759812997.5211868,
    1759812997.6603515,
    1759812997.915379,
    1759812997.954192,
    1759812998.1003237,
    1759812998.14135,
    1759812998.2288983,
    1759812998.3456771,
    1759812998.5091305,
    1759812998.5128825,
    1759812998.5184166,
    1759812998.530374,
    1759812998.5795684,
    1759812998.5795193,
    1759812999.0146277,
    1759812999.1714916,
    1759812999.2230139,
    1759812999.7220705,
    1759812999.8918712,
    1759812999.9115653,
    1759813000.0190847,
    1759813000.1812928,
    1759813000.2599094,
    1759813000.4092174,
    1759813000.5491283,
    1759813000.568393,
    1759813000.6505036,
    1759813000.8629417,
    1759813001.039118,
    1759813001.0515647,
    1759813001.1020024,
    1759813001.2418694,
    1759813001.2921085,
    1759813001.3108728,
    1759813001.3929703,
    1759813001.457752,
    1759813001.5246787,
    1759813001.9458065,
    1759813002.1738796,
    1759813002.1787207,
    1759813002.206299,
    1759813002.3591416,
    1759813002.494439,
    1759813002.913338,
    1759813003.0179725,
    1759813003.0863333,
    1759813003.1231365,
    1759813003.2871785,
    1759813003.5539675,
    1759813003.6922405,
    1759813003.7949345,
    1759813003.8560584,
    1759813003.9703498,
    1759813004.0192568,
    1759813004.044016,
    1759813004.052934,
    1759813004.2612293,
    1759813004.3236616,
    1759813004.3936322,
    1759813004.3935835,
    1759813004.5623813,
    1759813004.6125767,
    1759813004.7229676,
    1759813004.7296083,
    1759813004.7987466,
    1759813004.8149092,
    1759813005.0546253,
    1759813005.0593464,
    1759813005.2809846,
    1759813005.4604886,
    1759813005.5389056,
    1759813005.6158094,
    1759813005.6454344,
    1759813005.8058379,
    1759813005.8341665,
    1759813005.936408,
    1759813005.9452872,
    1759813005.9722698,
    1759813006.0636983,
    1759813006.1466417,
    1759813006.1937122,
    1759813006.2108374,
    1759813006.2781494,
    1759813006.3082745,
    1759813006.535306,
    1759813006.5932856,
    1759813006.8017776,
    1759813006.8529894,
    1759813006.940168,
    1759813006.9402149,
    1759813006.979574,
    1759813007.181436,
    1759813007.2482393,
    1759813007.3519838,
    1759813007.4119148,
    1759813007.4814758,
    1759813007.4856722,
    1759813007.5887017,
    1759813007.8154645,
    1759813007.8155515,
    1759813007.9971254,
    1759813008.0468879,
    1759813008.1683881,
    1759813008.2323742,
    1759813008.3435316,
    1759813008.414971,
    1759813008.4458604,
    1759813008.4580474,
    1759813008.4980638,
    1759813008.8319724,
    1759813008.8689404,
    1759813008.8945394,
    1759813008.906939,
    1759813009.157221,
    1759813009.1656172,
    1759813009.289745,
    1759813009.3130288,
    1759813009.3712993,
    1759813009.4572713,
    1759813009.5406158,
    1759813009.7513065,
    1759813009.7650144,
    1759813009.776271,
    1759813009.8500574,
    1759813009.9578218,
    1759813009.9825554,
    1759813009.9980104,
    1759813010.080519,
    1759813010.0804768,
    1759813010.2286103,
    1759813010.2554564,
    1759813010.4180717,
    1759813010.4182086,
    1759813010.4873805,
    1759813010.532172,
    1759813010.550731,
    1759813010.6277075,
    1759813010.6697555,
    1759813010.7528493,
    1759813010.8121276,
    1759813010.8194373,
    1759813010.909001,
    1759813010.9378865,
    1759813010.9979007,
    1759813011.035288,
    1759813011.0446293,
    1759813011.1473691,
    1759813011.2681296,
    1759813011.2934427,
    1759813011.426298,
    1759813011.753347,
    1759813011.855496,
    1759813012.01582,
    1759813012.0605364,
    1759813012.1592708,
    1759813012.4215612,
    1759813012.442981,
    1759813012.7638397,
    1759813012.7910032,
    1759813012.7909596,
    1759813012.790988,
    1759813012.8193603,
    1759813013.1020076,
    1759813013.2042694,
    1759813013.2739773,
    1759813013.299492,
    1759813013.3751612,
    1759813013.4019017,
    1759813013.5657523,
    1759813013.647909,
    1759813013.6838706,
    1759813013.7963157,
    1759813013.9901834,
    1759813014.127222,
    1759813014.146235,
    1759813014.2574165,
    1759813014.3352017,
    1759813014.4352117,
    1759813014.4716022,
    1759813014.696123,
    1759813014.909506,
    1759813014.959571,
    1759813015.1266677,
    1759813015.1719613,
    1759813015.2684598,
    1759813015.277205,
    1759813015.281622,
    1759813015.3727665,
    1759813015.6332805,
    1759813015.6333241,
    1759813015.8992295,
    1759813015.9145348,
    1759813016.3031945,
    1759813016.3240118,
    1759813016.3433414,
    1759813016.413664,
    1759813016.431675,
    1759813016.5674531,
    1759813016.8353422,
    1759813016.8515198,
    1759813017.0619605,
    1759813017.092485,
    1759813017.241858,
    1759813017.6186361,
    1759813017.6280706,
    1759813017.628031,
    1759813017.765225,
    1759813017.8930275,
    1759813017.9262538,
    1759813018.2361548,
    1759813018.3575153,
    1759813018.380193,
    1759813018.387892,
    1759813018.8750122,
    1759813019.000688,
    1759813019.0242374,
    1759813019.1913388,
    1759813019.4077876,
    1759813019.5885956,
    1759813019.7304087,
    1759813019.8287125,
    1759813019.828696,
    1759813019.9158669,
    1759813019.9302535,
    1759813019.938436,
    1759813019.9654033,
    1759813020.0119545,
    1759813020.0711958,
    1759813020.266693,
    1759813020.3235495,
    1759813020.4304874,
    1759813020.4588487,
    1759813020.5047317,
    1759813020.6999705,
    1759813020.787321,
    1759813020.8435237,
    1759813020.8489213,
    1759813020.8901727,
    1759813020.8901262,
    1759813021.1932168,
    1759813021.3361712,
    1759813021.4216053,
    1759813021.4216492,
    1759813021.4409587,
    1759813021.5655677,
    1759813021.6826246,
    1759813021.7113922,
    1759813021.7739112,
    1759813022.042625,
    1759813022.4833558,
    1759813022.7927597,
    1759813022.988565,
    1759813023.1784842,
    1759813023.4694524,
    1759813023.642581,
    1759813023.7208364,
    1759813023.7892501,
    1759813023.889691,
    1759813023.896741,
    1759813023.9028697,
    1759813023.9418163,
    1759813024.2457163,
    1759813024.3601103,
    1759813024.5277698,
    1759813024.553856,
    1759813024.849794,
    1759813024.8583071,
    1759813024.8635082,
    1759813024.9374225,
    1759813024.9932015,
    1759813025.3401794,
    1759813025.3517678,
    1759813025.356594,
    1759813025.403193,
    1759813025.452945,
    1759813025.5062702,
    1759813025.5868325,
    1759813025.7490888,
    1759813025.757224,
    1759813026.0054226,
    1759813026.0613325,
    1759813026.1416185,
    1759813026.3111079,
    1759813026.3936563,
    1759813026.462102,
    1759813026.5882008,
    1759813026.6339219,
    1759813026.7143989,
    1759813026.8070607,
    1759813026.9252975,
    1759813027.0040474,
    1759813027.0591109,
    1759813027.1118672,
    1759813027.1601443,
    1759813027.1821766,
    1759813027.346532,
    1759813027.4038439,
    1759813027.4747896,
    1759813027.610806,
    1759813027.6276178,
    1759813027.8024354,
    1759813027.8023853,
    1759813027.8774123,
    1759813027.9493563,
    1759813027.9833667,
    1759813028.098265,
    1759813028.1286414,
    1759813028.5381262,
    1759813028.5579865,
    1759813028.5987358,
    1759813028.6232855,
    1759813028.6370363,
    1759813028.6999953,
    1759813028.787083,
    1759813029.0826888,
    1759813029.092119,
    1759813029.1408424,
    1759813029.1781657,
    1759813029.1782105,
    1759813029.2557883,
    1759813029.3903272,
    1759813029.474456,
    1759813029.5220819,
    1759813029.5272095,
    1759813029.5918431,
    1759813029.6970356,
    1759813030.046357,
    1759813030.0464077,
    1759813030.114894,
    1759813030.128146,
    1759813030.2885785,
    1759813030.5750773,
    1759813030.6708078,
    1759813030.6958067,
    1759813030.7101223,
    1759813030.7860265,
    1759813030.970176,
    1759813031.1004388,
    1759813031.130056,
    1759813031.187989,
    1759813031.21997,
    1759813031.313726,
    1759813031.413456,
    1759813031.42535,
    1759813031.481523,
    1759813031.5459259,
    1759813031.6237712,
    1759813031.6793957,
    1759813031.801903,
    1759813032.035343,
    1759813032.0520854,
    1759813032.1654892,
    1759813032.1859474,
    1759813032.2464533,
    1759813032.4627688,
    1759813032.4789963,
    1759813032.4859736,
    1759813032.6945074,
    1759813032.7050164,
    1759813032.7454815,
    1759813032.803744,
    1759813032.938744,
    1759813032.945339,
    1759813033.0082777,
    1759813033.04853,
    1759813033.1020596,
    1759813033.1276429,
    1759813033.190365,
    1759813033.3939373,
    1759813033.6841085,
    1759813033.6868362,
    1759813033.7494905,
    1759813033.8331752,
    1759813033.8605285,
    1759813033.8850935,
    1759813033.9082112,
    1759813033.9218106,
    1759813034.0027747,
    1759813034.0925581,
    1759813034.1321876,
    1759813034.1561487,
    1759813034.2182024,
    1759813034.2763762,
    1759813034.2875543,
    1759813034.4471335,
    1759813034.5618567,
    1759813034.8464804,
    1759813034.9038265,
    1759813035.0545135,
    1759813035.0969524,
    1759813035.1964018,
    1759813035.3189511,
    1759813035.3367276,
    1759813035.3885055,
    1759813035.4273562,
    1759813035.4729004,
    1759813035.550925,
    1759813035.6806357,
    1759813035.7894552,
    1759813035.8579905,
    1759813035.8695755,
    1759813035.906417,
    1759813035.9162214,
    1759813035.9870799,
    1759813036.0421274,
    1759813036.251529,
    1759813036.4381223,
    1759813036.5288153,
    1759813036.584643,
    1759813036.6008868,
    1759813036.6524198,
    1759813036.7213585,
    1759813036.9135802,
    1759813037.4144778,
    1759813037.477428,
    1759813037.5986385,
    1759813037.6885276,
    1759813037.7004178,
    1759813037.748833,
    1759813037.830136,
    1759813037.842366,
    1759813037.8423214,
    1759813037.8521523,
    1759813037.9013503,
    1759813037.9762816,
    1759813037.9763281,
    1759813038.0140526,
    1759813038.1888473,
    1759813038.2063558,
    1759813038.314087,
    1759813038.3484416,
    1759813038.3599942,
    1759813038.36003,
    1759813038.427767,
    1759813038.4645727,
    1759813038.5367992,
    1759813038.6475255,
    1759813038.6881335,
    1759813038.7323136,
    1759813038.7642865,
    1759813038.7851799,
    1759813038.819797,
    1759813038.8267624,
    1759813038.83114,
    1759813038.8618217,
    1759813039.035355,
    1759813039.1843524,
    1759813039.1843054,
    1759813039.2260814,
    1759813039.283325,
    1759813039.3227785,
    1759813039.3863034,
    1759813039.3923872,
    1759813039.4583058,
    1759813039.5056953,
    1759813039.5637844,
    1759813039.5695524,
    1759813039.6775334,
    1759813039.7472754,
    1759813039.8797038,
    1759813039.9170115,
    1759813039.922997,
    1759813040.1716805,
    1759813040.4332557,
    1759813040.5038226,
    1759813040.6238842,
    1759813040.712747,
    1759813040.74574,
    1759813040.759654,
    1759813040.7596984,
    1759813040.7877858,
    1759813040.885741,
    1759813041.178541,
    1759813041.1886148,
    1759813041.25074,
    1759813041.363231,
    1759813041.4039662,
    1759813041.4724498,
    1759813041.515341,
    1759813041.5707042,
    1759813041.5828176,
    1759813041.6476936,
    1759813041.80539,
    1759813041.896586,
    1759813041.9862993,
    1759813042.0194938,
    1759813042.040952,
    1759813042.040885,
    1759813042.0409384,
    1759813042.0903935,
    1759813042.170266,
    1759813042.252965,
    1759813042.2574441,
    1759813042.2927642,
    1759813042.3074167,
    1759813042.3139756,
    1759813042.3714314,
    1759813042.4751968,
    1759813042.637718,
    1759813042.6716995,
    1759813042.6717448,
    1759813042.6806123,
    1759813042.869748,
    1759813042.9413102,
    1759813042.9695783,
    1759813043.1000395,
    1759813043.2410612,
    1759813043.265487,
    1759813043.298637,
    1759813043.4167874,
    1759813043.4168344,
    1759813043.4199488,
    1759813043.4481072,
    1759813043.467125,
    1759813043.4670813,
    1759813043.487572,
    1759813043.5281892,
    1759813043.562463,
    1759813043.7274306,
    1759813043.7592473,
    1759813043.852218,
    1759813043.9241662,
    1759813044.0150056,
    1759813044.2121363,
    1759813044.2768571,
    1759813044.2866542,
    1759813044.2927275,
    1759813044.299292,
    1759813044.29933,
    1759813044.3307798,
    1759813044.3635495,
    1759813044.3828313,
    1759813044.4741461,
    1759813044.672707,
    1759813044.6801724,
    1759813044.7727695,
    1759813044.8778594,
    1759813044.8921387,
    1759813044.933652,
    1759813045.041322,
    1759813045.0817475,
    1759813045.1610034,
    1759813045.192139,
    1759813045.196641,
    1759813045.269862,
    1759813045.2786498,
    1759813045.321297,
    1759813045.3448064,
    1759813045.404379,
    1759813045.4190025,
    1759813045.4587374,
    1759813045.5388308,
    1759813045.6626134,
    1759813045.806803,
    1759813046.1170902,
    1759813046.402137,
    1759813046.43088,
    1759813046.44782,
    1759813046.4884362,
    1759813046.5389698,
    1759813046.5878077,
    1759813046.8002424,
    1759813046.8120284,
    1759813046.815822,
    1759813046.8456025,
    1759813046.8538105,
    1759813046.876133,
    1759813046.982877,
    1759813047.012147,
    1759813047.0406883,
    1759813047.0928447,
    1759813047.1017246,
    1759813047.101686,
    1759813047.2450154,
    1759813047.3997126,
    1759813047.4719942,
    1759813047.5431058,
    1759813047.547013,
    1759813047.931437,
    1759813048.0452173,
    1759813048.16757,
    1759813048.1880903,
    1759813048.362289,
    1759813048.4847817,
    1759813048.7029755,
    1759813048.708381,
    1759813048.760567,
    1759813048.8069904,
    1759813048.8590138,
    1759813049.0266256,
    1759813049.0405846,
    1759813049.1770778,
    1759813049.2199726,
    1759813049.626518,
    1759813049.631389,
    1759813049.701473,
    1759813049.7429416,
    1759813049.7555144,
    1759813049.8374555,
    1759813049.8461552,
    1759813049.9163423,
    1759813049.9240253,
    1759813049.924069,
    1759813050.0600789,
    1759813050.0990841,
    1759813050.1840024,
    1759813050.2913897,
    1759813050.2913713,
    1759813050.291303,
    1759813050.381898,
    1759813050.4831212,
    1759813050.4893503,
    1759813050.4957852,
    1759813050.6404767,
    1759813050.6738153,
    1759813050.7114003,
    1759813050.7546968,
    1759813050.8011231,
    1759813050.8263946,
    1759813050.913022,
    1759813050.982101,
    1759813051.024204,
    1759813051.0942545,
    1759813051.1544745,
    1759813051.2538304,
    1759813051.2773902,
    1759813051.3057323,
    1759813051.6500278,
    1759813051.749102,
    1759813051.8535547,
    1759813051.9013703,
    1759813051.984575,
    1759813052.1173742,
    1759813052.3306346,
    1759813052.6664145,
    1759813052.7312095,
    1759813052.8415668,
    1759813053.1027763,
    1759813053.1945493,
    1759813053.2567782,
    1759813053.5037148,
    1759813053.614787,
    1759813053.6232102,
    1759813053.910869,
    1759813053.9453979,
    1759813053.997444,
    1759813054.0642445,
    1759813054.197242,
    1759813054.2305794,
    1759813054.4686773,
    1759813054.4761894,
    1759813054.541069,
    1759813054.5486329,
    1759813054.5524025,
    1759813054.6560564,
    1759813054.9257562,
    1759813054.9635148,
    1759813055.2133338,
    1759813055.315734,
    1759813055.4178107,
    1759813055.4177656,
    1759813055.5045428,
    1759813055.6569104,
    1759813055.7739108,
    1759813055.777486,
    1759813055.82516,
    1759813055.8875074,
    1759813056.229465,
    1759813056.343152,
    1759813056.3639276,
    1759813056.4901571,
    1759813056.5076182,
    1759813056.6189497,
    1759813056.6624734,
    1759813056.8826616,
    1759813056.9550323,
    1759813056.9684794,
    1759813056.9780784,
    1759813057.027799,
    1759813057.1415598,
    1759813057.172273,
    1759813057.2084188,
    1759813057.208372,
    1759813057.2169101,
    1759813057.3854778,
    1759813057.4433773,
    1759813057.4588675,
    1759813058.142763,
    1759813058.2652802,
    1759813058.2766056,
    1759813058.4473436,
    1759813058.6761606,
    1759813058.6990302,
    1759813058.7489936,
    1759813058.7584438,
    1759813058.895152,
    1759813059.1019986,
    1759813059.1165512,
    1759813059.252482,
    1759813059.309021,
    1759813059.3135724,
    1759813059.3540032,
    1759813059.4486942,
    1759813059.4909265,
    1759813059.6158025,
    1759813059.668172,
    1759813059.7642593,
    1759813059.8009112,
    1759813060.232617,
    1759813060.723915,
    1759813060.860301,
    1759813060.9208848,
    1759813060.9466283,
    1759813060.993088,
    1759813061.0892248,
    1759813061.0940282,
    1759813061.251552,
    1759813061.2530382,
    1759813061.4591339,
    1759813061.4772494,
    1759813061.5281625,
    1759813061.5818677,
    1759813061.5950353,
    1759813061.9008496,
    1759813061.9688604,
    1759813062.0003679,
    1759813062.164025,
    1759813062.1924572,
    1759813062.2480829,
    1759813062.4348185,
    1759813062.614871,
    1759813062.7464886,
    1759813062.8373394,
    1759813063.079484,
    1759813063.5142164,
    1759813063.5503967,
    1759813063.781813,
    1759813063.8379965,
    1759813064.0241323,
    1759813064.1133718,
    1759813064.267437,
    1759813064.2861722,
    1759813064.2947817,
    1759813064.3362865,
    1759813064.492049,
    1759813064.5507052,
    1759813064.563777,
    1759813064.580135,
    1759813064.614121,
    1759813064.6624582,
    1759813064.801999,
    1759813064.8388648,
    1759813064.9233904,
    1759813065.3091908,
    1759813065.4847717,
    1759813065.4847267,
    1759813065.742375,
    1759813065.834031,
    1759813065.9875164,
    1759813066.0138693,
    1759813066.1502995,
    1759813066.290108,
    1759813066.3036397,
    1759813066.6396139,
    1759813066.7018054,
    1759813067.0009196,
    1759813067.098116,
    1759813067.1768997,
    1759813067.1984105,
    1759813067.3342338,
    1759813067.5004766,
    1759813067.603846,
    1759813067.8255706,
    1759813067.883706,
    1759813067.9467404,
    1759813068.1797833,
    1759813068.291945,
    1759813068.2961333,
    1759813068.3210669,
    1759813068.3970318,
    1759813068.4314008,
    1759813068.53667,
    1759813068.5412102,
    1759813068.669611,
    1759813068.7490234,
    1759813068.9364934,
    1759813068.9522948,
    1759813068.966562,
    1759813069.1591918,
    1759813069.2836683,
    1759813069.3180661,
    1759813069.3583877,
    1759813069.4496126,
    1759813069.4919872,
    1759813069.648749,
    1759813069.6823602,
    1759813069.8778064,
    1759813069.918398,
    1759813070.0763438,
    1759813070.083213,
    1759813070.14972,
    1759813070.2472098,
    1759813070.5576856,
    1759813070.9667342,
    1759813071.113555,
    1759813071.6227958,
    1759813071.701318,
    1759813071.7625434,
    1759813071.9571586,
    1759813072.0144982,
    1759813072.0445368,
    1759813072.1026409,
    1759813072.130815,
    1759813072.277985,
    1759813072.4527113,
    1759813072.622963,
    1759813072.7098253,
    1759813072.7135491,
    1759813072.787771,
    1759813073.069201,
    1759813073.125037,
    1759813073.3066165,
    1759813073.385271,
    1759813073.3853002,
    1759813073.4699268,
    1759813073.592581,
    1759813073.688298,
    1759813073.704125,
    1759813073.7336962,
    1759813073.929786,
    1759813074.146156,
    1759813074.1662312,
    1759813074.3937955,
    1759813074.5902395,
    1759813074.9286594,
    1759813074.938412,
    1759813075.002819,
    1759813075.1400046,
    1759813075.8937182,
    1759813075.9130876,
    1759813075.9796913,
    1759813076.3587642,
    1759813076.7971184,
    1759813076.8425221,
    1759813076.8607721,
    1759813076.8900461,
    1759813076.9366536,
    1759813077.5827641,
    1759813077.7847228,
    1759813077.8159719,
    1759813077.8707826,
    1759813077.9735281,
    1759813078.152129,
    1759813078.2147803,
    1759813078.3344781,
    1759813078.4552407,
    1759813078.487917,
    1759813079.0377336,
    1759813079.1764107,
    1759813079.2271287,
    1759813079.304711,
    1759813079.31118,
    1759813079.4286354,
    1759813079.591714,
    1759813079.6181867,
    1759813079.7417748,
    1759813079.7514489,
    1759813079.7658136,
    1759813079.827487,
    1759813079.8445952,
    1759813079.8446333,
    1759813079.9241786,
    1759813079.9494736,
    1759813080.1958473,
    1759813080.2701426,
    1759813080.2959027,
    1759813080.4200568,
    1759813080.5480554,
    1759813080.5748353,
    1759813080.7628882,
    1759813080.8319197,
    1759813080.8438232,
    1759813080.8934069,
    1759813081.1412358,
    1759813081.1773875,
    1759813081.22497,
    1759813081.6306906,
    1759813081.6773272,
    1759813081.7292807,
    1759813081.7942336,
    1759813081.8293152,
    1759813081.9145744,
    1759813081.9292018,
    1759813081.9726057,
    1759813082.0790207,
    1759813082.159979,
    1759813082.211632,
    1759813082.3463855,
    1759813082.4694579,
    1759813082.594021,
    1759813082.6064801,
    1759813082.744499,
    1759813082.7981868,
    1759813082.860192,
    1759813082.8650274,
    1759813082.8743298,
    1759813082.9988384,
    1759813083.087754,
    1759813083.0977657,
    1759813083.1035833,
    1759813083.2731566,
    1759813083.2961009,
    1759813083.6096637,
    1759813083.9203086,
    1759813083.988228,
    1759813084.0547228,
    1759813084.0594342,
    1759813084.14449,
    1759813084.2628515,
    1759813084.3891728,
    1759813084.4246125,
    1759813084.526979,
    1759813084.5746083,
    1759813084.5850334,
    1759813084.6335168,
    1759813084.6549778,
    1759813084.6648862,
    1759813084.7206852,
    1759813084.7546458,
    1759813084.7621646,
    1759813084.7660391,
    1759813084.8241706,
    1759813084.8431017,
    1759813084.849443,
    1759813084.8967252,
    1759813084.8967698,
    1759813084.978466,
    1759813085.0243254,
    1759813085.0499623,
    1759813085.35226,
    1759813085.3808541,
    1759813085.4502132,
    1759813085.470461,
    1759813085.5262668,
    1759813085.5932167,
    1759813085.641315,
    1759813085.673832,
    1759813085.8644958,
    1759813086.080862,
    1759813086.2258914,
    1759813086.331676,
    1759813086.4143753,
    1759813086.6953797,
    1759813086.80687,
    1759813086.8253744,
    1759813086.9383965,
    1759813087.0556152,
    1759813087.5794053,
    1759813087.9902825,
    1759813088.0189133,
    1759813088.0686538,
    1759813088.092252,
    1759813088.1368313,
    1759813088.1723626,
    1759813088.1894805,
    1759813088.2288773,
    1759813088.3386476,
    1759813088.4849932,
    1759813088.7461748,
    1759813088.786281,
    1759813088.8791296,
    1759813089.03085,
    1759813089.2023416,
    1759813089.2197216,
    1759813089.2319596,
    1759813089.3446565,
    1759813089.5360904,
    1759813089.5522578,
    1759813089.6724336,
    1759813089.9754899,
    1759813090.0339088,
    1759813090.0632634,
    1759813090.1390507,
    1759813090.2537594,
    1759813090.3852832,
    1759813090.4344304,
    1759813090.523975,
    1759813090.5634298,
    1759813090.5745933,
    1759813090.6792393,
    1759813090.6944585,
    1759813090.694439,
    1759813091.0338154,
    1759813091.0666516,
    1759813091.103507,
    1759813091.1148207,
    1759813091.1782866,
    1759813091.1847663,
    1759813091.3150141,
    1759813091.3218946,
    1759813091.384041,
    1759813091.420585,
    1759813091.641818,
    1759813091.7218719,
    1759813091.7487583,
    1759813092.0552375,
    1759813092.1122847,
    1759813092.1164455,
    1759813092.2548842,
    1759813092.2639945,
    1759813092.2690794,
    1759813092.452926,
    1759813092.5532093,
    1759813092.6127424,
    1759813092.6386392,
    1759813092.6524353,
    1759813092.7420287,
    1759813092.8598835,
    1759813092.9149003,
    1759813092.9248328,
    1759813093.0018811,
    1759813093.024153,
    1759813093.0590122,
    1759813093.0680046,
    1759813093.1021159,
    1759813093.1544988,
    1759813093.2367089,
    1759813093.3323529,
    1759813093.432718,
    1759813093.6072617,
    1759813093.7595148,
    1759813093.8574202,
    1759813093.8711078,
    1759813094.064256,
    1759813094.21119,
    1759813094.3686435,
    1759813094.4738934,
    1759813094.7821262,
    1759813095.029768,
    1759813095.0391276,
    1759813095.3301716,
    1759813095.507617,
    1759813095.5510383,
    1759813095.6220164,
    1759813095.6494305,
    1759813095.7006588,
    1759813095.7192998,
    1759813095.7402244,
    1759813095.8207166,
    1759813095.904627,
    1759813096.226157,
    1759813096.30256,
    1759813096.3488483,
    1759813096.3888874,
    1759813096.827226,
    1759813096.8694506,
    1759813097.0115998,
    1759813097.0292819,
    1759813097.3086827,
    1759813097.3992262,
    1759813097.4031389,
    1759813097.4270208,
    1759813097.4617567,
    1759813097.5060973,
    1759813097.7427413,
    1759813097.7565327,
    1759813097.8493652,
    1759813098.0351527,
    1759813098.0351043,
    1759813098.078565,
    1759813098.0853422,
    1759813098.1553426,
    1759813098.2525532,
    1759813098.2773223,
    1759813098.3990479,
    1759813098.4679644,
    1759813098.4730287,
    1759813098.6293833,
    1759813098.6554303,
    1759813098.7308152,
    1759813098.7611804,
    1759813098.867797,
    1759813098.929815,
    1759813099.0740483,
    1759813099.1406312,
    1759813099.163815,
    1759813099.2045133,
    1759813099.3637497,
    1759813099.3658376,
    1759813099.3809874,
    1759813099.4588022,
    1759813099.5411005,
    1759813099.6937575,
    1759813099.7050562,
    1759813099.7689395,
    1759813099.872331,
    1759813099.9236023,
    1759813100.3763454,
    1759813100.4073126,
    1759813100.582735,
    1759813100.5937796,
    1759813100.7942052,
    1759813100.8259988,
    1759813101.135468,
    1759813101.2017772,
    1759813101.3944516,
    1759813101.4456172,
    1759813101.4906955,
    1759813101.5709736,
    1759813101.648215,
    1759813101.7725608,
    1759813101.8444974,
    1759813101.9539356,
    1759813101.9577496,
    1759813102.01587,
    1759813102.0158572,
    1759813102.024432,
    1759813102.0832355,
    1759813102.1044493,
    1759813102.1743472,
    1759813102.234207,
    1759813102.2342834,
    1759813102.2342694,
    1759813102.3379805,
    1759813102.4614286,
    1759813102.544045,
    1759813102.7429776,
    1759813102.9342582,
    1759813102.9580789,
    1759813103.160929,
    1759813103.1958904,
    1759813103.3022003,
    1759813103.3415742,
    1759813103.4688594,
    1759813103.5516603,
    1759813103.8419845,
    1759813103.924886,
    1759813103.9792883,
    1759813104.206298,
    1759813104.22787,
    1759813104.2445383,
    1759813104.2601354,
    1759813104.2988045,
    1759813104.3425708,
    1759813104.3698375,
    1759813104.384055,
    1759813104.467514,
    1759813104.493199,
    1759813104.977248,
    1759813105.0424914,
    1759813105.1649506,
    1759813105.2020285,
    1759813105.6231718,
    1759813105.6348882,
    1759813105.98871,
    1759813106.0399983,
    1759813106.0400455,
    1759813106.0828576,
    1759813106.1877828,
    1759813106.269375,
    1759813106.332782,
    1759813106.338361,
    1759813106.3632443,
    1759813106.494735,
    1759813106.5055661,
    1759813106.583153,
    1759813106.591138,
    1759813106.7633965,
    1759813106.8760018,
    1759813106.9892974,
    1759813106.9893322,
    1759813107.1369996,
    1759813107.2107725,
    1759813107.2639716,
    1759813107.3295865,
    1759813107.4273746,
    1759813107.526088,
    1759813107.5312514,
    1759813107.5599139,
    1759813107.7546933,
    1759813107.7711074,
    1759813107.85057,
    1759813107.863236,
    1759813107.921116,
    1759813108.074848,
    1759813108.0904133,
    1759813108.191568,
    1759813108.2300022,
    1759813108.3231552,
    1759813108.4121282,
    1759813108.6385074,
    1759813108.7269878,
    1759813108.7951555,
    1759813108.8028698,
    1759813108.8586435,
    1759813108.8764446,
    1759813108.895486,
    1759813108.9956,
    1759813109.0359974,
    1759813109.4547544,
    1759813109.5121148,
    1759813109.6072502,
    1759813109.6355724,
    1759813109.6515203,
    1759813109.657704,
    1759813109.6577318,
    1759813109.6933715,
    1759813109.9826887,
    1759813110.0049753,
    1759813110.046084,
    1759813110.0993001,
    1759813110.1296668,
    1759813110.1352484,
    1759813110.354979,
    1759813110.4083219,
    1759813110.44038,
    1759813110.521196,
    1759813110.614017,
    1759813110.6549063,
    1759813110.6974084,
    1759813110.9092262,
    1759813110.9139175,
    1759813110.9340546,
    1759813111.1610017,
    1759813111.2670763,
    1759813111.4193223,
    1759813111.5244224,
    1759813111.5292006,
    1759813111.5996099,
    1759813111.6173472,
    1759813111.691365,
    1759813111.7250514,
    1759813111.883044,
    1759813112.2628832,
    1759813112.262933,
    1759813112.2714453,
    1759813112.276721,
    1759813112.2851665,
    1759813112.4874272,
    1759813112.5066357,
    1759813112.6022527,
    1759813112.6054044,
    1759813112.6417348,
    1759813112.830679,
    1759813112.8307266,
    1759813112.8608968,
    1759813112.98432,
    1759813112.9929693,
    1759813113.1164112,
    1759813113.1730986,
    1759813113.3609002,
    1759813113.3680608,
    1759813113.3736033,
    1759813113.4701703,
    1759813113.6251297,
    1759813113.7379427,
    1759813113.8718846,
    1759813113.8939672,
    1759813113.9735148,
    1759813114.1202278,
    1759813114.180666,
    1759813114.1955938,
    1759813114.2168689,
    1759813114.339493,
    1759813114.5584266,
    1759813114.5684247,
    1759813114.7942414,
    1759813114.8321,
    1759813114.9110987,
    1759813115.0936718,
    1759813115.1112146,
    1759813115.1583843,
    1759813115.2922745,
    1759813115.4162638,
    1759813115.5919034,
    1759813115.6792748,
    1759813115.8177314,
    1759813115.851624,
    1759813115.9078138,
    1759813116.0907733,
    1759813116.1073954,
    1759813116.476127,
    1759813116.4905016,
    1759813116.4966798,
    1759813116.5313854,
    1759813116.575659,
    1759813116.6924152,
    1759813116.8533933,
    1759813116.8639383,
    1759813116.8951688,
    1759813117.0320284,
    1759813117.205869,
    1759813117.4275205,
    1759813117.510253,
    1759813117.9104555,
    1759813117.920938,
    1759813117.9400475,
    1759813117.979244,
    1759813118.034712,
    1759813118.041618,
    1759813118.1608045,
    1759813118.1655056,
    1759813118.2477329,
    1759813118.3878078,
    1759813118.4857004,
    1759813118.4935021,
    1759813118.718798,
    1759813118.7882154,
    1759813118.9622266,
    1759813118.9669638,
    1759813119.0180013,
    1759813119.0772383,
    1759813119.2993884,
    1759813119.4479935,
    1759813119.4479718,
    1759813119.6348467,
    1759813119.6661627,
    1759813119.951362,
    1759813119.9764056,
    1759813120.0079012,
    1759813120.1149542,
    1759813120.1185312,
    1759813120.4272232,
    1759813120.527548,
    1759813120.5662267,
    1759813120.6274147,
    1759813120.633811,
    1759813120.7924654,
    1759813120.8006425,
    1759813120.8412724,
    1759813120.8803787,
    1759813120.9182146,
    1759813120.9917006,
    1759813121.0874066,
    1759813121.137259,
    1759813121.1880832,
    1759813121.2327585,
    1759813121.2559798,
    1759813121.3050513,
    1759813121.3430536,
    1759813121.4492657,
    1759813121.6122072,
    1759813121.6202006,
    1759813121.7192895,
    1759813121.723477,
    1759813121.7404768,
    1759813121.8099513,
    1759813121.9710436,
    1759813121.996725,
    1759813121.996764,
    1759813122.119752,
    1759813122.12682,
    1759813122.1596172,
    1759813122.2348278,
    1759813122.3432515,
    1759813122.389461,
    1759813122.4136212,
    1759813122.4857192,
    1759813122.511738,
    1759813122.6006758,
    1759813122.7465658,
    1759813122.863124,
    1759813123.0083756,
    1759813123.0708523,
    1759813123.0994725,
    1759813123.345126,
    1759813123.4283834,
    1759813123.5239964,
    1759813123.606544,
    1759813123.6577442,
    1759813123.6677802,
    1759813123.7183628,
    1759813123.740383,
    1759813123.924027,
    1759813123.9292758,
    1759813124.0415497,
    1759813124.2212758,
    1759813124.3217485,
    1759813124.327886,
    1759813124.3866022,
    1759813124.4208872,
    1759813124.488229,
    1759813124.5958853,
    1759813124.7401097,
    1759813124.7529206,
    1759813124.887994,
    1759813124.9383,
    1759813125.2977753,
    1759813125.429054,
    1759813125.4578402,
    1759813125.465997,
    1759813125.4826896,
    1759813125.4967656,
    1759813125.5219464,
    1759813125.5637565,
    1759813125.6165936,
    1759813125.6208196,
    1759813125.6309307,
    1759813125.6522737,
    1759813125.6708543,
    1759813125.792904,
    1759813125.8321927,
    1759813126.0639598,
    1759813126.0640051,
    1759813126.0912309,
    1759813126.1563141,
    1759813126.2093906,
    1759813126.2188663,
    1759813126.2335362,
    1759813126.5131965,
    1759813126.7225842,
    1759813126.7649255,
    1759813126.7994323,
    1759813126.8664882,
    1759813127.020524,
    1759813127.035607,
    1759813127.1375005,
    1759813127.1449127,
    1759813127.1979427,
    1759813127.2705853,
    1759813127.445149,
    1759813127.6058373,
    1759813127.9147177,
    1759813127.9330091,
    1759813128.0048285,
    1759813128.1538787,
    1759813128.250227,
    1759813128.2818203,
    1759813128.4507596,
    1759813128.4861982,
    1759813128.5303838,
    1759813128.6665118,
    1759813128.680464,
    1759813128.8593931,
    1759813128.86881,
    1759813128.8781035,
    1759813129.0001006,
    1759813129.0080204,
    1759813129.0080588,
    1759813129.0829816,
    1759813129.2438178,
    1759813129.2493007,
    1759813129.3615808,
    1759813129.3983712,
    1759813129.4511719,
    1759813129.45474,
    1759813129.517371,
    1759813129.6285546,
    1759813129.8037596,
    1759813129.931266,
    1759813129.9730039,
    1759813129.9730482,
    1759813130.0622761,
    1759813130.156103,
    1759813130.2936733,
    1759813130.3927507,
    1759813130.5210853,
    1759813130.52606,
    1759813130.742853,
    1759813130.769686,
    1759813130.9107578,
    1759813131.0086398,
    1759813131.0625784,
    1759813131.2541082,
    1759813131.3482985,
    1759813131.428016,
    1759813131.5050457,
    1759813131.6550915,
    1759813131.748788,
    1759813131.7599423,
    1759813131.853062,
    1759813131.9050493,
    1759813131.9139464,
    1759813131.9207642,
    1759813131.995063,
    1759813132.0490787,
    1759813132.0922348,
    1759813132.1339133,
    1759813132.1338663,
    1759813132.2067587,
    1759813132.2696173,
    1759813132.3360052,
    1759813132.389174,
    1759813132.3927958,
    1759813132.4412217,
    1759813132.5047207,
    1759813132.510707,
    1759813132.6168008,
    1759813132.6529732,
    1759813132.7336814,
    1759813132.7787461,
    1759813132.7940676,
    1759813132.9439743,
    1759813132.9817276,
    1759813133.0297015,
    1759813133.0555391,
    1759813133.0971353,
    1759813133.1858146,
    1759813133.4102392,
    1759813133.4153388,
    1759813133.5396214,
    1759813133.8877118,
    1759813133.9145544,
    1759813134.4050996,
    1759813134.5137916,
    1759813134.5557432,
    1759813134.5647378,
    1759813134.7166274,
    1759813134.763416,
    1759813134.866984,
    1759813134.8723567,
    1759813135.1414847,
    1759813135.1470702,
    1759813135.1555228,
    1759813135.1803026,
    1759813135.460803,
    1759813135.4608793,
    1759813135.4608645,
    1759813135.4763856,
    1759813135.4905214,
    1759813135.5368302,
    1759813135.5493343,
    1759813135.7797668,
    1759813135.8070283,
    1759813135.9408107,
    1759813136.104816,
    1759813136.136603,
    1759813136.1582196,
    1759813136.2175608,
    1759813136.2916803,
    1759813136.3022478,
    1759813136.9071534,
    1759813136.9761422,
    1759813137.1838434,
    1759813137.430769,
    1759813137.4353929,
    1759813137.4630783,
    1759813137.5460818,
    1759813137.5670607,
    1759813137.7862437,
    1759813137.8321161,
    1759813137.8567085,
    1759813137.8681684,
    1759813137.8900754,
    1759813137.9219532,
    1759813137.9652739,
    1759813138.0891938,
    1759813138.1559172,
    1759813138.6123865,
    1759813138.6828754,
    1759813138.7958717,
    1759813138.912481,
    1759813139.005045,
    1759813139.1068404,
    1759813139.1128838,
    1759813139.2145405,
    1759813139.3053524,
    1759813139.4027913,
    1759813139.4878209,
    1759813139.513853,
    1759813139.5334492,
    1759813139.5381408,
    1759813139.6104941,
    1759813139.6541274,
    1759813139.6798313,
    1759813139.7230527,
    1759813139.7370439,
    1759813139.8600647,
    1759813139.878067,
    1759813139.90357,
    1759813139.9647884,
    1759813140.0667605,
    1759813140.2744,
    1759813140.4666064,
    1759813140.5292585,
    1759813140.6610775,
    1759813140.6883526,
    1759813140.6919525,
    1759813140.7468846,
    1759813140.769969,
    1759813140.7781277,
    1759813140.805995,
    1759813141.010693,
    1759813141.0144544,
    1759813141.0167878,
    1759813141.1321046,
    1759813141.2140868,
    1759813141.2771733,
    1759813141.3056726,
    1759813141.3790913,
    1759813141.3912005,
    1759813141.8141658,
    1759813141.901737,
    1759813141.930944,
    1759813141.978585,
    1759813142.0780792,
    1759813142.2590628,
    1759813142.3116515,
    1759813142.4159312,
    1759813142.502099,
    1759813142.522057,
    1759813142.529424,
    1759813142.7724829,
    1759813142.8517168,
    1759813142.9153087,
    1759813142.949546,
    1759813143.0715616,
    1759813143.3900716,
    1759813143.5561311,
    1759813143.601839,
    1759813143.6160836,
    1759813143.6160443,
    1759813143.7417035,
    1759813143.8289201,
    1759813143.8870711,
    1759813143.9354107,
    1759813143.9670172,
    1759813144.1228416,
    1759813144.137977,
    1759813144.1841576,
    1759813144.2932324,
    1759813144.3400536,
    1759813144.4076056,
    1759813144.7621102,
    1759813144.7992601,
    1759813144.8421938,
    1759813144.902364,
    1759813144.9580114,
    1759813145.090895,
    1759813145.1053648,
    1759813145.1922288,
    1759813145.2750094,
    1759813145.4858263,
    1759813145.5136342,
    1759813145.5338404,
    1759813145.6634789,
    1759813145.750267,
    1759813145.9127796,
    1759813145.96658,
    1759813145.9709392,
    1759813146.0141284,
    1759813146.045582,
    1759813146.0612962,
    1759813146.1358438,
    1759813146.1416032,
    1759813146.181009,
    1759813146.222278,
    1759813146.497256,
    1759813146.6496232,
    1759813146.6981199,
    1759813146.7141695,
    1759813146.8765066,
    1759813146.9444041,
    1759813147.0144665,
    1759813147.0182104,
    1759813147.0555732,
    1759813147.210507,
    1759813147.2591,
    1759813147.3563118,
    1759813147.5171425,
    1759813147.5968294,
    1759813147.6186686,
    1759813147.6707902,
    1759813147.7789066,
    1759813147.811629,
    1759813147.8371227,
    1759813147.9760857,
    1759813147.9835885,
    1759813148.017644,
    1759813148.1088462,
    1759813148.1629312,
    1759813148.1695666,
    1759813148.1945548,
    1759813148.2560005,
    1759813148.3891711,
    1759813148.4079964,
    1759813148.5030215,
    1759813148.5519006,
    1759813148.5706127,
    1759813148.6213105,
    1759813148.7100275,
    1759813148.866446,
    1759813148.8840475,
    1759813148.9434052,
    1759813149.028705,
    1759813149.032087,
    1759813149.0877619,
    1759813149.2160506,
    1759813149.248784,
    1759813149.3640249,
    1759813149.5229783,
    1759813149.5466957,
    1759813149.5564137,
    1759813149.6941307,
    1759813149.7237508,
    1759813149.7301521,
    1759813149.8544862,
    1759813149.980484,
    1759813150.0398932,
    1759813150.0720325,
    1759813150.1167629,
    1759813150.19198,
    1759813150.1972303,
    1759813150.2621086,
    1759813150.2888649,
    1759813150.2889047,
    1759813150.3224943,
    1759813150.3661633,
    1759813150.40588,
    1759813150.4256785,
    1759813150.4651678,
    1759813150.67418,
    1759813150.787906,
    1759813150.7954943,
    1759813150.824487,
    1759813150.834126,
    1759813150.941246,
    1759813150.9643831,
    1759813150.969159,
    1759813151.0075295,
    1759813151.0603435,
    1759813151.0769007,
    1759813151.1491256,
    1759813151.154049,
    1759813151.1540844,
    1759813151.3013012,
    1759813151.4359827,
    1759813151.6558855,
    1759813151.6738715,
    1759813151.8723075,
    1759813151.885747,
    1759813152.06217,
    1759813152.1557798,
    1759813152.1893287,
    1759813152.1933665,
    1759813152.287555,
    1759813152.3658502,
    1759813152.4503846,
    1759813152.4610689,
    1759813152.4875643,
    1759813152.524726,
    1759813152.5697377,
    1759813152.6325078,
    1759813152.6632278,
    1759813152.6681278,
    1759813152.67681,
    1759813152.8546252,
    1759813152.8546877,
    1759813152.9128962,
    1759813153.0233762,
    1759813153.028578,
    1759813153.071555,
    1759813153.0920773,
    1759813153.1276984,
    1759813153.2297373,
    1759813153.2810621,
    1759813153.3016577,
    1759813153.3789122,
    1759813153.5643935,
    1759813153.594059,
    1759813153.6222272,
    1759813153.644574,
    1759813153.6828718,
    1759813153.7196784,
    1759813153.8063061,
    1759813153.8105037,
    1759813153.8603988,
    1759813153.8848014,
    1759813153.9048262,
    1759813153.941812,
    1759813154.0211444,
    1759813154.186936,
    1759813154.19983,
    1759813154.2300723,
    1759813154.380785,
    1759813154.489399,
    1759813154.5859969,
    1759813154.9777067,
    1759813155.0119076,
    1759813155.1202486,
    1759813155.152622,
    1759813155.1919982,
    1759813155.3094792,
    1759813155.408906,
    1759813155.6250663,
    1759813155.6312416,
    1759813155.6891394,
    1759813155.7515585,
    1759813155.7620056,
    1759813155.7824273,
    1759813155.8386588,
    1759813155.851042,
    1759813156.0383406,
    1759813156.0502727,
    1759813156.0640957,
    1759813156.1445618,
    1759813156.2813132,
    1759813156.4315946,
    1759813156.4375856,
    1759813156.5040984,
    1759813156.5433156,
    1759813156.5738328,
    1759813156.6276433,
    1759813156.733647,
    1759813156.7494347,
    1759813156.7554348,
    1759813156.9504538,
    1759813156.9899795,
    1759813157.118172,
    1759813157.139589,
    1759813157.189103,
    1759813157.2008724,
    1759813157.3615735,
    1759813157.4606867,
    1759813157.5063727,
    1759813157.5403304,
    1759813157.6881385,
    1759813157.855303,
    1759813157.9149964,
    1759813158.1418116,
    1759813158.3134503,
    1759813158.5793223,
    1759813158.5991714,
    1759813158.7206833,
    1759813158.7801497,
    1759813158.817136,
    1759813158.8665853,
    1759813158.8666346,
    1759813159.0382738,
    1759813159.045502,
    1759813159.1085236,
    1759813159.1203783,
    1759813159.1587079,
    1759813159.191086,
    1759813159.241815,
    1759813159.3596425,
    1759813159.3772063,
    1759813159.4441774,
    1759813159.4562745,
    1759813159.5125434,
    1759813159.5688415,
    1759813159.6347435,
    1759813159.7341528,
    1759813159.8379056,
    1759813159.9003046,
    1759813159.9761453,
    1759813160.151333,
    1759813160.1790876,
    1759813160.235681,
    1759813160.512864,
    1759813160.558061,
    1759813160.5943632,
    1759813160.6793995,
    1759813160.6920147,
    1759813160.7721667,
    1759813160.8088195,
    1759813160.9241328,
    1759813160.929168,
    1759813161.022115,
    1759813161.1038196,
    1759813161.1167257,
    1759813161.1343431,
    1759813161.2287257,
    1759813161.2286901,
    1759813161.2551637,
    1759813161.2911606,
    1759813161.4797018,
    1759813161.499389,
    1759813161.5215979,
    1759813161.578734,
    1759813161.6362796,
    1759813161.672064,
    1759813161.727755,
    1759813161.7962337,
    1759813161.8230286,
    1759813161.8533957,
    1759813161.9873252,
    1759813162.0821822,
    1759813162.0923874,
    1759813162.1780965,
    1759813162.2659106,
    1759813162.3992631,
    1759813162.425203,
    1759813162.6149695,
    1759813162.866262,
    1759813162.9208539,
    1759813162.9274745,
    1759813162.9976327,
    1759813163.0352533,
    1759813163.1083405,
    1759813163.1897712,
    1759813163.2103574,
    1759813163.2232463,
    1759813163.3318014,
    1759813163.3910878,
    1759813163.4650078,
    1759813163.470664,
    1759813163.5003479,
    1759813163.563145,
    1759813163.601777,
    1759813163.6112695,
    1759813163.8681078,
    1759813164.0247164,
    1759813164.1248431,
    1759813164.1365395,
    1759813164.1968267,
    1759813164.2174122,
    1759813164.2869933,
    1759813164.3386989,
    1759813164.367114,
    1759813164.4772434,
    1759813164.4989822,
    1759813164.5803733,
    1759813164.5896008,
    1759813164.6509895,
    1759813164.7822585,
    1759813164.8083146,
    1759813164.837271,
    1759813164.8511953,
    1759813164.943954,
    1759813165.035814,
    1759813165.0565462,
    1759813165.1050224,
    1759813165.1050692,
    1759813165.1120377,
    1759813165.4142792,
    1759813165.4756393,
    1759813165.7078893,
    1759813165.7755702,
    1759813165.930382,
    1759813165.9879026,
    1759813166.0256724,
    1759813166.0528102,
    1759813166.138489,
    1759813166.1565773,
    1759813166.2642672,
    1759813166.2642207,
    1759813166.4232635,
    1759813166.498539,
    1759813166.6364365,
    1759813166.6402419,
    1759813166.6904032,
    1759813166.701043,
    1759813166.7894886,
    1759813167.053721,
    1759813167.066003,
    1759813167.1361825,
    1759813167.1464903,
    1759813167.2055414,
    1759813167.2634792,
    1759813167.4210982,
    1759813167.425738,
    1759813167.4446204,
    1759813167.4912422,
    1759813167.586174,
    1759813167.7718353,
    1759813167.8010507,
    1759813167.8885992,
    1759813167.990434,
    1759813168.1603916,
    1759813168.1688724,
    1759813168.4320564,
    1759813168.4860785,
    1759813168.5123646,
    1759813168.6032143,
    1759813168.660326,
    1759813168.6710818,
    1759813168.6771793,
    1759813168.8079393,
    1759813168.8135612,
    1759813168.9359186,
    1759813168.9580078,
    1759813169.1868885,
    1759813169.4983501,
    1759813169.5343165,
    1759813169.6847932,
    1759813169.761855,
    1759813169.7670374,
    1759813169.7978911,
    1759813169.870803,
    1759813170.032117,
    1759813170.179377,
    1759813170.2104816,
    1759813170.3321233,
    1759813170.425193,
    1759813170.4571812
  ],
  "mean_ttft_ms": 58385.681525851935,
  "median_ttft_ms": 71289.95927700089,
  "std_ttft_ms": 26066.926831888504,
  "p99_ttft_ms": 88475.57086949615,
  "mean_tpot_ms": 138.79686794486207,
  "median_tpot_ms": 137.87582774966722,
  "std_tpot_ms": 11.402110715794016,
  "p99_tpot_ms": 172.6746222588675,
  "mean_itl_ms": 140.0940029776137,
  "median_itl_ms": 134.84296599926893,
  "std_itl_ms": 25.9116680960398,
  "p99_itl_ms": 234.94278174803185
}