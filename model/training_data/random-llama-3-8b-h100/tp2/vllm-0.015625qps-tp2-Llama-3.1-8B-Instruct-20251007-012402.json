{
  "date": "20251007-012402",
  "backend": "vllm",
  "model_id": "meta-llama/Llama-3.1-8B-Instruct",
  "tokenizer_id": "meta-llama/Llama-3.1-8B-Instruct",
  "num_prompts": 9,
  "tensor_parallel_size": 2,
  "request_rate": 0.015625,
  "burstiness": 1.0,
  "max_concurrency": null,
  "duration": 685.6939810969998,
  "completed": 7,
  "total_input_tokens": 43417,
  "total_output_tokens": 109,
  "request_throughput": 0.010208635620223951,
  "request_goodput:": null,
  "output_throughput": 0.15896304037205866,
  "total_token_throughput": 63.47729628655252,
  "input_lens": [
    13911,
    3103,
    5864,
    16384,
    42946,
    681,
    31474,
    1691,
    1783
  ],
  "output_lens": [
    9,
    24,
    2,
    16,
    0,
    46,
    0,
    8,
    4
  ],
  "ttfts": [
    0.04221099699861952,
    0.026171055000304477,
    0.03306782700019539,
    0.057884369998646434,
    0.0,
    0.02294263600197155,
    0.0,
    0.023940177001350094,
    0.021422751000500284
  ],
  "itls": [],
  "generated_texts": [
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad",
    "\ufffd.",
    "\ufffd.\ufffd.\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.",
    "",
    "\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_"
  ],
  "errors": [
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    ""
  ],
  "request_timestamps": [
    1759799556.3665915,
    1759799566.74019,
    1759799582.85367,
    1759799614.172498,
    1759799801.541366,
    1759800011.314154,
    1759800018.0135703
  ],
  "mean_ttft_ms": 32.519973285941106,
  "median_ttft_ms": 26.171055000304477,
  "std_ttft_ms": 12.342517134324083,
  "p99_ttft_ms": 56.94396761864481,
  "mean_tpot_ms": 4.134458725237879,
  "median_tpot_ms": 4.259716733294125,
  "std_tpot_ms": 0.3443018740112149,
  "p99_tpot_ms": 4.486239997438436,
  "mean_itl_ms": 4.284612862772751,
  "median_itl_ms": 4.308638999646064,
  "std_itl_ms": 0.4263753929148247,
  "p99_itl_ms": 5.004841719128307
}