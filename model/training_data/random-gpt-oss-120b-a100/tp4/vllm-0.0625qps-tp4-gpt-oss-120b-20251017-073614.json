{"date": "20251017-073614", "backend": "vllm", "model_id": "openai/gpt-oss-120b", "tokenizer_id": "openai/gpt-oss-120b", "num_prompts": 19, "tensor_parallel_size": 4, "request_rate": 0.0625, "burstiness": 1.0, "max_concurrency": null, "duration": 345.17060930799926, "completed": 18, "total_input_tokens": 116308, "total_output_tokens": 114, "request_throughput": 0.052148124766724896, "request_goodput:": null, "output_throughput": 0.33027145685592435, "total_token_throughput": 337.28827675509143, "input_lens": [13911, 3103, 5864, 16384, 15588, 681, 5682, 1691, 22106, 3138, 2341, 9894, 4615, 2284, 3255, 2884, 10337, 11837, 2819], "output_lens": [2, 6, 7, 13, 9, 12, 1, 3, 0, 16, 4, 10, 4, 2, 1, 12, 2, 3, 7], "ttfts": [0.07155444000090938, 0.1752238769986434, 0.32220990199857624, 1.0028260920007597, 0.9325686330012104, 0.05094211799951154, 0.31164897499911604, 0.09653404900018359, 0.0, 0.18595914500110666, 0.11691627199979848, 0.5516561000004003, 0.6046292539977003, 0.13357226800144417, 0.18202543599909404, 0.16467780499806395, 0.5819960049993824, 0.6520897519985738, 0.15994843500084244], "itls": [[0.00750316699850373], [0.0062490849995811, 0.0066842939995694906, 0.00664300800053752, 0.006515781002235599, 0.006991565998760052], [0.005437694002466742, 0.0057809629979601596, 0.006188401999679627, 0.005819194000650896, 0.006143127000541426, 0.006109263998951064], [0.005303468002239242, 0.00792283099872293, 0.00784609000038472, 0.007981480997841572, 0.008119617999909678, 0.008171064000634942, 0.008117484001559205, 0.007990488000359619, 0.008021233999897959, 0.008059936997597106, 0.008094180000625784, 0.008650747000501724], [0.006150671000796137, 0.007681678001972614, 0.00784007299807854, 0.007768520001263823, 0.007779259998642374, 0.007852304999687476, 0.008015249000891345, 0.008321919001900824], [0.005341467000107514, 0.006182221000926802, 0.006001375000778353, 0.005903342000237899, 0.00605667699710466, 0.006213759003003361, 0.00591077599892742, 0.006026571998518193, 0.005980936002742965, 0.0061921189990243874, 0.00602474799961783], [], [0.0056243840008392, 0.006346799000311876], [], [0.006401672999345465, 0.006568704000528669, 0.006615720998524921, 0.006791749001422431, 0.006689319001452532, 0.006637140999373514, 0.006753817997378064, 0.006741515000612708, 0.006660315000772243, 0.006683917999907862, 0.006721859001117991, 0.0068339969984663185, 0.006548907000251347, 0.006696652999380603, 0.00707943400266231], [0.005106693002744578, 0.00571671799843898, 0.00543916199967498], [0.07947572500052047, 0.08476081900153076, 0.04014244099744246, 0.006607814000744838, 0.007200186999398284, 0.007703003000642639, 0.006317663999652723, 0.0067046559997834265, 0.007020772001851583], [0.006613435001781909, 0.007188465999206528, 0.007541350998508278], [0.006405198000720702], [], [0.006123876999481581, 0.0063808960003370885, 0.006516047000332037, 0.006359505998261739, 0.006211260002601193, 0.006451766999816755, 0.0063052859986783005, 0.006241615999897476, 0.005336913000064669, 0.005602275999990525, 0.005789355000160867], [0.006016818002535729], [0.0056945579999592155, 0.007377239999186713], [0.0061203199984447565, 0.0065418049998697825, 0.006579575001524063, 0.006548517001647269, 0.006745013997715432, 0.006994617000600556]], "generated_texts": ["<|", "<|vq_lbr_", "\n\n```\n\nIt looks like the", "<|vq_12473|>\n\nIt looks like you're", "<|vq_lbr\n\nIt looks like", "<|vq_lbr_image_12457|>", "<", "<|vq", "", "\ufffd\n\nIt seems like the text you provided is a mix of different languages and characters", "<|vq_l", "<|vq_lbr\n\nIt looks like the", "<|vq_l", "<|", "<", "<|vq_lbr_image_12457|>", "<|", "<|vq", "<|vq_lbr_124"], "errors": ["", "", "", "", "", "", "", "", "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 360, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 388, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n", "", "", "", "", "", "", "", "", "", ""], "request_timestamps": [1760686229.131401, 1760686247.783393, 1760686281.7875004, 1760686321.9110847, 1760686325.8347392, 1760686339.1157346, 1760686371.1802747, 1760686382.5515525, 1760686462.9946885, 1760686464.3783352, 1760686469.6048868, 1760686469.7453558, 1760686499.2935872, 1760686515.9708061, 1760686545.4503126, 1760686550.3896477, 1760686558.521157, 1760686571.385822], "mean_ttft_ms": 349.83214211085095, "median_ttft_ms": 183.99229050010035, "std_ttft_ms": 289.3392176318475, "p99_ttft_ms": 990.8823239708362, "mean_tpot_ms": 7.859821102615076, "median_tpot_ms": 6.5620549169883216, "std_tpot_ms": 5.069667044796539, "p99_tpot_ms": 24.40547314714017, "mean_itl_ms": 8.602761531354494, "median_itl_ms": 6.5936945011344505, "std_itl_ms": 11.283699732526086, "p99_itl_ms": 79.73997970057097}