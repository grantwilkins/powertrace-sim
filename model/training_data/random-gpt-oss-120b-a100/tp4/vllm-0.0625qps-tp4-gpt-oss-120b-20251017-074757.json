{"date": "20251017-074757", "backend": "vllm", "model_id": "openai/gpt-oss-120b", "tokenizer_id": "openai/gpt-oss-120b", "num_prompts": 19, "tensor_parallel_size": 4, "request_rate": 0.0625, "burstiness": 1.0, "max_concurrency": null, "duration": 345.13171726899964, "completed": 18, "total_input_tokens": 116308, "total_output_tokens": 114, "request_throughput": 0.05215400120983547, "request_goodput:": null, "output_throughput": 0.330308674328958, "total_token_throughput": 337.3262849361925, "input_lens": [13911, 3103, 5864, 16384, 15588, 681, 5682, 1691, 22106, 3138, 2341, 9894, 4615, 2284, 3255, 2884, 10337, 11837, 2819], "output_lens": [2, 6, 7, 13, 9, 12, 1, 3, 0, 16, 4, 10, 4, 2, 1, 12, 2, 3, 7], "ttfts": [0.07166677900022478, 0.03513558199847466, 0.044094372999097686, 0.08239432499976829, 0.08807588099807617, 0.024221078998380108, 0.039749903000483755, 0.025931620999472216, 0.0, 0.04228276100184303, 0.030521128999680514, 0.0548488839995116, 0.035724704001040664, 0.029225432001112495, 0.03306312099812203, 0.03351673900033347, 0.05419439099932788, 0.0643997750012204, 0.03036509600133286], "itls": [[0.007559592002507998], [0.0067408020004222635, 0.006619153999054106, 0.0065437040029792115, 0.006520901999465423, 0.006870632998470683], [0.006784299002902117, 0.006788907998270588, 0.00694949700118741, 0.006866491999971913, 0.006927225000254111, 0.007105526998202549], [0.0071174490003613755, 0.008922459001041716, 0.00940466699830722, 0.00903636099974392, 0.009208932002366055, 0.009271228998841252, 0.009174097001960035, 0.009172823996777879, 0.009231645002728328, 0.009168827997200424, 0.009216045000357553, 0.009681311999884201], [0.0069849129977228586, 0.008880325000063749, 0.008995609001431148, 0.00898225499986438, 0.00901811099902261, 0.008986322001874214, 0.00907360499695642, 0.009503836001385935], [0.006363200998748653, 0.005917555001360597, 0.006251377999433316, 0.005813480998767773, 0.0061174180009402335, 0.006007613999827299, 0.006096668999816757, 0.00598424000054365, 0.005981935999443522, 0.006011230001604417, 0.006512132000352722], [], [0.006233394997252617, 0.006487558002845617], [], [0.0065139980033563916, 0.006620345997362165, 0.006390569000359392, 0.006733616999554215, 0.006717285999911837, 0.006569782002770808, 0.006790472998545738, 0.00654067800132907, 0.00652017999891541, 0.006751970999175683, 0.006615566999244038, 0.00645823500235565, 0.006692048998957034, 0.00664097499975469, 0.00659729399922071], [0.005798146998131415, 0.005771066000306746, 0.005873706999409478], [0.006549380999786081, 0.007753434001642745, 0.00794326699906378, 0.007806421999703161, 0.007812783998815576, 0.007862266000302043, 0.007921115000499412, 0.007899706000898732, 0.008188122999854386], [0.006350572002702393, 0.006609633997868514, 0.006934789998922497], [0.008760680000705179], [], [0.006803306001529563, 0.006490031999419443, 0.0065997259989671875, 0.006405165000614943, 0.006789850998757174, 0.006452022000303259, 0.006634290999500081, 0.006461850000050617, 0.006542480001371587, 0.0066729329992085695, 0.006982029000937473], [0.006714093000482535], [0.006635343001107685, 0.008437578999291873], [0.00657039299767348, 0.006562549002410378, 0.0065371909986424726, 0.006433348000427941, 0.006598905998544069, 0.006667064000794198]], "generated_texts": ["<|", "<|vq_lbr_", "\n\n```\n\nIt looks like the", "<|vq_12471|>\n\nIt looks like your", "<|vq_lbr\n\nIt looks like", "<|vq_lbr_image_12457|>", "<", "<|vq", "", "\ufffd\n\nIt seems like the text you provided is a mix of different languages and characters", "<|vq_l", "<|vq_lbr\n\nIt looks like the", "<|vq_l", "<|", "<", "<|vq_lbr_image_12457|>", "<|", "<|vq", "<|vq_lbr_124"], "errors": ["", "", "", "", "", "", "", "", "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 360, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 388, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n", "", "", "", "", "", "", "", "", "", ""], "request_timestamps": [1760686932.0742846, 1760686950.725167, 1760686984.7290947, 1760687024.8228936, 1760687028.7475321, 1760687042.0365865, 1760687074.1062608, 1760687085.4862, 1760687165.9256005, 1760687167.30798, 1760687172.5351357, 1760687172.6757052, 1760687202.2255027, 1760687218.9027288, 1760687248.3633287, 1760687253.299005, 1760687261.4300103, 1760687274.2906423], "mean_ttft_ms": 45.522865277639035, "median_ttft_ms": 37.73730350076221, "std_ttft_ms": 18.972835602747658, "p99_ttft_ms": 87.11001647836382, "mean_tpot_ms": 7.15184052818146, "median_tpot_ms": 6.686482000077376, "std_tpot_ms": 0.9620582125318987, "p99_tpot_ms": 9.013367420834584, "mean_itl_ms": 7.188162822937254, "median_itl_ms": 6.703070999719785, "std_itl_ms": 1.0964965914959166, "p99_itl_ms": 9.512709801310846}