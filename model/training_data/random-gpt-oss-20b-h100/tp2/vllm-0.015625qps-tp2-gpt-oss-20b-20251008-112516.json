{
  "date": "20251008-112516",
  "backend": "vllm",
  "model_id": "openai/gpt-oss-20b",
  "tokenizer_id": "openai/gpt-oss-20b",
  "num_prompts": 9,
  "tensor_parallel_size": 2,
  "request_rate": 0.015625,
  "burstiness": 1.0,
  "max_concurrency": null,
  "duration": 743.2451597539985,
  "completed": 7,
  "total_input_tokens": 43417,
  "total_output_tokens": 109,
  "request_throughput": 0.009418157532726996,
  "request_goodput:": null,
  "output_throughput": 0.14665416729532035,
  "total_token_throughput": 58.56210353849646,
  "input_lens": [
    13911,
    3103,
    5864,
    16384,
    42946,
    681,
    31474,
    1691,
    1783
  ],
  "output_lens": [
    9,
    24,
    2,
    16,
    0,
    46,
    0,
    8,
    4
  ],
  "ttfts": [
    0.04071466199820861,
    0.025398722998943413,
    0.033787472999392776,
    0.05832242500036955,
    0.0,
    0.021895005000260426,
    0.0,
    0.0218239440000616,
    0.02219889600019087
  ],
  "itls": [
    0.0027105077501801134,
    0.0028720997826262055,
    0.0015758189983898774,
    0.0028398329333867876,
    0.0028398329333867876,
    0.002913834399967325,
    0.002913834399967325,
    0.003365632666827878,
    0.002838400333227279
  ],
  "generated_texts": [
    "<|vq_clip_12273|>",
    "<|vq_clip_12273|>this is a screenshot of a conversation between two users on a messaging platform",
    "File<",
    "<|vq_clip_12273|>this is a screenshot of a",
    "",
    "<|vq_clip_12273|>this is a screenshot of a conversation between two users on a messaging app. the user is asking for a photo of a person named \"sara\" and the other user is responding",
    "",
    "ly<|vq_clip_12273",
    "<|vq_clip"
  ],
  "errors": [
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    ""
  ],
  "request_timestamps": [
    1759921973.6111555,
    1759921989.7087317,
    1759922021.0149164,
    1759922170.1858287,
    1759922269.2741282,
    1759922424.959212,
    1759922649.0705097
  ],
  "mean_ttft_ms": 32.020161142489606,
  "median_ttft_ms": 25.398722998943413,
  "std_ttft_ms": 12.633906439099231,
  "p99_ttft_ms": 57.265959220239885,
  "mean_tpot_ms": 2.6621105778871987,
  "median_tpot_ms": 2.8398083998278407,
  "std_tpot_ms": 0.4476225480142713,
  "p99_tpot_ms": 2.9120865196841006,
  "mean_itl_ms": 2.8885862871308325,
  "median_itl_ms": 2.9324310016818345,
  "std_itl_ms": 0.5203539168813903,
  "p99_itl_ms": 3.4848939976654947
}