{
  "date": "20251008-104750",
  "backend": "vllm",
  "model_id": "openai/gpt-oss-20b",
  "tokenizer_id": "openai/gpt-oss-20b",
  "num_prompts": 9,
  "tensor_parallel_size": 2,
  "request_rate": 0.015625,
  "burstiness": 1.0,
  "max_concurrency": null,
  "duration": 743.0940752889983,
  "completed": 7,
  "total_input_tokens": 43417,
  "total_output_tokens": 109,
  "request_throughput": 0.00942007241448886,
  "request_goodput:": null,
  "output_throughput": 0.14668398473989794,
  "total_token_throughput": 58.57401027329173,
  "input_lens": [
    13911,
    3103,
    5864,
    16384,
    42946,
    681,
    31474,
    1691,
    1783
  ],
  "output_lens": [
    9,
    24,
    2,
    16,
    0,
    46,
    0,
    8,
    4
  ],
  "ttfts": [
    0.04301686699909624,
    0.09199400299985427,
    0.1392385339968314,
    0.35573324500001036,
    0.0,
    0.037060734000988305,
    0.0,
    0.059023537000030046,
    0.061523169999418315
  ],
  "itls": [
    0.002643103875016095,
    0.0028295867390545977,
    0.0009554740026942454,
    0.0028022773333456524,
    0.0028022773333456524,
    0.0028736509333409936,
    0.0028736509333409936,
    0.0026699102854763623,
    0.0024093513335780394
  ],
  "generated_texts": [
    "<|vq_clip_12273|>",
    "<|vq_clip_12273|>this is a screenshot of a conversation between two users on a messaging app",
    "File<",
    "<|vq_clip_12273|>this is a screenshot of a",
    "",
    "<|vq_clip_12273|>this is a screenshot of a conversation between two users on a messaging app. the user is asking for a photo of a person named \"sara\" and the other user is responding",
    "",
    "ly<|vq_clip_12273",
    "<|vq_clip"
  ],
  "errors": [
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    ""
  ],
  "request_timestamps": [
    1759919727.5698488,
    1759919743.667199,
    1759919774.9657855,
    1759919924.0574396,
    1759920023.12783,
    1759920178.7604835,
    1759920402.8738503
  ],
  "mean_ttft_ms": 112.51286999946127,
  "median_ttft_ms": 61.523169999418315,
  "std_ttft_ms": 104.38610563331888,
  "p99_ttft_ms": 342.7435623398195,
  "mean_tpot_ms": 2.4546770016942814,
  "median_tpot_ms": 2.66987499968049,
  "std_tpot_ms": 0.628956332227305,
  "p99_tpot_ms": 2.87100110292406,
  "mean_itl_ms": 2.788692921575264,
  "median_itl_ms": 2.9077065009914804,
  "std_itl_ms": 0.5214109442078273,
  "p99_itl_ms": 3.216756030342367
}