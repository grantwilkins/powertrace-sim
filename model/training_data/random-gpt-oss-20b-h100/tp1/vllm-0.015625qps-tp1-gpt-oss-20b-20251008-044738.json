{
  "date": "20251008-044738",
  "backend": "vllm",
  "model_id": "openai/gpt-oss-20b",
  "tokenizer_id": "openai/gpt-oss-20b",
  "num_prompts": 9,
  "tensor_parallel_size": 1,
  "request_rate": 0.015625,
  "burstiness": 1.0,
  "max_concurrency": null,
  "duration": 743.0679099879999,
  "completed": 8,
  "total_input_tokens": 41852,
  "total_output_tokens": 726,
  "request_throughput": 0.010766176136080477,
  "request_goodput:": null,
  "output_throughput": 0.9770304843493033,
  "total_token_throughput": 57.30028094025432,
  "input_lens": [
    8192,
    2303,
    4302,
    8192,
    21473,
    520,
    15737,
    1269,
    1337
  ],
  "output_lens": [
    3,
    116,
    75,
    10,
    0,
    219,
    10,
    149,
    144
  ],
  "ttfts": [
    0.030410414000016317,
    0.02927142799990179,
    0.03172586300001967,
    0.04025776499997846,
    0.0,
    0.02223675599998387,
    0.06662482100000489,
    0.020768864000274334,
    0.022155608000048232
  ],
  "itls": [
    0.002763859999959095,
    0.0034071775826087897,
    0.00358744994594577,
    0.0033304712222211996,
    0.0033304712222211996,
    0.0033750746788991154,
    0.003299347333318615,
    0.003390280520270116,
    0.00339041230769202
  ],
  "generated_texts": [
    " ",
    "<|vq_clip_12273|>this image displays a list of 10 items that are being used in a project. the items are labeled as \"10 items\" and \"10 items\". the list includes a variety of items such as a \"10 items\" and \"10 items\" and a \"10 items\" and \"10 items\". the items are arranged in a neat and organized manner, with each item labeled as \"10 items\" and \"10 items\". the list appears to be a part of a larger project, possibly a software or a website.",
    "\u00e9ration<|vq_clip_12273|>this is a screenshot of a code editor with a python script. the script is written in a dark theme and contains a function that calculates the sum of two numbers. the code editor has a dark background with a green text color. the script is written in a dark theme and contains a function that calculates the sum of two",
    "<|vq_clip_12273|>",
    "",
    "<|vq_clip_12273|>this image displays a list of 10 items, each with a different color and shape. the items are arranged in a grid, with each item having a different color and shape. the first item is a blue square with a white circle in the center, the second item is a green square with a white circle in the center, and the third item is a yellow square with a white circle in the center. the fourth item is a red square with a white circle in the center, and the fifth item is a purple square with a white circle in the center. the sixth item is a blue square with a white circle in the center, the seventh item is a green square with a white circle in the center, and the eighth item is a yellow square with a white circle in the center. the ninth item is a red square with a white circle in the center, and the tenth item is a purple square with a white circle in the center. the image is a screenshot of a website that displays a list of items in a grid.",
    "<|vq_clip_12273|>",
    "<|vq_clip_12273|>this is a screenshot of a code editor with a python script. the script is written in a dark theme and has a line of code that says \"print('hello world')\". the code editor is open to a file named \"main.py\" and has a line of code that says \"print('hello world')\". the code editor also has a line of code that says \"print('hello world')\". the code editor is open to a file named \"main.py\" and has a line of code that says \"print('hello world')\". the code editor is open to a file named \"main.py\" and has a line of code that says \"print('hello world')\".",
    "<|vq_clip_12273|>this is a screenshot of a conversation between two users on a messaging app. the user is asking for a photo of a person named \"sara\" and the other user is responding with a photo of a person named \"sara\". the conversation is in a casual tone and the user is asking for a photo of a person named \"sara\" and the other user is responding with a photo of a person named \"sara\". the conversation is in a casual tone and the user is asking for a photo of a person named \"sara\". the conversation is in a casual tone and the user is asking for a photo of a person named \"sara\"."
  ],
  "errors": [
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    ""
  ],
  "request_timestamps": [
    1759898115.4509735,
    1759898131.5483603,
    1759898162.849977,
    1759898312.007332,
    1759898411.0275097,
    1759898559.9590013,
    1759898566.618673,
    1759898790.6775856
  ],
  "mean_ttft_ms": 32.931439875028445,
  "median_ttft_ms": 29.840920999959053,
  "std_ttft_ms": 14.091427860376793,
  "p99_ttft_ms": 64.77912708000302,
  "mean_tpot_ms": 3.3179538401720143,
  "median_tpot_ms": 3.382676059911679,
  "std_tpot_ms": 0.22424032587200926,
  "p99_tpot_ms": 3.5748281041506926,
  "mean_itl_ms": 3.4050829554313613,
  "median_itl_ms": 3.3956534999788346,
  "std_itl_ms": 0.23752897490366723,
  "p99_itl_ms": 4.265880790026131
}