{
  "date": "20251008-043509",
  "backend": "vllm",
  "model_id": "openai/gpt-oss-20b",
  "tokenizer_id": "openai/gpt-oss-20b",
  "num_prompts": 9,
  "tensor_parallel_size": 1,
  "request_rate": 0.015625,
  "burstiness": 1.0,
  "max_concurrency": null,
  "duration": 743.1626404669998,
  "completed": 7,
  "total_input_tokens": 43417,
  "total_output_tokens": 109,
  "request_throughput": 0.00941920330602361,
  "request_goodput:": null,
  "output_throughput": 0.1466704514795105,
  "total_token_throughput": 58.568606156854806,
  "input_lens": [
    13911,
    3103,
    5864,
    16384,
    42946,
    681,
    31474,
    1691,
    1783
  ],
  "output_lens": [
    9,
    24,
    2,
    16,
    0,
    46,
    0,
    8,
    4
  ],
  "ttfts": [
    0.048639099000183705,
    0.12171634800006359,
    0.19850669500010554,
    0.5582335579999835,
    0.0,
    0.0460445889998482,
    0.0,
    0.08220427599985669,
    0.07980941800019536
  ],
  "itls": [
    0.0032795193749848295,
    0.0033135288260837115,
    0.0015685340001709847,
    0.003443403466674984,
    0.003443403466674984,
    0.0033778385555554755,
    0.0033778385555554755,
    0.003444452714282826,
    0.0029321340000478813
  ],
  "generated_texts": [
    "<|vq_clip_12273|>",
    "<|vq_clip_12273|>this is a screenshot of a conversation between two users on a messaging app",
    "File<",
    "<|vq_clip_12273|>this is a screenshot of a",
    "",
    "<|vq_clip_12273|>this is a screenshot of a conversation between two users on a messaging app. the user is asking for a photo of a person named \"sara\" and the other user is responding",
    "",
    "ly<|vq_clip_12273",
    "<|vq_clip"
  ],
  "errors": [
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    ""
  ],
  "request_timestamps": [
    1759897366.7082627,
    1759897382.796181,
    1759897414.0919929,
    1759897563.229722,
    1759897662.3035152,
    1759897817.9677043,
    1759898042.079781
  ],
  "mean_ttft_ms": 162.16485471431952,
  "median_ttft_ms": 82.20427599985669,
  "std_ttft_ms": 168.76668349188438,
  "p99_ttft_ms": 536.6499462199906,
  "mean_tpot_ms": 3.0512538358515107,
  "median_tpot_ms": 3.3135139565175726,
  "std_tpot_ms": 0.6267226000605048,
  "p99_tpot_ms": 3.4443455079941225,
  "mean_itl_ms": 3.338992254904113,
  "median_itl_ms": 3.425742999979775,
  "std_itl_ms": 0.48094273886739886,
  "p99_itl_ms": 4.002073400079096
}