{
  "date": "20251007-174924",
  "backend": "vllm",
  "model_id": "meta-llama/Llama-3.1-8B-Instruct",
  "tokenizer_id": "meta-llama/Llama-3.1-8B-Instruct",
  "num_prompts": 9,
  "tensor_parallel_size": 4,
  "request_rate": 0.015625,
  "burstiness": 1.0,
  "max_concurrency": null,
  "duration": 685.5464131279996,
  "completed": 8,
  "total_input_tokens": 33388,
  "total_output_tokens": 120,
  "request_throughput": 0.0116695235315983,
  "request_goodput:": null,
  "output_throughput": 0.17504285297397448,
  "total_token_throughput": 48.87779931209948,
  "input_lens": [
    4868,
    1085,
    2052,
    8192,
    21473,
    238,
    15737,
    592,
    624
  ],
  "output_lens": [
    9,
    24,
    2,
    16,
    0,
    46,
    11,
    8,
    4
  ],
  "ttfts": [
    0.022272316999988107,
    0.01755854300017745,
    0.02031229300018822,
    0.039641198000026634,
    0.0,
    0.033161340999868116,
    0.07632588999967993,
    0.019418889000007766,
    0.015800447999936296
  ],
  "itls": [
    0.002862211499973455,
    0.0029471883912976964,
    0.002775121000013314,
    0.0029858115999862396,
    0.0029858115999862396,
    0.0030085452222212753,
    0.00299383790002139,
    0.0030265508570924532,
    0.0028591123333778037
  ],
  "generated_texts": [
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    ".t\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_"
  ],
  "errors": [
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    ""
  ],
  "request_timestamps": [
    1759858679.3823495,
    1759858689.7612813,
    1759858705.8626647,
    1759858737.1473613,
    1759858924.4701557,
    1759858985.2780297,
    1759859134.1381989,
    1759859140.836726
  ],
  "mean_ttft_ms": 30.561364874984065,
  "median_ttft_ms": 21.292305000088163,
  "std_ttft_ms": 18.935613294169602,
  "p99_ttft_ms": 73.75796155970419,
  "mean_tpot_ms": 2.9322156040843597,
  "median_tpot_ms": 2.966480810131991,
  "std_tpot_ms": 0.08415872042417098,
  "p99_tpot_ms": 3.025241692012998,
  "mean_itl_ms": 2.976173455351727,
  "median_itl_ms": 2.9680220000045665,
  "std_itl_ms": 0.32142095045936875,
  "p99_itl_ms": 3.702987750102693
}