{
  "date": "20251006-013407",
  "backend": "vllm",
  "model_id": "meta-llama/Llama-3.1-8B-Instruct",
  "tokenizer_id": "meta-llama/Llama-3.1-8B-Instruct",
  "num_prompts": 9,
  "tensor_parallel_size": 1,
  "request_rate": 0.015625,
  "burstiness": 1.0,
  "max_concurrency": null,
  "duration": 685.8727477959983,
  "completed": 8,
  "total_input_tokens": 33388,
  "total_output_tokens": 120,
  "request_throughput": 0.011663971233304447,
  "request_goodput:": null,
  "output_throughput": 0.1749595684995667,
  "total_token_throughput": 48.85454351069568,
  "input_lens": [
    4868,
    1085,
    2052,
    8192,
    21473,
    238,
    15737,
    592,
    624
  ],
  "output_lens": [
    9,
    24,
    2,
    16,
    0,
    46,
    11,
    8,
    4
  ],
  "ttfts": [
    0.026403699001093628,
    0.0391256090006209,
    0.06349877100001322,
    0.25855744099681033,
    0.0,
    0.020112930000323104,
    0.5241337959996599,
    0.029205326001829235,
    0.03016904000105569
  ],
  "itls": [
    0.0073061617499661224,
    0.007163882869581753,
    0.007435536997945746,
    0.0075662530667614195,
    NaN,
    0.007473874111100384,
    0.007838121299937485,
    0.007118610285812922,
    0.007130735000828281
  ],
  "generated_texts": [
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    ".t\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_"
  ],
  "errors": [
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    ""
  ],
  "request_timestamps": [
    1759713762.0056236,
    1759713772.3888452,
    1759713788.4996572,
    1759713819.8161066,
    1759714007.2410755,
    1759714068.1015337,
    1759714217.0582597,
    1759714223.7558568
  ],
  "mean_ttft_ms": 123.90082650017575,
  "median_ttft_ms": 34.647324500838295,
  "std_ttft_ms": 168.5172774740758,
  "p99_ttft_ms": 505.5434511494604,
  "mean_tpot_ms": 7.379073470167277,
  "median_tpot_ms": 7.37067356112675,
  "std_tpot_ms": 0.23428193441152778,
  "p99_tpot_ms": 7.8190618638582245,
  "mean_itl_ms": 7.411392482156641,
  "median_itl_ms": 7.204089999504504,
  "std_itl_ms": 0.4759606119510203,
  "p99_itl_ms": 8.969341200463532
}