{
  "date": "20251007-062710",
  "backend": "vllm",
  "model_id": "meta-llama/Llama-3.1-8B-Instruct",
  "tokenizer_id": "meta-llama/Llama-3.1-8B-Instruct",
  "num_prompts": 9,
  "tensor_parallel_size": 4,
  "request_rate": 0.015625,
  "burstiness": 1.0,
  "max_concurrency": null,
  "duration": 685.815204438004,
  "completed": 7,
  "total_input_tokens": 43417,
  "total_output_tokens": 109,
  "request_throughput": 0.010206831161954477,
  "request_goodput:": null,
  "output_throughput": 0.15893494237900543,
  "total_token_throughput": 63.466076165032945,
  "input_lens": [
    13911,
    3103,
    5864,
    16384,
    42946,
    681,
    31474,
    1691,
    1783
  ],
  "output_lens": [
    9,
    24,
    2,
    16,
    0,
    46,
    0,
    8,
    4
  ],
  "ttfts": [
    0.04151472199737327,
    0.06556803400599165,
    0.08934201800002484,
    0.2360625750006875,
    0.0,
    0.044486215003416874,
    0.0,
    0.04563994899945101,
    0.04404167000029702
  ],
  "itls": [],
  "generated_texts": [
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad_\u00ad",
    "\ufffd.",
    "\ufffd.\ufffd.\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.\ufffd\ufffd.",
    "",
    "\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0\ufffd\u00a0",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_"
  ],
  "errors": [
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    ""
  ],
  "request_timestamps": [
    1759817744.4732466,
    1759817754.8504195,
    1759817770.9613316,
    1759817802.2868793,
    1759817989.700482,
    1759818199.4582427,
    1759818206.1572828
  ],
  "mean_ttft_ms": 80.95074042960603,
  "median_ttft_ms": 45.63994899945101,
  "std_ttft_ms": 65.31321793143042,
  "p99_ttft_ms": 227.25934158064769,
  "mean_tpot_ms": 2.6265135818653405,
  "median_tpot_ms": 2.8444303752621636,
  "std_tpot_ms": 0.4994963070201733,
  "p99_tpot_ms": 3.103004936293776,
  "mean_itl_ms": 2.8919755882359857,
  "median_itl_ms": 2.94274399857386,
  "std_itl_ms": 0.4554637335145952,
  "p99_itl_ms": 3.4269140164542473
}