{
  "date": "20251007-063841",
  "backend": "vllm",
  "model_id": "meta-llama/Llama-3.1-8B-Instruct",
  "tokenizer_id": "meta-llama/Llama-3.1-8B-Instruct",
  "num_prompts": 9,
  "tensor_parallel_size": 4,
  "request_rate": 0.015625,
  "burstiness": 1.0,
  "max_concurrency": null,
  "duration": 685.8734869200052,
  "completed": 8,
  "total_input_tokens": 33388,
  "total_output_tokens": 120,
  "request_throughput": 0.011663958663754349,
  "request_goodput:": null,
  "output_throughput": 0.17495937995631522,
  "total_token_throughput": 48.85449086313509,
  "input_lens": [
    4868,
    1085,
    2052,
    8192,
    21473,
    238,
    15737,
    592,
    624
  ],
  "output_lens": [
    9,
    24,
    2,
    16,
    0,
    46,
    11,
    8,
    4
  ],
  "ttfts": [
    0.022281322999333497,
    0.020378689005156048,
    0.02055821000249125,
    0.03964255200116895,
    0.0,
    0.020004525998956524,
    0.06571051799983252,
    0.018867671002226416,
    0.01692716400430072
  ],
  "itls": [],
  "generated_texts": [
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_\ufffd_",
    "\r\n\t",
    ".t\n\t\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t\ufffd\t",
    "",
    "",
    "",
    "\ufffd_\ufffd_\ufffd_\ufffd_",
    "\ufffd_\ufffd_"
  ],
  "errors": [
    "",
    "",
    "",
    "",
    "Traceback (most recent call last):\n  File \"/home/azureuser/powertrace-sim/client/backend_request_func.py\", line 296, in async_request_openai_completions\n    async for chunk_bytes in response.content:\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/miniconda3/envs/inference/lib/python3.12/site-packages/aiohttp/streams.py\", line 380, in readuntil\n    raise ValueError(\"Chunk too big\")\nValueError: Chunk too big\n",
    "",
    "",
    "",
    ""
  ],
  "request_timestamps": [
    1759818435.7959588,
    1759818446.1792746,
    1759818462.2902024,
    1759818493.6044495,
    1759818681.033673,
    1759818741.9008484,
    1759818890.849145,
    1759818897.548741
  ],
  "mean_ttft_ms": 28.04633162668324,
  "median_ttft_ms": 20.46844950382365,
  "std_ttft_ms": 15.707154550778212,
  "p99_ttft_ms": 63.88576037992606,
  "mean_tpot_ms": 2.9444744135336047,
  "median_tpot_ms": 2.936545722107127,
  "std_tpot_ms": 0.12012060281532427,
  "p99_tpot_ms": 3.186158833074539,
  "mean_itl_ms": 2.93885814296248,
  "median_itl_ms": 2.963149996503489,
  "std_itl_ms": 0.2694911943673561,
  "p99_itl_ms": 3.2691657695249887
}