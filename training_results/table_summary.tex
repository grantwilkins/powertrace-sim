\begin{table*}[!htb]
\centering
\small
\caption{Training performance of BiGRU classifiers per model, averaged across hardware (A100, H100) and tensor parallelism levels. 
Metrics report mean~$\pm$~standard deviation across configurations. 
Arrows indicate the direction of improvement.}
\label{tab:train_eval_model}
\vspace{0.4em}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Validation F1~$\uparrow$} & \textbf{Autocorr~$R^2~\uparrow$} & \textbf{Transition~MAE~(W)~$\downarrow$} & \textbf{Expected Calibration Error} $\downarrow$ \\
\midrule
\texttt{Llama-3.1 (8B)} & 0.50~$\pm$~0.01 & 1.00~$\pm$~0.00 & 81.9~$\pm$~21.2 & 0.0371~$\pm$~0.0136 \\
\texttt{Llama-3.1 (70B)} & 0.52~$\pm$~0.08 & 1.00~$\pm$~0.00 & 221.0~$\pm$~37.5 & 0.1122~$\pm$~0.0886 \\
\texttt{DeepSeek-Distill (8B)} & 0.52~$\pm$~0.08 & 1.00~$\pm$~0.00 & 99.8~$\pm$~40.9 & 0.0427~$\pm$~0.0227 \\
\texttt{DeepSeek-Distill (70B)} & 0.60~$\pm$~0.03 & 1.00~$\pm$~0.00 & 202.3~$\pm$~57.2 & 0.0904~$\pm$~0.0289 \\
\bottomrule
\end{tabular}
\end{table*}